–ò–°–•–û–î–ù–´–ô –ö–û–î –ü–†–û–ï–ö–¢–ê KAZrag (–î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π)\n==================================================\n\n

==================== –ù–ê–ß–ê–õ–û –§–ê–ô–õ–ê: main.py ====================
# –§–∞–π–ª: main.py
====================================================================================================
"""–û—Å–Ω–æ–≤–Ω–æ–π –º–æ–¥—É–ª—å –¥–ª—è –∑–∞–ø—É—Å–∫–∞ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è KAZrag."""

import os
import sys
import unittest  # –Ø–≤–Ω—ã–π –∏–º–ø–æ—Ä—Ç –¥–ª—è PyInstaller
from pathlib import Path

import uvicorn
from dotenv import load_dotenv

# –ò–º–ø–æ—Ä—Ç –º–æ–¥—É–ª–µ–π –¥–ª—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è
from app.app_factory import create_app
from app.startup import startup_event_handler
from config.logging_config import setup_logging, get_logger, setup_intercept_handler

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è —á–µ—Ä–µ–∑ —Ü–µ–Ω—Ç—Ä–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–π –º–æ–¥—É–ª—å
setup_logging()
setup_intercept_handler()
logger = get_logger(__name__)

# –ó–∞–≥—Ä—É–∂–∞–µ–º –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è –∏–∑ .env —Ñ–∞–π–ª–∞
load_dotenv()

# –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —Ç–æ–∫–µ–Ω HuggingFace –¥–ª—è transformers, –µ—Å–ª–∏ –æ–Ω –∑–∞–¥–∞–Ω
hf_token = os.getenv("HUGGINGFACE_TOKEN")
if hf_token:
    os.environ["HF_TOKEN"] = hf_token

# –°–æ–∑–¥–∞–Ω–∏–µ –æ—Å–Ω–æ–≤–Ω–æ–≥–æ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è FastAPI —á–µ—Ä–µ–∑ —Ñ–∞–±—Ä–∏–∫—É
app = create_app()





# –†–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏—è –æ–±—Ä–∞–±–æ—Ç—á–∏–∫–æ–≤ —Å–æ–±—ã—Ç–∏–π –∑–∞–ø—É—Å–∫–∞ –∏ –æ—Å—Ç–∞–Ω–æ–≤–∫–∏
@app.on_event("startup")
async def startup_event():
    """Handle application startup event."""
    startup_event_handler()

if __name__ == "__main__":
    logger.info("–ù–∞—á–∞–ª–æ –∑–∞–ø—É—Å–∫–∞ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è")
    # –ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –ø—Ä–∏ –∑–∞–ø—É—Å–∫–µ —Å –ø—Ä–æ–≤–µ—Ä–∫–æ–π Qdrant
    try:
        logger.info("–ü–æ–ø—ã—Ç–∫–∞ –∏–º–ø–æ—Ä—Ç–∞ ConfigManager")
        from config.config_manager import ConfigManager
        logger.info("ConfigManager —É—Å–ø–µ—à–Ω–æ –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω")
        logger.info("–ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏")
        config_manager = ConfigManager.get_instance()
        config_manager.get()
        logger.info("–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
    except RuntimeError as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏: {e}")
        sys.exit(1)
    # –°–æ–∑–¥–∞–Ω–∏–µ –æ—Å–Ω–æ–≤–Ω–æ–≥–æ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è FastAPI —á–µ—Ä–µ–∑ —Ñ–∞–±—Ä–∏–∫—É
    logger.info("–°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è")
    app = create_app()
    logger.info("–ü—Ä–∏–ª–æ–∂–µ–Ω–∏–µ —É—Å–ø–µ—à–Ω–æ —Å–æ–∑–¥–∞–Ω–æ")
    # –†–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏—è –æ–±—Ä–∞–±–æ—Ç—á–∏–∫–æ–≤ —Å–æ–±—ã—Ç–∏–π –∑–∞–ø—É—Å–∫–∞ –∏ –æ—Å—Ç–∞–Ω–æ–≤–∫–∏
    logger.info("–†–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏—è –æ–±—Ä–∞–±–æ—Ç—á–∏–∫–æ–≤ —Å–æ–±—ã—Ç–∏–π")
    @app.on_event("startup")
    async def startup_event():
        """Handle application startup event."""
        startup_event_handler()
    logger.info("–û–±—Ä–∞–±–æ—Ç—á–∏–∫–∏ —Å–æ–±—ã—Ç–∏–π –∑–∞—Ä–µ–≥–∏—Å—Ç—Ä–∏—Ä–æ–≤–∞–Ω—ã")
    # –ó–∞–ø—É—Å–∫ —Å–µ—Ä–≤–µ—Ä–∞
    try:
        logger.info("–ó–∞–ø—É—Å–∫ —Å–µ—Ä–≤–µ—Ä–∞ –Ω–∞ http://127.0.0.1:8000")
        # Exclude common log file patterns and logs directory from reload watcher
        uvicorn.run(
            "main:app",
            host="127.0.0.1",
            port=8000,
            reload=True,
            reload_excludes=["*.log", "logs/*", "logs/*", "logs/*", "*.log.*", "*.log~"],
            reload_dirs=["."]
        )
        logger.info("–°–µ—Ä–≤–µ—Ä –∑–∞–ø—É—â–µ–Ω")
    except Exception as e:
        logger.exception(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–ø—É—Å–∫–µ —Å–µ—Ä–≤–µ—Ä–∞: {e}")
        sys.exit(1)
====================================================================================================
==================== –ö–û–ù–ï–¶ –§–ê–ô–õ–ê: main.py ====================



==================== –ù–ê–ß–ê–õ–û –§–ê–ô–õ–ê: README.md ====================
# –§–∞–π–ª: README.md
====================================================================================================
# KAZrag

KAZrag ‚Äî —ç—Ç–æ –ø–æ–∏—Å–∫–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º Qdrant –∏ FastAPI. –ü—Ä–æ–µ–∫—Ç –ø–æ–∑–≤–æ–ª—è–µ—Ç –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å PDF-—Ñ–∞–π–ª—ã, –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å –∏—Ö –≤ Markdown, –∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞—Ç—å —Ç–µ–∫—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ –∏ –≤—ã–ø–æ–ª–Ω—è—Ç—å —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ –ø–æ –∫–æ–ª–ª–µ–∫—Ü–∏—è–º.

---

## üì¶ –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –ø—Ä–æ–µ–∫—Ç–∞

- `main.py` ‚Äî –æ—Å–Ω–æ–≤–Ω–æ–π —Å–∫—Ä–∏–ø—Ç (FastAPI + –ª–æ–≥–∏–∫–∞ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏ –∏ –ø–æ–∏—Å–∫–∞)
- `app/` ‚Äî –º–æ–¥—É–ª—å –≤–µ–±-–ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è
  - `app_factory.py` ‚Äî —Å–æ–∑–¥–∞–Ω–∏–µ –∏ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è FastAPI –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è
  - `routes.py` ‚Äî —Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏—è –º–∞—Ä—à—Ä—É—Ç–æ–≤
  - `startup.py` ‚Äî –æ–±—Ä–∞–±–æ—Ç—á–∏–∫–∏ —Å–æ–±—ã—Ç–∏–π –∑–∞–ø—É—Å–∫–∞ –∏ –æ—Å—Ç–∞–Ω–æ–≤–∫–∏
- `core/` ‚Äî –æ—Å–Ω–æ–≤–Ω—ã–µ –º–æ–¥—É–ª–∏ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è
  - `embedding/` ‚Äî —Ä–∞–±–æ—Ç–∞ —Å —ç–º–±–µ–¥–¥–µ—Ä–∞–º–∏
    - `embeddings.py` ‚Äî –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —ç–º–±–µ–¥–¥–µ—Ä–æ–≤
    - `gguf_embeddings.py` ‚Äî —Ä–∞–±–æ—Ç–∞ —Å GGUF –º–æ–¥–µ–ª—è–º–∏
    - `sparse_embedding_adapter.py` ‚Äî –∞–¥–∞–ø—Ç–µ—Ä –¥–ª—è —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω—ã—Ö —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
  - `indexing/` ‚Äî –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
    - `indexer.py` ‚Äî –æ—Å–Ω–æ–≤–Ω–æ–π –∏–Ω–¥–µ–∫—Å–∞—Ç–æ—Ä
    - `indexer_component.py` ‚Äî –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏
    - `multilevel_indexer.py` ‚Äî –º–Ω–æ–≥–æ—É—Ä–æ–≤–Ω–µ–≤—ã–π –∏–Ω–¥–µ–∫—Å–∞—Ç–æ—Ä
    - `document_loader.py` ‚Äî –∑–∞–≥—Ä—É–∑—á–∏–∫ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
    - `metadata_manager.py` ‚Äî –º–µ–Ω–µ–¥–∂–µ—Ä –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
    - `text_splitter.py` ‚Äî —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –Ω–∞ —á–∞–Ω–∫–∏
    - `chunker.py` ‚Äî –±–∞–∑–æ–≤—ã–π —á–∞–Ω–∫–µ—Ä
    - `paragraph_chunker.py` ‚Äî —á–∞–Ω–∫–µ—Ä –ø–æ –∞–±–∑–∞—Ü–∞–º
    - `sentence_chunker.py` ‚Äî —á–∞–Ω–∫–µ—Ä –ø–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è–º
    - `multilevel_chunker.py` ‚Äî –º–Ω–æ–≥–æ—É—Ä–æ–≤–Ω–µ–≤—ã–π —á–∞–Ω–∫–µ—Ä
    - `indexer_additional.py` ‚Äî –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏
  - `search/` ‚Äî –ø–æ–∏—Å–∫ –ø–æ –∫–æ–ª–ª–µ–∫—Ü–∏—è–º
    - `searcher.py` ‚Äî –æ—Å–Ω–æ–≤–Ω–æ–π –ø–æ–∏—Å–∫
    - `search_strategy.py` ‚Äî —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ –ø–æ–∏—Å–∫–∞
    - `search_executor.py` ‚Äî –∏—Å–ø–æ–ª–Ω–∏—Ç–µ–ª—å –ø–æ–∏—Å–∫–∞
    - `collection_analyzer.py` ‚Äî –∞–Ω–∞–ª–∏–∑ –∫–æ–ª–ª–µ–∫—Ü–∏–π
  - `converting/` ‚Äî –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è —Ñ–∞–π–ª–æ–≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ñ–æ—Ä–º–∞—Ç–æ–≤
    - `multi_format_converter.py` ‚Äî –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è —Ñ–∞–π–ª–æ–≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ñ–æ—Ä–º–∞—Ç–æ–≤ –≤ Markdown
    - `file_converter.py` ‚Äî –±–∞–∑–æ–≤—ã–π –∫–æ–Ω–≤–µ—Ä—Ç–µ—Ä —Ñ–∞–π–ª–æ–≤
    - `file_processor.py` ‚Äî –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä —Ñ–∞–π–ª–æ–≤
    - `manager.py` ‚Äî –º–µ–Ω–µ–¥–∂–µ—Ä –∫–æ–Ω–≤–µ—Ä—Ç–µ—Ä–æ–≤
    - `mineru_service.py` ‚Äî —Å–µ—Ä–≤–∏—Å –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å MinerU
    - `pdf_to_md_chunker.py` ‚Äî —á–∞–Ω–∫–µ—Ä PDF –≤ Markdown
    - `converters/` ‚Äî –º–æ–¥—É–ª–∏ –¥–ª—è –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–∏ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ñ–æ—Ä–º–∞—Ç–æ–≤
      - `base.py` ‚Äî –±–∞–∑–æ–≤—ã–π –∫–ª–∞—Å—Å –¥–ª—è –∫–æ–Ω–≤–µ—Ä—Ç–µ—Ä–æ–≤
      - `djvu_converter.py` ‚Äî –∫–æ–Ω–≤–µ—Ä—Ç–µ—Ä DJVU –≤ PDF
      - `unstructured_converter.py` ‚Äî —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –∫–æ–Ω–≤–µ—Ä—Ç–µ—Ä
  - `qdrant/` ‚Äî –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–µ —Å Qdrant
    - `qdrant_client.py` ‚Äî –∫–ª–∏–µ–Ω—Ç Qdrant
    - `qdrant_collections.py` ‚Äî —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –∫–æ–ª–ª–µ–∫—Ü–∏—è–º–∏ Qdrant
    - `qdrant_collections_additional.py` ‚Äî –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –∫–æ–ª–ª–µ–∫—Ü–∏—è–º–∏
  - `utils/` ‚Äî –≤—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ –º–æ–¥—É–ª–∏
    - `constants.py` ‚Äî –∫–æ–Ω—Å—Ç–∞–Ω—Ç—ã –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è
    - `exception_handlers.py` ‚Äî –æ–±—Ä–∞–±–æ—Ç—á–∏–∫–∏ –∏—Å–∫–ª—é—á–µ–Ω–∏–π
    - `collection_manager.py` ‚Äî –º–µ–Ω–µ–¥–∂–µ—Ä –∫–æ–ª–ª–µ–∫—Ü–∏–π
    - `dependencies.py` ‚Äî –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ FastAPI
- `web/` ‚Äî –≤–µ–±-–ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è FastAPI
  - `search_app.py` ‚Äî –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ –¥–ª—è –ø–æ–∏—Å–∫–∞
  - `admin_app.py` ‚Äî –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ –¥–ª—è –∞–¥–º–∏–Ω–∏—Å—Ç—Ä–∏—Ä–æ–≤–∞–Ω–∏—è
- `scripts/` ‚Äî –≤—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ —Å–∫—Ä–∏–ø—Ç—ã
  - `run_search.py` ‚Äî —Å–∫—Ä–∏–ø—Ç –¥–ª—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –ø–æ–∏—Å–∫–∞
  - `run_indexing.py` ‚Äî —Å–∫—Ä–∏–ø—Ç –¥–ª—è –∑–∞–ø—É—Å–∫–∞ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏
  - `clear_log.py` ‚Äî —Å–∫—Ä–∏–ø—Ç –¥–ª—è –æ—á–∏—Å—Ç–∫–∏ –ª–æ–≥–æ–≤
  - `evaluate_embeddings.py` ‚Äî —Å–∫—Ä–∏–ø—Ç –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –º–æ–¥–µ–ª–µ–π —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
  - `run_evaluation.py` ‚Äî —Å–∫—Ä–∏–ø—Ç –¥–ª—è –∑–∞–ø—É—Å–∫–∞ –æ—Ü–µ–Ω–∫–∏ –º–æ–¥–µ–ª–µ–π —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
  - `cleanup_test_collections.py` ‚Äî —Å–∫—Ä–∏–ø—Ç –¥–ª—è –æ—á–∏—Å—Ç–∫–∏ —Ç–µ—Å—Ç–æ–≤—ã—Ö –∫–æ–ª–ª–µ–∫—Ü–∏–π
  - `README_evaluation.md` ‚Äî –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è –ø–æ –æ—Ü–µ–Ω–∫–µ –º–æ–¥–µ–ª–µ–π —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
- `config/` ‚Äî –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è
  - `settings.py` ‚Äî —Ä–∞–±–æ—Ç–∞ —Å –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏ (Pydantic –º–æ–¥–µ–ª—å)
  - `config_manager.py` ‚Äî —Ü–µ–Ω—Ç—Ä–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–π –º–µ–Ω–µ–¥–∂–µ—Ä –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
  - `logging_config.py` ‚Äî –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
- `templates/` ‚Äî HTML-—à–∞–±–ª–æ–Ω—ã –¥–ª—è –≤–µ–±-–∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞
  - `index.html` ‚Äî —Å—Ç—Ä–∞–Ω–∏—Ü–∞ –ø–æ–∏—Å–∫–∞
  - `search_results.html` ‚Äî —Å—Ç—Ä–∞–Ω–∏—Ü–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ–∏—Å–∫–∞
  - `settings.html` ‚Äî —Å—Ç—Ä–∞–Ω–∏—Ü–∞ –Ω–∞—Å—Ç—Ä–æ–µ–∫
- `requirements.txt` ‚Äî –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ Python
- `pyproject.toml` ‚Äî –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –ø—Ä–æ–µ–∫—Ç–∞
- `.env.example` ‚Äî –ø—Ä–∏–º–µ—Ä —Ñ–∞–π–ª–∞ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è
- `.gitignore` ‚Äî –∏—Å–∫–ª—é—á–µ–Ω–∏—è –¥–ª—è Git
- `LICENSE` ‚Äî –ª–∏—Ü–µ–Ω–∑–∏—è MIT

---

## üöÄ –ë—ã—Å—Ç—Ä—ã–π —Å—Ç–∞—Ä—Ç

### 1. –ö–ª–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è

```bash
git clone https://github.com/Neuro-Storm/KAZrag.git
cd KAZrag
```

### 2. –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π

–†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –≤–∏—Ä—Ç—É–∞–ª—å–Ω–æ–µ –æ–∫—Ä—É–∂–µ–Ω–∏–µ:

```bash
python -m venv venv
venv\Scripts\activate  # Windows
# –∏–ª–∏
source venv/bin/activate  # Linux/Mac
```

–£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏:

```bash
pip install -r requirements.txt
```

### 3. –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è

–°–∫–æ–ø–∏—Ä—É–π—Ç–µ `.env.example` –≤ `.env` –∏ –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –æ—Ç—Ä–µ–¥–∞–∫—Ç–∏—Ä—É–π—Ç–µ:

```bash
cp .env.example .env
```

–í `.env` –º–æ–∂–Ω–æ –Ω–∞—Å—Ç—Ä–æ–∏—Ç—å:
- `ADMIN_API_KEY` –¥–ª—è –∑–∞—â–∏—Ç—ã –∞–¥–º–∏–Ω–∫–∏
- `HUGGINGFACE_TOKEN` –¥–ª—è –¥–æ—Å—Ç—É–ø–∞ –∫ –ø—Ä–∏–≤–∞—Ç–Ω—ã–º –º–æ–¥–µ–ª—è–º –Ω–∞ HuggingFace
- `FASTEMBED_CACHE_DIR` –¥–ª—è —É–∫–∞–∑–∞–Ω–∏—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –∫—ç—à–∞ –º–æ–¥–µ–ª–µ–π BM25 (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é `./models/fastembed_cache`)

### 4. –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –∏ –∑–∞–ø—É—Å–∫ Qdrant

–ó–∞–ø—É—Å—Ç–∏—Ç–µ Qdrant –ª–æ–∫–∞–ª—å–Ω–æ —Å –ø–æ–º–æ—â—å—é Docker:

```bash
docker run -p 6333:6333 -p 6334:6334 \
    -v $(pwd)/qdrant_storage:/qdrant/storage:z \
    qdrant/qdrant
```

–ò–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ –æ–±–ª–∞—á–Ω—É—é –≤–µ—Ä—Å–∏—é Qdrant, —É–∫–∞–∑–∞–≤ URL –≤ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏.

### 5. –£—Å—Ç–∞–Ω–æ–≤–∫–∞ MinerU (–¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ PDF)

```bash
pip install uv
uv pip install -U "mineru[core]"
```

> **–í–∞–∂–Ω–æ:** MinerU —Ç—Ä–µ–±—É–µ—Ç Python < 3.13 –∏ –º–æ–∂–µ—Ç —É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞—Ç—å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ (`magic-pdf`).

---

## üõ†Ô∏è –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ

1. –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ Qdrant –∑–∞–ø—É—â–µ–Ω.
2. –ê–∫—Ç–∏–≤–∏—Ä—É–π—Ç–µ –≤–∏—Ä—Ç—É–∞–ª—å–Ω–æ–µ –æ–∫—Ä—É–∂–µ–Ω–∏–µ (`venv\Scripts\activate` –∏–ª–∏ `source venv/bin/activate`).
3. –ü–æ–º–µ—Å—Ç–∏—Ç–µ —Ñ–∞–π–ª—ã —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ñ–æ—Ä–º–∞—Ç–æ–≤ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –≤ –ø–∞–ø–∫—É `pdfs_to_process/`. –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞—é—Ç—Å—è —Å–ª–µ–¥—É—é—â–∏–µ —Ñ–æ—Ä–º–∞—Ç—ã:
   - PDF (–æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—é—Ç—Å—è –Ω–∞–ø—Ä—è–º—É—é MinerU)
   - DJVU (–∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É—é—Ç—Å—è –≤ PDF, –∑–∞—Ç–µ–º –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—é—Ç—Å—è MinerU)
   - –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è JPG, PNG (–æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—é—Ç—Å—è –Ω–∞–ø—Ä—è–º—É—é MinerU)
   - –î–æ–∫—É–º–µ–Ω—Ç—ã DOCX, TXT, HTML (–∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É—é—Ç—Å—è –≤ Markdown)
   - –ü—Ä–µ–∑–µ–Ω—Ç–∞—Ü–∏–∏ PPT, PPTX (–∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É—é—Ç—Å—è –≤ Markdown)
   - –¢–∞–±–ª–∏—Ü—ã XLSX, XLS (–∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É—é—Ç—Å—è –≤ Markdown)

### –í–µ–±-–∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å

–ó–∞–ø—É—Å—Ç–∏—Ç–µ FastAPI-–ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ:

```bash
python main.py
```

- –í–µ–±-–∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å –±—É–¥–µ—Ç –¥–æ—Å—Ç—É–ø–µ–Ω –ø–æ –∞–¥—Ä–µ—Å—É: [http://localhost:8000](http://localhost:8000)
- Swagger UI: [http://localhost:8000/docs](http://localhost:8000/docs)
- –ê–¥–º–∏–Ω–∫–∞: [http://localhost:8000/settings](http://localhost:8000/settings)

–í –≤–µ–±-–∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–µ –¥–æ—Å—Ç—É–ø–Ω—ã —Å–ª–µ–¥—É—é—â–∏–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏:
- –ü–æ–∏—Å–∫ –ø–æ –∫–æ–ª–ª–µ–∫—Ü–∏—è–º —Å —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–µ–π –ø–æ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º
- –£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏, –≤–∫–ª—é—á–∞—è –≤—ã–±–æ—Ä –º–æ–¥–µ–ª–∏ –¥–ª—è –ø–ª–æ—Ç–Ω—ã—Ö –≤–µ–∫—Ç–æ—Ä–æ–≤
- **–ù–∞—Å—Ç—Ä–æ–π–∫–∞ reranker –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ–∏—Å–∫–∞**
- –£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏ –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–∏ —Ñ–∞–π–ª–æ–≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ñ–æ—Ä–º–∞—Ç–æ–≤
- –ó–∞–ø—É—Å–∫ –ø—Ä–æ—Ü–µ—Å—Å–∞ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏ –∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ñ–∞–π–ª–æ–≤
- –ü—Ä–æ—Å–º–æ—Ç—Ä –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –∫–æ–ª–ª–µ–∫—Ü–∏—è—Ö

### –ö–æ–Ω—Å–æ–ª—å–Ω—ã–π –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å

–ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω–æ, –≤—ã –º–æ–∂–µ—Ç–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∫–æ–Ω—Å–æ–ª—å–Ω—ã–π –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å –¥–ª—è –ø–æ–∏—Å–∫–∞:

```bash
python consolemain.py
```

–ö–æ–Ω—Å–æ–ª—å–Ω—ã–π –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç —Ç–µ –∂–µ —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ–∏—Å–∫–∞, —á—Ç–æ –∏ –≤–µ–±-–∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å, –Ω–æ –≤ —Ç–µ–∫—Å—Ç–æ–≤–æ–º —Ä–µ–∂–∏–º–µ.

–¢–∞–∫–∂–µ –¥–æ—Å—Ç—É–ø–µ–Ω —Å–∫—Ä–∏–ø—Ç –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –ø–æ–∏—Å–∫–æ–≤–æ–π —Å–∏—Å—Ç–µ–º—ã —Å –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–Ω—ã–º–∏ –≤–æ–ø—Ä–æ—Å–∞–º–∏:

```bash
python scripts/run_search_test.py
```

–î–ª—è –¥–æ—Å—Ç—É–ø–∞ –∫ –∞–¥–º–∏–Ω–∫–µ –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ API –∫–ª—é—á, —É–∫–∞–∑–∞–Ω–Ω—ã–π –≤ —Ñ–∞–π–ª–µ `.env`.

---

## üß™ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ

–ü—Ä–æ–µ–∫—Ç –≤–∫–ª—é—á–∞–µ—Ç –≤ —Å–µ–±—è –∫–∞–∫ unit-—Ç–µ—Å—Ç—ã, —Ç–∞–∫ –∏ –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–æ–Ω–Ω—ã–µ —Ç–µ—Å—Ç—ã –¥–ª—è –æ–±–µ—Å–ø–µ—á–µ–Ω–∏—è –∫–∞—á–µ—Å—Ç–≤–∞ –∫–æ–¥–∞.

### –ó–∞–ø—É—Å–∫ —Ç–µ—Å—Ç–æ–≤

–î–ª—è –∑–∞–ø—É—Å–∫–∞ –≤—Å–µ—Ö —Ç–µ—Å—Ç–æ–≤ –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ pytest:

```bash
# –ó–∞–ø—É—Å–∫ –≤—Å–µ—Ö —Ç–µ—Å—Ç–æ–≤
python -m pytest tests -v

# –ó–∞–ø—É—Å–∫ —Ç–µ—Å—Ç–æ–≤ —Å –æ—Ç—á–µ—Ç–æ–º –æ –ø–æ–∫—Ä—ã—Ç–∏–∏
python -m pytest tests --cov=core --cov=web --cov=config --cov-report=html

# –ó–∞–ø—É—Å–∫ —Ç–æ–ª—å–∫–æ unit-—Ç–µ—Å—Ç–æ–≤
python -m pytest tests/test_config_manager.py tests/test_collection_manager.py tests/test_embedding_manager.py tests/test_indexer_components.py tests/test_search_components.py tests/test_web_routes.py tests/test_paragraph_chunker.py tests/test_sentence_chunker.py -v

# –ó–∞–ø—É—Å–∫ —Ç–æ–ª—å–∫–æ –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–æ–Ω–Ω—ã—Ö —Ç–µ—Å—Ç–æ–≤
python -m pytest tests/test_indexing_integration.py tests/test_search_integration.py tests/test_web_api_integration.py -v
```

### –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Ç–æ–∫–µ–Ω–∞ HuggingFace

–î–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –ø—Ä–∞–≤–∏–ª—å–Ω–æ—Å—Ç–∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ —Ç–æ–∫–µ–Ω–∞ HuggingFace –º–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Ç–µ—Å—Ç–æ–≤—ã–π —Å–∫—Ä–∏–ø—Ç:

```bash
python scripts/test_hf_token.py
```

## üìä –û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–µ–π —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤

–ü—Ä–æ–µ–∫—Ç –≤–∫–ª—é—á–∞–µ—Ç –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –Ω–∞–∏–±–æ–ª–µ–µ –ø–æ–¥—Ö–æ–¥—è—â–µ–π –¥–ª—è –≤–∞—à–µ–π –∑–∞–¥–∞—á–∏.

### –ó–∞–ø—É—Å–∫ –æ—Ü–µ–Ω–∫–∏

–ü–æ–¥—Ä–æ–±–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Ç–æ–º, –∫–∞–∫ –∑–∞–ø—É—Å—Ç–∏—Ç—å –æ—Ü–µ–Ω–∫—É –º–æ–¥–µ–ª–µ–π —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤, –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –≤ —Ñ–∞–π–ª–µ [scripts/README_evaluation.md](scripts/README_evaluation.md).

```bash
# –ó–∞–ø—É—Å–∫ –æ—Ü–µ–Ω–∫–∏ –º–æ–¥–µ–ª–µ–π —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
python scripts/run_evaluation.py
```

---

## üîÑ CI/CD –∏ –ª–∏–Ω—Ç–∏–Ω–≥

–ü—Ä–æ–µ–∫—Ç –∏—Å–ø–æ–ª—å–∑—É–µ—Ç GitHub Actions –¥–ª—è –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω–æ–π –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ –∏ –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω–æ–π –¥–æ—Å—Ç–∞–≤–∫–∏.

### –õ–∏–Ω—Ç–∏–Ω–≥ –∫–æ–¥–∞

–ü—Ä–æ–µ–∫—Ç –∏—Å–ø–æ–ª—å–∑—É–µ—Ç `ruff` –¥–ª—è –ª–∏–Ω—Ç–∏–Ω–≥–∞ –∫–æ–¥–∞. –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –≤ —Ñ–∞–π–ª–µ `ruff.toml`.

–î–ª—è –∑–∞–ø—É—Å–∫–∞ –ª–∏–Ω—Ç–µ—Ä–∞:

```bash
# –£—Å—Ç–∞–Ω–æ–≤–∫–∞ ruff
pip install ruff

# –ó–∞–ø—É—Å–∫ –ª–∏–Ω—Ç–µ—Ä–∞
ruff check .

# –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –æ—à–∏–±–æ–∫ (–≥–¥–µ –≤–æ–∑–º–æ–∂–Ω–æ)
ruff check . --fix
```

---

## ‚öôÔ∏è –û—Å–Ω–æ–≤–Ω—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏

- –û–±—Ä–∞–±–æ—Ç–∫–∞ –∏ —Ä–∞–∑–±–∏–µ–Ω–∏–µ PDF –Ω–∞ Markdown (MinerU)
- –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ñ–∞–π–ª–æ–≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ñ–æ—Ä–º–∞—Ç–æ–≤:
  - –î–æ–∫—É–º–µ–Ω—Ç—ã: DOCX, TXT, HTML
  - –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è: JPG, PNG (—Å –ø–æ–º–æ—â—å—é MinerU)
  - –ü—Ä–µ–∑–µ–Ω—Ç–∞—Ü–∏–∏: PPT, PPTX
  - –¢–∞–±–ª–∏—Ü—ã: XLSX, XLS
  - DJVU (–∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ—Ç—Å—è –≤ PDF, –∑–∞—Ç–µ–º –≤ Markdown)
- –ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –∏ Markdown —Ñ–∞–π–ª–æ–≤ –≤ Qdrant —Å –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
- –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ –ø–æ –∫–æ–ª–ª–µ–∫—Ü–∏—è–º —Å –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å—é —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –ø–æ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º
- **–ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–æ–∏—Å–∫–æ–≤—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤ –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –ø–æ–≤—Ç–æ—Ä–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤**
- **Reranker –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ–∏—Å–∫–∞** (–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –ª–æ–∫–∞–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏ –≤ —Ñ–æ—Ä–º–∞—Ç–µ GGUF) (–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –ª–æ–∫–∞–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏ –≤ —Ñ–æ—Ä–º–∞—Ç–µ GGUF)
- –í–µ–±-–∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å –¥–ª—è –ø–æ–∏—Å–∫–∞ –∏ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏
- –ì–∏–±–∫–∞—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ –º–æ–¥–µ–ª–µ–π, –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏ –∏ –ø—É—Ç–µ–π
- –ü–æ–¥–¥–µ—Ä–∂–∫–∞ GGUF –º–æ–¥–µ–ª–µ–π –¥–ª—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
- –ë–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å —á–µ—Ä–µ–∑ API –∫–ª—é—á–∏
- –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ –æ–±—Ä–∞–±–æ—Ç–∫–∞ –æ—à–∏–±–æ–∫
- –¶–µ–Ω—Ç—Ä–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–µ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–µ–π —Å –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º
- –†–∞–∑–ª–∏—á–Ω—ã–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ —á–∞–Ω–∫–∏–Ω–≥–∞:
  - –ü–æ —Å–∏–º–≤–æ–ª–∞–º (—Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞—è)
  - –ü–æ –∞–±–∑–∞—Ü–∞–º (—Å –Ω–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º—ã–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º –∞–±–∑–∞—Ü–µ–≤ –≤ —á–∞–Ω–∫–µ –∏ –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ–º)
  - –ü–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è–º (—Å –Ω–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º—ã–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π –≤ —á–∞–Ω–∫–µ –∏ –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ–º)
- –ú–Ω–æ–≥–æ—É—Ä–æ–≤–Ω–µ–≤—ã–π —á–∞–Ω–∫–∏–Ω–≥ (–æ–¥–∏–Ω —Ñ—Ä–∞–≥–º–µ–Ω—Ç - –Ω–µ—Å–∫–æ–ª—å–∫–æ –≤–µ–∫—Ç–æ—Ä–æ–≤):
  - –ü–æ–ª–Ω–æ—Å—Ç—å—é –Ω–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º—ã–µ –º–∞–∫—Ä–æ-—á–∞–Ω–∫–∏ (—Ä–∞–∑–º–µ—Ä, —Å—Ç—Ä–∞—Ç–µ–≥–∏—è, –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ)
  - –ü–æ–ª–Ω–æ—Å—Ç—å—é –Ω–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º—ã–µ –º–∏–∫—Ä–æ-—á–∞–Ω–∫–∏ (—Ä–∞–∑–º–µ—Ä, —Å—Ç—Ä–∞—Ç–µ–≥–∏—è, –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ)
  - –õ—é–±–∞—è –∫–æ–º–±–∏–Ω–∞—Ü–∏—è —Å—Ç—Ä–∞—Ç–µ–≥–∏–π –Ω–∞ —Ä–∞–∑–Ω—ã—Ö —É—Ä–æ–≤–Ω—è—Ö
  - –ò–Ω—Ç—É–∏—Ç–∏–≤–Ω—ã–π –≤–µ–±-–∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å —Å —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ–º –Ω–∞—Å—Ç—Ä–æ–µ–∫
- –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ —á–∞–Ω–∫–∏–Ω–≥–∞ —á–µ—Ä–µ–∑ –≤–µ–±-–∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å
- –ò—Å—Ç–æ—Ä–∏—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –≤—ã–±–æ—Ä–∞
- –û—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏–µ —Å–æ—Å—Ç–æ—è–Ω–∏—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏

---

## üìö –ü–æ–ª–µ–∑–Ω—ã–µ —Å—Å—ã–ª–∫–∏

- [Qdrant ‚Äî –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è](https://qdrant.tech/)
- [MinerU ‚Äî GitHub](https://github.com/opendatalab/MinerU)
- [–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ —Ç–æ–∫–µ–Ω–∞ HuggingFace](docs/huggingface_token.md)

---

## üìù –õ–∏—Ü–µ–Ω–∑–∏—è

–ü—Ä–æ–µ–∫—Ç —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω—è–µ—Ç—Å—è –ø–æ–¥ –ª–∏—Ü–µ–Ω–∑–∏–µ–π MIT. –°–º. —Ñ–∞–π–ª [LICENSE](LICENSE).
====================================================================================================
==================== –ö–û–ù–ï–¶ –§–ê–ô–õ–ê: README.md ====================



==================== –ù–ê–ß–ê–õ–û –§–ê–ô–õ–ê: app\app_factory.py ====================
# –§–∞–π–ª: app\app_factory.py
====================================================================================================
"""Module for creating and configuring the FastAPI application."""

import logging
import os
from pathlib import Path

from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import RedirectResponse
from fastapi.staticfiles import StaticFiles

# Import routes registration
from app.routes import register_routes

# Import exception handlers
from core.utils.exception_handlers import add_exception_handlers

# Import resource path resolver
from config.resource_path import resource_path

logger = logging.getLogger(__name__)


def _parse_allowed_origins(allowed_origins_env: str) -> list:
    """Parse ALLOWED_ORIGINS environment variable into a list of origins.
    
    Args:
        allowed_origins_env: Environment variable value
        
    Returns:
        List of origins for CORS middleware
    """
    # Handle special case of "*" (allow all origins)
    if allowed_origins_env.strip() == "*":
        return ["*"]
    
    # Split by comma and strip whitespace
    origins = [origin.strip() for origin in allowed_origins_env.split(',') if origin.strip()]
    
    # Validate and normalize origins
    normalized_origins = []
    for origin in origins:
        # Skip empty origins
        if not origin:
            continue
            
        # Add scheme if missing (assume https for security)
        if not origin.startswith(('http://', 'https://')):
            origin = f"https://{origin}"
            
        normalized_origins.append(origin)
    
    return normalized_origins


def create_app() -> FastAPI:
    """Create and configure the FastAPI application.
    
    Returns:
        FastAPI: Configured FastAPI application instance
    """
    # Create the main FastAPI application
    app = FastAPI(
        title="KAZrag",
        description="–ü–æ–∏—Å–∫–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞"
    )
    
    # Use asgi-request-id middleware for generating request IDs
    from asgi_request_id import RequestIDMiddleware
    app.add_middleware(RequestIDMiddleware)
    
    # Add centralized exception handlers
    add_exception_handlers(app)
    
    # Add CORS middleware
    allowed_origins_env = os.getenv("ALLOWED_ORIGINS", "*") or "*"
    allowed_origins = _parse_allowed_origins(allowed_origins_env)
    
    app.add_middleware(
        CORSMiddleware,
        allow_origins=allowed_origins,
        allow_credentials=True,
        allow_methods=["*"],
        allow_headers=["*"],
    )
    
    # Mount static files
    static_dir = resource_path("web/static")
    if static_dir.exists():
        app.mount("/static", StaticFiles(directory=str(static_dir)), name="static")
    else:
        logger.warning(f"Static directory {static_dir} does not exist")
    
    # Register routes
    register_routes(app)
    
    # Add root route for redirecting to search page
    @app.get("/", response_class=RedirectResponse)
    async def root():
        """Root route - redirects to search page"""
        return RedirectResponse(url="/api/search/")
    
    # Add settings route that redirects to admin
    @app.get("/settings", response_class=RedirectResponse)
    async def settings_redirect():
        """Settings route - redirects to admin panel"""
        return RedirectResponse(url="/api/admin/settings/")
    
    return app
====================================================================================================
==================== –ö–û–ù–ï–¶ –§–ê–ô–õ–ê: app\app_factory.py ====================



==================== –ù–ê–ß–ê–õ–û –§–ê–ô–õ–ê: app\routes.py ====================
# –§–∞–π–ª: app\routes.py
====================================================================================================
"""Module for registering application routes."""

from fastapi import FastAPI

from web.admin_app import app as admin_router
from web.search_app import app as search_router


def register_routes(app: FastAPI) -> None:
    """Register all application routes.
    
    Args:
        app (FastAPI): The FastAPI application instance
    """
    # Register the search router with prefix
    app.include_router(search_router, prefix="/api/search")
    
    # Register the admin router with prefix
    app.include_router(admin_router, prefix="/api/admin")
====================================================================================================
==================== –ö–û–ù–ï–¶ –§–ê–ô–õ–ê: app\routes.py ====================



==================== –ù–ê–ß–ê–õ–û –§–ê–ô–õ–ê: app\startup.py ====================
# –§–∞–π–ª: app\startup.py
====================================================================================================
"""Module for application startup and shutdown event handlers."""

import logging

from config.config_manager import ConfigManager
from core.search.search_strategy import SearchStrategy
from core.qdrant.qdrant_client import get_qdrant_client

logger = logging.getLogger(__name__)


def startup_event_handler() -> None:
    """Logic to execute when the application starts.
    
    This includes loading configuration and checking Qdrant availability.
    """
    logger.info("Application startup event triggered")
    
    # Load configuration at startup (without Qdrant check to avoid blocking startup)
    try:
        logger.info("Loading configuration")
        config_manager = ConfigManager.get_instance()
        config = config_manager.get()
        logger.info("Configuration successfully loaded")
        
        # Initialize BM25 collection if enabled
        if config.use_bm25:
            logger.info("Initializing BM25 collection configuration")
            try:
                client = get_qdrant_client(config)
                from core.embedding.sparse_embedding_adapter import SparseEmbeddingAdapter
                sparse_emb = SparseEmbeddingAdapter(config)
                strategy = SearchStrategy(client, config.collection_name, None, sparse_emb)
                strategy.create_or_update_collection_for_bm25()
                logger.info("BM25 collection configuration initialized successfully")
            except Exception as e:
                logger.exception(f"Error initializing BM25 collection configuration: {e}")
    except Exception as e:
        logger.exception(f"Error loading configuration: {e}")
        # Don't exit here, as we want the app to start even if config has issues
        # The app will handle config errors when needed
====================================================================================================
==================== –ö–û–ù–ï–¶ –§–ê–ô–õ–ê: app\startup.py ====================



==================== –ù–ê–ß–ê–õ–û –§–ê–ô–õ–ê: config\config_manager.py ====================
# –§–∞–π–ª: config\config_manager.py
====================================================================================================
"""Module for centralized configuration management using pydantic-settings."""

import json
import logging
from typing import Optional, Dict, Any
from pathlib import Path

from cachetools import TTLCache
from pydantic import ValidationError

from config.settings_model import Config
from config.resource_path import resource_path

logger = logging.getLogger(__name__)


class ConfigManager:
    """Centralized configuration manager with caching and validation using pydantic-settings."""
    
    _instance: Optional['ConfigManager'] = None
    
    def __init__(self, cache_ttl: int = 60):
        """Initialize ConfigManager.
        
        Args:
            cache_ttl: Time to live for cached configuration in seconds
        """
        self.cache_ttl = cache_ttl
        # Use TTLCache for configuration caching
        self._cache = TTLCache(maxsize=1, ttl=cache_ttl)
        self._config_instance: Optional[Config] = None
        
    @classmethod
    def get_instance(cls, cache_ttl: int = 60) -> 'ConfigManager':
        """Get singleton instance of ConfigManager.
        
        Args:
            cache_ttl: Time to live for cached configuration in seconds
            
        Returns:
            ConfigManager: Singleton instance
        """
        if cls._instance is None:
            cls._instance = cls(cache_ttl)
        return cls._instance
    
    def _load_config_from_file(self) -> Config:
        """Load configuration from file without caching.
        
        Returns:
            Config: Loaded configuration object
        """
        logger.debug("Loading configuration from file")
        config_file = resource_path("config/config.json")
        
        if not config_file.exists():
            # Create default config if file doesn't exist
            config = Config()
            self.save(config)
            return config
            
        try:
            with open(config_file, encoding="utf-8") as f:
                config_dict = json.load(f)
                
            # Create Config object with validation
            config = Config(**config_dict)
            return config
            
        except ValidationError as e:
            logger.exception(f"Configuration validation errors: {e.errors()}")
            # Return default config if validation fails
            config = Config()
            self.save(config)
            return config
            
        except Exception as e:
            logger.exception(f"Error loading configuration: {e}")
            # Return default config if any other error occurs
            config = Config()
            self.save(config)
            return config
    
    def _load_config_from_settings(self) -> Config:
        """Load configuration from environment variables and settings.
        
        Returns:
            Config: Loaded configuration object
        """
        try:
            # Load config from environment variables and defaults
            config = Config()
            return config
        except Exception as e:
            logger.exception(f"Error loading configuration from settings: {e}")
            # Fallback to file-based config
            return self._load_config_from_file()
    
    def load(self) -> Config:
        """Load configuration from file with caching.
        
        Returns:
            Config: Loaded configuration object
        """
        # Check if we have a valid cached config
        if 'config' in self._cache:
            logger.debug("Returning cached configuration")
            return self._cache['config']
        
        # Try to load from file first
        try:
            config = self._load_config_from_file()
        except Exception as e:
            logger.warning(f"Failed to load config from file, falling back to settings: {e}")
            # Fallback to settings-based config
            config = self._load_config_from_settings()
        
        # Update cache
        self._cache['config'] = config
        
        return config
    
    def save(self, config: Config) -> None:
        """Save configuration to file.
        
        Args:
            config: Configuration object to save
        """
        try:
            # Save to file
            config.save_to_file()
            
            # Update cache
            self._cache['config'] = config
            
            logger.info("Configuration saved successfully")
            
        except Exception as e:
            logger.exception(f"Error saving configuration: {e}")
            raise
    
    def get(self) -> Config:
        """Get configuration (alias for load).
        
        Returns:
            Config: Configuration object
        """
        return self.load()
    
    def reload(self) -> Config:
        """Force reload configuration from file, bypassing cache.
        
        Returns:
            Config: Reloaded configuration object
        """
        # Clear the cache
        self._cache.clear()
        
        # Load fresh config
        return self.load()
    
    def update_from_dict(self, updates: Dict[str, Any]) -> Config:
        """Update configuration from dictionary.
        
        Args:
            updates: Dictionary with configuration updates
            
        Returns:
            Config: Updated configuration object
        """
        try:
            # Get current config
            current_config = self.get()
            
            # Convert to dict and update
            config_dict = current_config.model_dump()
            config_dict.update(updates)
            
            # Validate new BM25 fields
            if 'use_bm25' in updates:
                config.use_bm25 = updates['use_bm25']
                config.sparse_vector_name = updates.get('sparse_vector_name', config.sparse_vector_name)
            
            # Create new config object
            updated_config = Config(**config_dict)
            
            # Save updated config
            self.save(updated_config)
            
            return updated_config
            
        except Exception as e:
            logger.exception(f"Error updating configuration: {e}")
            raise
====================================================================================================
==================== –ö–û–ù–ï–¶ –§–ê–ô–õ–ê: config\config_manager.py ====================



==================== –ù–ê–ß–ê–õ–û –§–ê–ô–õ–ê: config\logging_config.py ====================
# –§–∞–π–ª: config\logging_config.py
====================================================================================================
"""Centralized logging configuration using loguru."""

import logging
from pathlib import Path

from loguru import logger


def setup_logging(
    level: str = "INFO",
    log_file: str = None,
    rotation: str = "100 MB",
    retention: str = "30 days",
    format: str = "{time:YYYY-MM-DD HH:mm:ss} | {level: <8} | {name}:{function}:{line} - {message}",
    serialize: bool = False
) -> None:
    """Set up centralized logging configuration with loguru.
    
    Args:
        level: Logging level (default: "INFO")
        log_file: Path to log file (default: logs/app.log)
        rotation: Log file rotation policy (default: "100 MB")
        retention: Log file retention policy (default: "30 days")
        format: Log message format
        serialize: Whether to serialize logs as JSON (default: False)
    """
    # Remove default logger to avoid duplicate logs
    logger.remove()
    
    # Create logs directory
    if log_file is None:
        log_dir = Path("logs")
        log_dir.mkdir(parents=True, exist_ok=True)
        log_file = str(log_dir / "app.log")
    
    # Add file sink with rotation and retention
    logger.add(
        log_file,
        level=level,
        format=format,
        rotation=rotation,
        retention=retention,
        encoding="utf-8",
        serialize=serialize
    )
    
    # Add console sink
    logger.add(
        lambda msg: print(msg, end=""),
        level=level,
        format=format,
        serialize=serialize
    )
    
    # Reduce verbosity of the file watcher to avoid log-change loops
    try:
        logger.level("watchfiles", no=30)  # Set to WARNING level
    except ValueError:
        # Level already exists, that's fine
        pass


# Convenience function for getting logger instance
def get_logger(name: str = None):
    """Get a logger instance.
    
    Args:
        name: Logger name (optional)
        
    Returns:
        loguru logger instance
    """
    return logger


# Intercept standard logging messages and redirect to loguru
class InterceptHandler(logging.Handler):
    def emit(self, record):
        # Get corresponding Loguru level if it exists
        try:
            level = logger.level(record.levelname).name
        except ValueError:
            level = record.levelno

        # Find caller from where originated the logged message
        frame, depth = logging.currentframe(), 2
        while frame.f_code.co_filename == logging.__file__:
            frame = frame.f_back
            depth += 1

        logger.opt(depth=depth, exception=record.exc_info).log(level, record.getMessage())


# Setup intercept handler for standard logging
def setup_intercept_handler():
    """Setup intercept handler to redirect standard logging to loguru."""
    logging.basicConfig(handlers=[InterceptHandler()], level=0)
====================================================================================================
==================== –ö–û–ù–ï–¶ –§–ê–ô–õ–ê: config\logging_config.py ====================



==================== –ù–ê–ß–ê–õ–û –§–ê–ô–õ–ê: config\resource_path.py ====================
# –§–∞–π–ª: config\resource_path.py
====================================================================================================
"""Module for resolving resource paths in both development and packaged environments."""

import sys
from pathlib import Path


def resource_path(relative_path: str) -> Path:
    """Get absolute path to resource, works for dev and for PyInstaller bundles.
    
    Args:
        relative_path: Relative path to the resource
        
    Returns:
        Absolute path to the resource
    """
    try:
        # PyInstaller creates a temp folder and stores path in _MEIPASS
        base_path = Path(sys._MEIPASS)
    except Exception:
        # In development environment, use the current directory
        base_path = Path(__file__).resolve().parent.parent

    return base_path / relative_path
====================================================================================================
==================== –ö–û–ù–ï–¶ –§–ê–ô–õ–ê: config\resource_path.py ====================



==================== –ù–ê–ß–ê–õ–û –§–ê–ô–õ–ê: config\settings.py ====================
# –§–∞–π–ª: config\settings.py
====================================================================================================
"""–ú–æ–¥—É–ª—å –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–µ–π –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è - —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å —Å –ø—Ä–µ–¥—ã–¥—É—â–µ–π –≤–µ—Ä—Å–∏–µ–π."""

from config.settings_model import Config
from config.config_manager import ConfigManager

# –î–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
CONFIG_FILE = "config/config.json"

# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
def load_config(reload: bool = False) -> Config:
    """–ó–∞–≥—Ä—É–∑–∏—Ç—å –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –¥–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏.
    
    Args:
        reload: –ï—Å–ª–∏ True, –ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ –ø–µ—Ä–µ–∑–∞–≥—Ä—É–∑–∏—Ç—å –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
        
    Returns:
        Config: –û–±—ä–µ–∫—Ç –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
    """
    manager = ConfigManager.get_instance()
    if reload:
        return manager.reload()
    return manager.get()

__all__ = ["Config", "ConfigManager", "CONFIG_FILE", "load_config"]
====================================================================================================
==================== –ö–û–ù–ï–¶ –§–ê–ô–õ–ê: config\settings.py ====================



==================== –ù–ê–ß–ê–õ–û –§–ê–ô–õ–ê: config\settings_model.py ====================
# –§–∞–π–ª: config\settings_model.py
====================================================================================================
"""–ú–æ–¥—É–ª—å –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–µ–π –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º pydantic-settings."""

from typing import Any, Dict, List, Optional
from pathlib import Path

from pydantic import Field
from pydantic_settings import BaseSettings, SettingsConfigDict

# Import resource path resolver
from config.resource_path import resource_path


class Config(BaseSettings):
    """–ú–æ–¥–µ–ª—å –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è —Å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –∑–∞–≥—Ä—É–∑–∫–æ–π –∏–∑ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤."""
    
    model_config = SettingsConfigDict(
        env_file=".env",
        env_file_encoding="utf-8",
        env_nested_delimiter="__",
        case_sensitive=False,
        extra="ignore"
    )
    
    # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏
    folder_path: str = "./data_to_index"
    collection_name: str = "final-dense-collection"
    current_hf_model: str = "sentence-transformers/all-MiniLM-L6-v2"
    hf_model_history: List[str] = Field(default=["sentence-transformers/all-MiniLM-L6-v2"])
    chunk_size: int = 500
    chunk_overlap: int = 100
    
    # –ù–æ–≤—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è —á–∞–Ω–∫–∏–Ω–≥–∞
    chunking_strategy: str = "character"  # "character", "paragraph" –∏–ª–∏ "sentence"
    
    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è —á–∞–Ω–∫–∏–Ω–≥–∞ –ø–æ –∞–±–∑–∞—Ü–∞–º
    paragraphs_per_chunk: int = 3
    paragraph_overlap: int = 1
    
    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è —á–∞–Ω–∫–∏–Ω–≥–∞ –ø–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è–º
    sentences_per_chunk: int = 5
    sentence_overlap: int = 1
    
    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è –º–Ω–æ–≥–æ—É—Ä–æ–≤–Ω–µ–≤–æ–≥–æ —á–∞–Ω–∫–∏–Ω–≥–∞
    use_multilevel_chunking: bool = False
    
    # –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –º–æ–¥–µ–ª–µ–π
    enable_pre_chunking_optimization: bool = True  # –í–∫–ª—é—á–∏—Ç—å –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω—ã–π chunking –ø—Ä–∏ –æ—Ü–µ–Ω–∫–µ –º–Ω–æ–∂–µ—Å—Ç–≤–∞ –º–æ–¥–µ–ª–µ–π
    
    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –º–∞–∫—Ä–æ-—á–∞–Ω–∫–∏–Ω–≥–∞
    multilevel_macro_strategy: str = "character"  # "character", "paragraph" –∏–ª–∏ "sentence"
    multilevel_macro_chunk_size: int = 10000
    multilevel_macro_chunk_overlap: int = 1000
    multilevel_macro_paragraphs_per_chunk: int = 5
    multilevel_macro_paragraph_overlap: int = 1
    multilevel_macro_sentences_per_chunk: int = 10
    multilevel_macro_sentence_overlap: int = 1
    
    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –º–∏–∫—Ä–æ-—á–∞–Ω–∫–∏–Ω–≥–∞
    multilevel_micro_strategy: str = "character"  # "character", "paragraph" –∏–ª–∏ "sentence"
    multilevel_micro_chunk_size: int = 1000
    multilevel_micro_chunk_overlap: int = 100
    multilevel_micro_paragraphs_per_chunk: int = 3
    multilevel_micro_paragraph_overlap: int = 1
    multilevel_micro_sentences_per_chunk: int = 5
    multilevel_micro_sentence_overlap: int = 1
    
    device: str = "auto"
    
    # –ù–æ–≤—ã–µ –æ—Ç–¥–µ–ª—å–Ω—ã–µ —Ä–µ–∂–∏–º—ã –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏
    index_dense: bool = True
    index_bm25: bool = False
    index_hybrid: bool = False
    embedding_batch_size: int = Field(default=32, ge=1)  # –ú–∏–Ω–∏–º—É–º 1
    indexing_batch_size: int = Field(default=50, ge=1)   # –ú–∏–Ω–∏–º—É–º 1
    force_recreate: bool = True
    memory_threshold: int = 500 * 1024 * 1024  # 500MB –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
    sparse_embedding: Optional[str] = "Qdrant/bm25"
    is_indexed: bool = False  # –§–ª–∞–≥, —É–∫–∞–∑—ã–≤–∞—é—â–∏–π, –±—ã–ª–∞ –ª–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∞ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—è
    
    # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ Qdrant
    qdrant_url: str = "http://localhost:6333"
    
    # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ MinerU
    mineru_input_pdf_dir: str = "./pdfs_to_process"
    mineru_output_md_dir: str = "./data_to_index"
    mineru_enable_formula_parsing: bool = False
    mineru_enable_table_parsing: bool = False
    mineru_model_source: str = "huggingface"
    mineru_models_dir: str = ""
    mineru_backend: str = "pipeline"
    mineru_method: str = "auto"
    mineru_lang: str = "east_slavic"
    mineru_sglang_url: str = ""
    mineru_subprocess_timeout: int = 600  # –¢–∞–π–º–∞—É—Ç –¥–ª—è subprocess –≤—ã–∑–æ–≤–∞ mineru –≤ —Å–µ–∫—É–Ω–¥–∞—Ö
    
    # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è
    config_cache_ttl: int = Field(default=60, ge=1)  # –ú–∏–Ω–∏–º—É–º 1 —Å–µ–∫—É–Ω–¥–∞
    qdrant_client_cache_ttl: int = Field(default=60, ge=1)  # –ú–∏–Ω–∏–º—É–º 1 —Å–µ–∫—É–Ω–¥–∞
    collections_cache_ttl: int = Field(default=60, ge=1)  # –ú–∏–Ω–∏–º—É–º 1 —Å–µ–∫—É–Ω–¥–∞
    
    # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ GGUF –º–æ–¥–µ–ª–µ–π
    gguf_model_n_ctx: int = Field(default=4096, ge=1)  # –ú–∏–Ω–∏–º—É–º 1
    
    # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –ø–æ–∏—Å–∫–∞
    search_default_k: int = Field(default=5, ge=1)  # –ú–∏–Ω–∏–º—É–º 1 —Ä–µ–∑—É–ª—å—Ç–∞—Ç
    use_hybrid: bool = False  # –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –≥–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–∏—Å–∫ (dense + sparse)
    hybrid_alpha: float = Field(default=0.7, ge=0.0, le=1.0)  # –í–µ—Å –¥–ª—è dense –≤ –≥–∏–±—Ä–∏–¥–Ω–æ–º –ø–æ–∏—Å–∫–µ
    
    # BM25 Native Sparse Configuration
    use_bm25: bool = True  # Enable native BM25 via sparse vectors with IDF
    sparse_vector_name: str = "bm25_text"  # Name of the sparse vector field
    bm25_tokenizer: str = "word"  # Tokenizer type: word, whitespace, prefix
    bm25_min_token_len: int = 2
    
    # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ reranker
    reranker_enabled: bool = False
    reranker_model: str = "cross-encoder/ms-marco-MiniLM-L-6-v2"
    reranker_top_k: int = Field(default=5, ge=1)  # –ú–∏–Ω–∏–º—É–º 1 —Ä–µ–∑—É–ª—å—Ç–∞—Ç
    
    # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ Qdrant
    qdrant_retry_attempts: int = Field(default=3, ge=1)  # –ú–∏–Ω–∏–º—É–º 1 –ø–æ–ø—ã—Ç–∫–∞
    qdrant_retry_wait_time: int = Field(default=2, ge=1)  # –ú–∏–Ω–∏–º—É–º 1 —Å–µ–∫—É–Ω–¥–∞
    
    # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
    indexing_default_batch_size: int = Field(default=32, ge=1)  # –ú–∏–Ω–∏–º—É–º 1
    
    # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
    enable_metadata_extraction: bool = True
    metadata_custom_fields: Dict[str, Any] = Field(default={})
    metadata_extract_pdf: bool = True
    metadata_extract_image: bool = True
    metadata_extract_docx: bool = True
    
    # –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ (–¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏)
    config_file_path: str = str(resource_path("config/config.json"))
    
    # –¢–æ–∫–µ–Ω HuggingFace –¥–ª—è –ø—Ä–∏–≤–∞—Ç–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π
    huggingface_token: Optional[str] = Field(default=None, exclude=True)

    def save_to_file(self, file_path: Optional[str] = None) -> None:
        """–°–æ—Ö—Ä–∞–Ω–∏—Ç—å –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –≤ JSON —Ñ–∞–π–ª.
        
        Args:
            file_path: –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è config_file_path)
        """
        import json
        if file_path is None:
            file_path = self.config_file_path
            
        config_dict = self.model_dump()
        with open(file_path, "w", encoding="utf-8") as f:
            json.dump(config_dict, f, indent=4, ensure_ascii=False)
====================================================================================================
==================== –ö–û–ù–ï–¶ –§–ê–ô–õ–ê: config\settings_model.py ====================



==================== –ù–ê–ß–ê–õ–û –§–ê–ô–õ–ê: core\converting\file_processor.py ====================
# –§–∞–π–ª: core\converting\file_processor.py
====================================================================================================
"""Module for processing files and directories with advanced features."""

import logging
import re
from dataclasses import dataclass
from enum import Enum
from pathlib import Path
from typing import Callable, Dict, List, Optional, Set

import pandas as pd

logger = logging.getLogger(__name__)


class FileType(Enum):
    """Enumeration of supported file types."""
    PDF = "pdf"
    DJVU = "djvu"
    IMAGE = "image"
    DOCUMENT = "document"
    PRESENTATION = "presentation"
    SPREADSHEET = "spreadsheet"
    HTML = "html"
    UNKNOWN = "unknown"


@dataclass
class FileProcessingResult:
    """Data class for file processing results."""
    file_path: Path
    output_path: Optional[Path]
    success: bool
    error_message: Optional[str]
    processing_time: float = 0.0
    metadata: Dict = None


class FileProcessor:
    """Advanced file processor with scanning, grouping, and conversion capabilities."""
    
    # Supported file extensions by type
    FILE_EXTENSIONS = {
        FileType.PDF: {'.pdf'},
        FileType.DJVU: {'.djvu'},
        FileType.IMAGE: {'.jpg', '.jpeg', '.png', '.bmp', '.tiff', '.gif'},
        FileType.DOCUMENT: {'.docx', '.doc', '.txt', '.md', '.rtf'},
        FileType.PRESENTATION: {'.pptx', '.ppt'},
        FileType.SPREADSHEET: {'.xlsx', '.xls', '.csv'},
        FileType.HTML: {'.html', '.htm'}
    }
    
    def __init__(self, progress_callback: Optional[Callable[[int, int], None]] = None):
        """Initialize FileProcessor.
        
        Args:
            progress_callback: Optional callback function for progress reporting
        """
        self.progress_callback = progress_callback
        self.supported_extensions = self._get_all_supported_extensions()
        
    def _get_all_supported_extensions(self) -> Set[str]:
        """Get all supported file extensions."""
        extensions = set()
        for ext_set in self.FILE_EXTENSIONS.values():
            extensions.update(ext_set)
        return extensions
    
    def scan_directory(
        self, 
        directory: Path, 
        recursive: bool = True
    ) -> Dict[FileType, List[Path]]:
        """Scan directory and identify files by type using pandas DataFrame.
        
        Args:
            directory: Directory to scan
            recursive: Whether to scan subdirectories recursively
            
        Returns:
            Dict[FileType, List[Path]]: Files grouped by type
            
        Raises:
            FileNotFoundError: If directory doesn't exist
            PermissionError: If no permission to access directory
        """
        if not directory.exists():
            raise FileNotFoundError(f"Directory not found: {directory}")
            
        if not directory.is_dir():
            raise ValueError(f"Path is not a directory: {directory}")
            
        try:
            # Use pathlib.Path.rglob for file scanning
            if recursive:
                all_files = list(directory.rglob("*"))
            else:
                all_files = list(directory.iterdir())
                
            # Filter only files (not directories)
            files = [f for f in all_files if f.is_file()]
            
            # Create DataFrame for efficient processing
            if files:
                df = pd.DataFrame({
                    'path': files,
                    'name': [f.name for f in files],
                    'extension': [f.suffix.lower() for f in files],
                    'size': [f.stat().st_size for f in files]
                })
                
                # Identify file types
                def identify_file_type(ext):
                    for file_type, extensions in self.FILE_EXTENSIONS.items():
                        if ext in extensions:
                            return file_type
                    return FileType.UNKNOWN
                
                df['file_type'] = df['extension'].apply(identify_file_type)
                
                # Group files by type
                files_by_type = {}
                for file_type in FileType:
                    file_paths = df[df['file_type'] == file_type]['path'].tolist()
                    files_by_type[file_type] = file_paths
            else:
                # Initialize empty lists for all file types
                files_by_type = {file_type: [] for file_type in FileType}
                
            # Log results
            total_files = sum(len(files) for files in files_by_type.values())
            logger.info(f"Scanned {total_files} files in {directory}")
            
            return files_by_type
            
        except PermissionError:
            raise PermissionError(f"No permission to access directory: {directory}")
        except Exception as e:
                logger.exception(f"Error scanning directory {directory}: {e}")
                raise
    
    def process_files(
        self, 
        files_by_type: Dict[FileType, List[Path]], 
        output_dir: Path,
        converter_manager=None  # Will be imported when needed
    ) -> List[FileProcessingResult]:
        """Process files of different types using appropriate converters.
        
        Args:
            files_by_type: Files grouped by type
            output_dir: Output directory for processed files
            converter_manager: Converter manager instance (optional)
            
        Returns:
            List[FileProcessingResult]: Processing results
        """
        results = []
        total_files = sum(len(files) for files in files_by_type.values())
        processed_count = 0
        
        # Create output directory if it doesn't exist
        output_dir.mkdir(parents=True, exist_ok=True)
        
        # Import converter manager if not provided
        if converter_manager is None:
            from core.converting.manager import ConverterManager
            converter_manager = ConverterManager()
        
        # Process each file type
        for file_type, files in files_by_type.items():
            if not files:
                continue
                
            logger.info(f"Processing {len(files)} {file_type.value} files")
            
            for file_path in files:
                try:
                    # Report progress
                    processed_count += 1
                    if self.progress_callback:
                        self.progress_callback(processed_count, total_files)
                    
                    # Process file based on type
                    output_path = self._process_file_by_type(
                        file_path, 
                        file_type, 
                        output_dir, 
                        converter_manager
                    )
                    
                    # Check if processing was successful
                    if output_path is not None:
                        results.append(FileProcessingResult(
                            file_path=file_path,
                            output_path=output_path,
                            success=True,
                            error_message=None,
                            metadata={"file_type": file_type.value}
                        ))
                    else:
                        results.append(FileProcessingResult(
                            file_path=file_path,
                            output_path=None,
                            success=False,
                            error_message="File processing failed (no output path returned)",
                            metadata={"file_type": file_type.value}
                        ))
                    
                except Exception as e:
                    logger.error(f"Error processing {file_path}: {e}")
                    results.append(FileProcessingResult(
                        file_path=file_path,
                        output_path=None,
                        success=False,
                        error_message=str(e),
                        metadata={"file_type": file_type.value}
                    ))
        
        return results
    
    def _process_file_by_type(
        self, 
        file_path: Path, 
        file_type: FileType, 
        output_dir: Path,
        converter_manager
    ) -> Optional[Path]:
        """Process file based on its type.
        
        Args:
            file_path: Path to file
            file_type: File type
            output_dir: Output directory
            converter_manager: Converter manager instance
            
        Returns:
            Optional[Path]: Output file path or None if failed
        """
        try:
            # For PDF, DJVU, and image files, we might use MinerU or other specialized tools
            if file_type in [FileType.PDF, FileType.DJVU, FileType.IMAGE]:
                # For now, delegate to existing conversion logic
                # In the future, we can implement more sophisticated processing
                return self._process_with_mineru(file_path, file_type, output_dir)
            else:
                # Use converter manager for other file types
                converted_files = converter_manager.convert_file(file_path, output_dir)
                return converted_files[0] if converted_files else None
                
        except Exception as e:
            logger.error(f"Error processing {file_path} as {file_type.value}: {e}")
            raise
    
    def _process_with_mineru(
        self, 
        file_path: Path, 
        file_type: FileType, 
        output_dir: Path
    ) -> Optional[Path]:
        """Process file with MinerU service.
        
        Args:
            file_path: Path to file
            file_type: File type
            output_dir: Output directory
            
        Returns:
            Optional[Path]: Output file path or None if failed
        """
        try:
            # Import the MinerU service
            from core.converting.mineru_service import MinerUService
            
            # Create MinerU service instance
            mineru_service = MinerUService()
            
            # Check if MinerU is available
            if not mineru_service.is_mineru_available():
                logger.error("MinerU is not available. Please install the 'mineru' package.")
                return None
            
            # Get configuration
            from config.config_manager import ConfigManager
            config_manager = ConfigManager.get_instance()
            config = config_manager.get()
            
            # Create temporary input directory for this file
            temp_input_dir = output_dir / "temp_input"
            temp_input_dir.mkdir(exist_ok=True)
            
            # Copy file to temporary input directory
            temp_file_path = temp_input_dir / file_path.name
            with open(file_path, 'rb') as src, open(temp_file_path, 'wb') as dst:
                dst.write(src.read())
            
            # Process with MinerU service
            success = mineru_service.process_pdfs(
                input_pdf_dir=str(temp_input_dir),
                output_md_dir=str(output_dir),
                enable_formula_parsing=config.mineru_enable_formula_parsing,
                enable_table_parsing=config.mineru_enable_table_parsing,
                model_source=config.mineru_model_source,
                models_dir=config.mineru_models_dir if config.mineru_models_dir else None,
                backend=config.mineru_backend,
                method=config.mineru_method,
                lang=config.mineru_lang,
                sglang_url=config.mineru_sglang_url if config.mineru_sglang_url else None
            )
            
            # Clean up temporary input directory
            import shutil
            shutil.rmtree(temp_input_dir, ignore_errors=True)
            
            if success:
                # Return the output file path
                output_file = output_dir / f"{file_path.stem}.md"
                return output_file if output_file.exists() else None
            else:
                return None
            
        except Exception as e:
            logger.error(f"Error processing {file_path} with MinerU service: {e}")
            # Clean up temporary input directory if it exists
            temp_input_dir = output_dir / "temp_input"
            if temp_input_dir.exists():
                import shutil
                shutil.rmtree(temp_input_dir, ignore_errors=True)
            return None
    
    def get_statistics(self, files_by_type: Dict[FileType, List[Path]]) -> Dict[str, int]:
        """Get statistics about files by type using pandas DataFrame.
        
        Args:
            files_by_type: Files grouped by type
            
        Returns:
            Dict[str, int]: Statistics
        """
        # Create DataFrame for efficient statistics calculation
        data = []
        total = 0
        
        for file_type, files in files_by_type.items():
            count = len(files)
            data.append({'file_type': file_type.value, 'count': count})
            total += count
            
        # Create DataFrame
        df = pd.DataFrame(data)
        
        # Convert to dictionary
        stats = df.set_index('file_type')['count'].to_dict()
        stats['total'] = total
        
        return stats
====================================================================================================
==================== –ö–û–ù–ï–¶ –§–ê–ô–õ–ê: core\converting\file_processor.py ====================



==================== –ù–ê–ß–ê–õ–û –§–ê–ô–õ–ê: core\converting\manager.py ====================
# –§–∞–π–ª: core\converting\manager.py
====================================================================================================
"""Module for managing format converters."""

import importlib
import logging
import threading
from pathlib import Path
from typing import Dict, List

from .converters.base import BaseConverter

logger = logging.getLogger(__name__)


class ConverterManager:
    """Manager for handling different format converters."""
    
    _instance = None
    _lock = threading.Lock()
    
    def __new__(cls):
        """Thread-safe singleton implementation."""
        if cls._instance is None:
            with cls._lock:
                # Double-checked locking pattern
                if cls._instance is None:
                    cls._instance = super(ConverterManager, cls).__new__(cls)
        return cls._instance
    
    def __init__(self):
        """Initialize the converter manager with available converters."""
        # Check if already initialized to prevent re-initialization
        if hasattr(self, '_initialized'):
            return
            
        self.converters: Dict[str, BaseConverter] = {}
        self._load_converters()
        self._initialized = True
    
    def _load_converters(self):
        """Load available converters dynamically."""
        # Use unstructured converter for all supported formats
        try:
            from .converters.unstructured_converter import UnstructuredConverter
            converter = UnstructuredConverter()
            self.converters['unstructured'] = converter
            logger.info("Loaded unstructured converter for all supported formats")
        except ImportError as e:
            logger.warning(f"Could not load unstructured converter: {e}")
        except Exception as e:
            logger.error(f"Error loading unstructured converter: {e}")
    
    def convert_file(self, file_path: Path, output_dir: Path) -> List[Path]:
        """
        Convert a file to Markdown format using the appropriate converter.
        
        Args:
            file_path (Path): Path to the input file
            output_dir (Path): Directory to save the output files
            
        Returns:
            List[Path]: List of paths to the converted Markdown files
        """
        if not file_path.exists():
            raise FileNotFoundError(f"File not found: {file_path}")
            
        # Use unstructured converter for all files
        if 'unstructured' in self.converters:
            converter = self.converters['unstructured']
            try:
                return converter.convert(file_path, output_dir)
            except Exception as e:
                logger.error(f"Error converting {file_path.name} with {converter.__class__.__name__}: {e}")
                raise  # Re-raise the exception to be handled by the caller
        else:
            raise ValueError(f"No converter available for file: {file_path}")
    
    def get_supported_extensions(self) -> List[str]:
        """
        Get list of all supported file extensions.
        
        Returns:
            List[str]: List of supported extensions
        """
        # Get supported extensions from unstructured converter
        if 'unstructured' in self.converters:
            converter = self.converters['unstructured']
            if hasattr(converter, 'supported_extensions'):
                return converter.supported_extensions()
        
        return []
====================================================================================================
==================== –ö–û–ù–ï–¶ –§–ê–ô–õ–ê: core\converting\manager.py ====================



==================== –ù–ê–ß–ê–õ–û –§–ê–ô–õ–ê: core\converting\mineru_service.py ====================
# –§–∞–π–ª: core\converting\mineru_service.py
====================================================================================================
"""Service for handling MinerU operations."""

import logging
from pathlib import Path
from typing import Optional

logger = logging.getLogger(__name__)


class MinerUService:
    """Service for handling MinerU operations."""
    
    def __init__(self):
        """Initialize the MinerU service."""
        pass
    
    def process_pdfs(
        self,
        input_pdf_dir: str,
        output_md_dir: str,
        enable_formula_parsing: bool = False,
        enable_table_parsing: bool = False,
        model_source: str = "huggingface",
        models_dir: Optional[str] = None,
        backend: str = "pipeline",
        method: str = "auto",
        lang: str = "east_slavic",
        sglang_url: Optional[str] = None
    ) -> bool:
        """Process PDF files using MinerU.
        
        Args:
            input_pdf_dir (str): Path to directory with PDF files.
            output_md_dir (str): Path to save results (markdown + images).
            enable_formula_parsing (bool): Enable formula parsing (default: False).
            enable_table_parsing (bool): Enable table parsing (default: False).
            model_source (str): Model source: 'huggingface', 'modelscope' or 'local'.
            models_dir (Optional[str]): Path to local models (only for model_source='local').
            backend (str): Processing backend: 'pipeline', 'vlm-transformers' or 'vlm-sglang-client'.
            method (str): Parsing method: 'auto', 'txt' or 'ocr'.
            lang (str): OCR language (default: 'east_slavic' for East Slavic languages).
            sglang_url (Optional[str]): Sglang server URL (only for backend='vlm-sglang-client').
            
        Returns:
            bool: True if all files were processed successfully, False otherwise.
        """
        try:
            # Import the MinerU processing function
            from core.converting.pdf_to_md_chunker import process_pdfs_and_chunk
            
            # Process PDFs and return the result
            return process_pdfs_and_chunk(
                input_pdf_dir=input_pdf_dir,
                output_md_dir=output_md_dir,
                enable_formula_parsing=enable_formula_parsing,
                enable_table_parsing=enable_table_parsing,
                model_source=model_source,
                models_dir=models_dir,
                backend=backend,
                method=method,
                lang=lang,
                sglang_url=sglang_url
            )
            
        except Exception as e:
            logger.error(f"Error processing PDFs with MinerU: {e}")
            return False
    
    def is_mineru_available(self) -> bool:
        """Check if MinerU is available.
        
        Returns:
            bool: True if MinerU is available, False otherwise
        """
        try:
            import subprocess
            result = subprocess.run(["mineru", "--help"], 
                                  capture_output=True, text=True, timeout=30)
            return result.returncode == 0
        except (subprocess.TimeoutExpired, FileNotFoundError, Exception):
            return False
====================================================================================================
==================== –ö–û–ù–ï–¶ –§–ê–ô–õ–ê: core\converting\mineru_service.py ====================



==================== –ù–ê–ß–ê–õ–û –§–ê–ô–õ–ê: core\converting\multi_format_converter.py ====================
# –§–∞–π–ª: core\converting\multi_format_converter.py
====================================================================================================
"""Module for converting various file formats to Markdown."""

import logging
from pathlib import Path
from typing import List, Tuple

from config.config_manager import ConfigManager

# Import the new file processor
from .file_processor import FileProcessor

# Import the new converter manager
from .manager import ConverterManager

# Import the PDF converter

logger = logging.getLogger(__name__)

# Get singleton instance of ConfigManager
config_manager = ConfigManager.get_instance()

# Get singleton instance of ConverterManager
_converter_manager = ConverterManager()


def convert_files_to_md(input_dir: str, output_dir: str) -> Tuple[bool, str]:
    """
    Convert files of various formats to Markdown using the new FileProcessor.
    
    Args:
        input_dir (str): Directory containing files to convert
        output_dir (str): Directory to save converted Markdown files
        
    Returns:
        Tuple[bool, str]: (success, status message)
    """
    try:
        input_path = Path(input_dir)
        output_path = Path(output_dir)
        
        if not input_path.exists() or not input_path.is_dir():
            raise FileNotFoundError(f"Input directory not found: {input_dir}")
            
        # Create output directory if it doesn't exist
        output_path.mkdir(parents=True, exist_ok=True)
        
        # Initialize file processor
        file_processor = FileProcessor()
        
        # Scan directory for files
        files_by_type = file_processor.scan_directory(input_path, recursive=True)
        
        # Get statistics
        stats = file_processor.get_statistics(files_by_type)
        logger.info(f"Found files: {stats}")
        
        # Process files
        results = file_processor.process_files(files_by_type, output_path, _converter_manager)
        
        # Count successful conversions
        successful_conversions = sum(1 for result in results if result.success)
        failed_conversions = len(results) - successful_conversions
        
        if failed_conversions > 0:
            logger.warning(f"{failed_conversions} files failed to convert")
            
        logger.info(f"Successfully converted {successful_conversions} files to Markdown")
        return True, f"converted_{successful_conversions}_files"
        
    except Exception as e:
        logger.exception(f"Error converting files: {e}")
        return False, f"conversion_error: {str(e)}"


def get_supported_formats() -> List[str]:
    """
    Get list of supported file formats.
    
    Returns:
        List[str]: List of supported formats
    """
    # Start with formats handled by MinerU
    formats = ['.pdf', '.djvu', '.jpg', '.jpeg', '.png']
    
    # Add formats supported by our converters
    formats.extend(_converter_manager.get_supported_extensions())
    
    # Remove duplicates and return
    return list(set(formats))
====================================================================================================
==================== –ö–û–ù–ï–¶ –§–ê–ô–õ–ê: core\converting\multi_format_converter.py ====================



==================== –ù–ê–ß–ê–õ–û –§–ê–ô–õ–ê: core\converting\pdf_to_md_chunker.py ====================
# –§–∞–π–ª: core\converting\pdf_to_md_chunker.py
====================================================================================================
"""
–ú–æ–¥—É–ª—å –¥–ª—è –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è PDF –≤ Markdown —Å –ø–æ–º–æ—â—å—é —É—Ç–∏–ª–∏—Ç—ã MinerU.

–û—Å–Ω–æ–≤–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏:
- –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ—Ç PDF —Ñ–∞–π–ª—ã –≤ Markdown —Ñ–æ—Ä–º–∞—Ç
- –°–æ—Ö—Ä–∞–Ω—è–µ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –≤ –æ—Ç–¥–µ–ª—å–Ω—É—é –ø–∞–ø–∫—É
- –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç —Ä–∞–∑–ª–∏—á–Ω—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –ø–∞—Ä—Å–∏–Ω–≥–∞ (—Ñ–æ—Ä–º—É–ª—ã, —Ç–∞–±–ª–∏—Ü—ã)
- –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç —Ñ–∞–π–ª—ã —á–µ—Ä–µ–∑ subprocess –≤—ã–∑–æ–≤ mineru

–°—Ç—Ä—É–∫—Ç—É—Ä–∞ –≤—ã—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö:
output_md_dir/
  ‚îú‚îÄ‚îÄ filename/
  ‚îÇ   ‚îú‚îÄ‚îÄ filename.md       # –û—Å–Ω–æ–≤–Ω–æ–π markdown —Ñ–∞–π–ª
  ‚îÇ   ‚îî‚îÄ‚îÄ images/           # –ü–∞–ø–∫–∞ —Å –∏–∑–≤–ª–µ—á–µ–Ω–Ω—ã–º–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏
"""

import logging
import os
import shutil
import subprocess
from pathlib import Path
from typing import Any, Dict, Optional

logger = logging.getLogger(__name__)

def setup_env(model_source: str, models_dir: Optional[str], enable_formula_parsing: bool, enable_table_parsing: bool) -> dict:
    """
    –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ—Ç –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è –¥–ª—è MinerU.
    
    Args:
        model_source (str): –ò—Å—Ç–æ—á–Ω–∏–∫ –º–æ–¥–µ–ª–µ–π.
        models_dir (Optional[str]): –ü—É—Ç—å –∫ –ª–æ–∫–∞–ª—å–Ω—ã–º –º–æ–¥–µ–ª—è–º.
        enable_formula_parsing (bool): –í–∫–ª—é—á–∏—Ç—å –ø–∞—Ä—Å–∏–Ω–≥ —Ñ–æ—Ä–º—É–ª.
        enable_table_parsing (bool): –í–∫–ª—é—á–∏—Ç—å –ø–∞—Ä—Å–∏–Ω–≥ —Ç–∞–±–ª–∏—Ü.
        
    Returns:
        dict: –ù–∞—Å—Ç—Ä–æ–µ–Ω–Ω—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è.
    """
    env = os.environ.copy()
    env["MINERU_MODEL_SOURCE"] = model_source
    if models_dir and model_source == "local":
        env["MINERU_MODELS_DIR"] = models_dir

    # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ —Ñ–ª–∞–≥–æ–≤ –ø–∞—Ä—Å–∏–Ω–≥–∞ —Ñ–æ—Ä–º—É–ª –∏ —Ç–∞–±–ª–∏—Ü —á–µ—Ä–µ–∑ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è
    env["MINERU_ENABLE_FORMULA_PARSING"] = "true" if enable_formula_parsing else "false"
    env["MINERU_ENABLE_TABLE_PARSING"] = "true" if enable_table_parsing else "false"
    
    return env


def run_subprocess(pdf_file: Path, temp_output_dir: Path, backend: str, method: str, lang: str, sglang_url: Optional[str], env: dict) -> subprocess.CompletedProcess:
    """
    –ó–∞–ø—É—Å–∫–∞–µ—Ç subprocess –≤—ã–∑–æ–≤ mineru.
    
    Args:
        pdf_file (Path): –ü—É—Ç—å –∫ PDF —Ñ–∞–π–ª—É.
        temp_output_dir (Path): –í—Ä–µ–º–µ–Ω–Ω–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è –≤—ã–≤–æ–¥–∞.
        backend (str): –ë—ç–∫–µ–Ω–¥ –æ–±—Ä–∞–±–æ—Ç–∫–∏.
        method (str): –ú–µ—Ç–æ–¥ –ø–∞—Ä—Å–∏–Ω–≥–∞.
        lang (str): –Ø–∑—ã–∫ –¥–ª—è OCR.
        sglang_url (Optional[str]): URL —Å–µ—Ä–≤–µ—Ä–∞ sglang.
        env (dict): –ü–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è.
        
    Returns:
        subprocess.CompletedProcess: –†–µ–∑—É–ª—å—Ç–∞—Ç –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è subprocess.
    """
    logger.info(f"–û–±—Ä–∞–±–æ—Ç–∫–∞ —Ñ–∞–π–ª–∞ (Subprocess): {pdf_file.name}")

    # –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–æ–º–∞–Ω–¥—ã –¥–ª—è –≤—ã–∑–æ–≤–∞ mineru —Å —É–∫–∞–∑–∞–Ω–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
    cmd = [
        "mineru",
        "-p", str(pdf_file),
        "-o", str(temp_output_dir), # –í—ã–≤–æ–¥–∏–º –≤–æ –≤—Ä–µ–º–µ–Ω–Ω—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é
        "-b", backend,
        "--output-format", "md" # –£–∫–∞–∑—ã–≤–∞–µ–º —Ñ–æ—Ä–º–∞—Ç –≤—ã–≤–æ–¥–∞
    ]
    
    if method != "auto":
         cmd.extend(["--method", method])
    
    if lang and lang.lower() not in ['auto', 'none']:
         cmd.extend(["--lang", lang])
         
    if backend == "vlm-sglang-client" and sglang_url:
        cmd.extend(["-u", sglang_url])

    logger.info(f"  -> –í—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è –∫–æ–º–∞–Ω–¥–∞: {' '.join(cmd)}")

    return subprocess.run(
        cmd,
        env=env,
        capture_output=True,
        text=True,
        timeout=600  # –§–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ç–∞–π–º–∞—É—Ç 10 –º–∏–Ω—É—Ç
    )

def postprocess_output(pdf_file: Path, pdf_stem: str, temp_output_dir: Path, final_output_dir: Path, output_root_path: Path) -> bool:
    """
    –ü–æ—Å—Ç-–æ–±—Ä–∞–±–æ—Ç–∫–∞ –≤—ã–≤–æ–¥–∞ mineru.
    
    Args:
        pdf_file (Path): –ü—É—Ç—å –∫ PDF —Ñ–∞–π–ª—É.
        pdf_stem (str): –ò–º—è PDF —Ñ–∞–π–ª–∞ –±–µ–∑ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è.
        temp_output_dir (Path): –í—Ä–µ–º–µ–Ω–Ω–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏.
        final_output_dir (Path): –§–∏–Ω–∞–ª—å–Ω–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤.
        output_root_path (Path): –ö–æ—Ä–Ω–µ–≤–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è –≤—ã–≤–æ–¥–∞.
        
    Returns:
        bool: –£—Å–ø–µ—à–Ω–æ—Å—Ç—å –ø–æ—Å—Ç-–æ–±—Ä–∞–±–æ—Ç–∫–∏.
    """
    # –õ–æ–≥–∏—Ä—É–µ–º —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ –≤—Ä–µ–º–µ–Ω–Ω–æ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –¥–ª—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏
    logger.debug(f"–°–æ–¥–µ—Ä–∂–∏–º–æ–µ temp_output_dir ({temp_output_dir}):")
    if temp_output_dir.exists():
        for item in temp_output_dir.rglob("*"):
            logger.debug(f"  - {item.relative_to(temp_output_dir)} ({'DIR' if item.is_dir() else 'FILE'})")
    else:
        logger.debug(f"  -> temp_output_dir –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç.")

    # –ü–æ—Å—Ç-–æ–±—Ä–∞–±–æ—Ç–∫–∞: –ø–µ—Ä–µ–Ω–æ—Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∏–∑ –≤—Ä–µ–º–µ–Ω–Ω–æ–π –ø–∞–ø–∫–∏ –≤ —Ñ–∏–Ω–∞–ª—å–Ω—É—é
    # MinerU —Å–æ–∑–¥–∞–µ—Ç –≤–ª–æ–∂–µ–Ω–Ω—ã–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –≤–∏–¥–∞ temp_output_dir/{pdf_stem}/auto/
    mineru_output_dir = temp_output_dir / pdf_stem / "auto"
    
    # –ï—Å–ª–∏ —Ç–∞–∫–æ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –Ω–µ—Ç, –∏—â–µ–º .md —Ñ–∞–π–ª –≤ temp_output_dir –∏–ª–∏ –ø–æ–¥–ø–∞–ø–∫–∞—Ö
    if not mineru_output_dir.exists():
        md_files = list(temp_output_dir.rglob("*.md"))
        if not md_files:
            logger.warning(f"  -> –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ: –ù–µ –Ω–∞–π–¥–µ–Ω .md —Ñ–∞–π–ª –ø–æ—Å–ª–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏ {pdf_file.name}.")
            return False
            
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é —Å –ø–µ—Ä–≤—ã–º –Ω–∞–π–¥–µ–Ω–Ω—ã–º .md —Ñ–∞–π–ª–æ–º
        mineru_output_dir = md_files[0].parent
        logger.info(f"  -> –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è —Å –Ω–∞–π–¥–µ–Ω–Ω—ã–º —Ñ–∞–π–ª–æ–º: {mineru_output_dir.relative_to(temp_output_dir)}")
    else:
        logger.info(f"  -> –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è MinerU: {mineru_output_dir.relative_to(temp_output_dir)}")

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ .md —Ñ–∞–π–ª–∞
    target_md_file = mineru_output_dir / f"{pdf_stem}.md"
    if not target_md_file.exists():
        # –ò—â–µ–º –ª—é–±–æ–π .md —Ñ–∞–π–ª –≤ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
        md_files = list(mineru_output_dir.glob("*.md"))
        if not md_files:
            logger.warning(f"  -> –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ: –ù–µ –Ω–∞–π–¥–µ–Ω .md —Ñ–∞–π–ª –≤ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ {mineru_output_dir}.")
            return False
        target_md_file = md_files[0]
        logger.info(f"  -> –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –Ω–∞–π–¥–µ–Ω–Ω—ã–π —Ñ–∞–π–ª: {target_md_file.name}")

    # –°–æ–∑–¥–∞–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—É—é –ø–∞–ø–∫—É –¥–ª—è —ç—Ç–æ–≥–æ PDF
    final_output_dir.mkdir(parents=True, exist_ok=True)
    
    # –ö–æ–ø–∏—Ä—É–µ–º .md —Ñ–∞–π–ª
    final_md_path = final_output_dir / f"{pdf_stem}.md"
    shutil.copy2(target_md_file, final_md_path)  # –ò—Å–ø–æ–ª—å–∑—É–µ–º copy2 –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
    logger.info(f"  -> .md —Ñ–∞–π–ª —Å–∫–æ–ø–∏—Ä–æ–≤–∞–Ω –≤: {final_md_path.relative_to(output_root_path)}")
    
    # –ö–æ–ø–∏—Ä—É–µ–º –ø–∞–ø–∫—É —Å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏, –µ—Å–ª–∏ –æ–Ω–∞ –µ—Å—Ç—å
    images_dir_in_temp = mineru_output_dir / "images"
    if images_dir_in_temp.exists() and images_dir_in_temp.is_dir():
        images_dir_final = final_output_dir / "images"
        if images_dir_final.exists():
            shutil.rmtree(images_dir_final)
        shutil.copytree(images_dir_in_temp, images_dir_final)
        logger.info(f"  -> –ü–∞–ø–∫–∞ 'images' —Å–∫–æ–ø–∏—Ä–æ–≤–∞–Ω–∞ –≤: {images_dir_final.relative_to(output_root_path)}")
        
    return True


def process_pdfs_and_chunk(
    input_pdf_dir: str,
    output_md_dir: str,
    enable_formula_parsing: bool = False,
    enable_table_parsing: bool = False,
    model_source: str = "huggingface",
    models_dir: Optional[str] = None,
    backend: str = "pipeline",
    method: str = "auto",
    lang: str = "east_slavic", # –ò–∑–º–µ–Ω–µ–Ω–æ –∑–Ω–∞—á–µ–Ω–∏–µ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
    sglang_url: Optional[str] = None,
    device: str = "cpu"
) -> bool:
    """
    –û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ PDF —Ñ–∞–π–ª–æ–≤.
    
    –î–ª—è –∫–∞–∂–¥–æ–≥–æ PDF —Ñ–∞–π–ª–∞ –≤ input_pdf_dir:
    1. –°–æ–∑–¥–∞–µ—Ç –≤—Ä–µ–º–µ–Ω–Ω—É—é —Ä–∞–±–æ—á—É—é –ø–∞–ø–∫—É
    2. –í—ã–∑—ã–≤–∞–µ—Ç mineru –¥–ª—è –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–∏ –≤ markdown
    3. –ü–µ—Ä–µ–Ω–æ—Å–∏—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ —Ñ–∏–Ω–∞–ª—å–Ω—É—é –ø–∞–ø–∫—É
    4. –û—á–∏—â–∞–µ—Ç –≤—Ä–µ–º–µ–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã

    Args:
        input_pdf_dir (str): –ü—É—Ç—å –∫ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ —Å PDF —Ñ–∞–π–ª–∞–º–∏.
        output_md_dir (str): –ü—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ (markdown + –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è).
        enable_formula_parsing (bool): –í–∫–ª—é—á–∏—Ç—å –ø–∞—Ä—Å–∏–Ω–≥ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö —Ñ–æ—Ä–º—É–ª (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é False).
        enable_table_parsing (bool): –í–∫–ª—é—á–∏—Ç—å –ø–∞—Ä—Å–∏–Ω–≥ —Ç–∞–±–ª–∏—Ü (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é False).
        model_source (str): –ò—Å—Ç–æ—á–Ω–∏–∫ –º–æ–¥–µ–ª–µ–π: 'huggingface', 'modelscope' –∏–ª–∏ 'local'.
        models_dir (Optional[str]): –ü—É—Ç—å –∫ –ª–æ–∫–∞–ª—å–Ω—ã–º –º–æ–¥–µ–ª—è–º (—Ç–æ–ª—å–∫–æ –¥–ª—è model_source='local').
        backend (str): –ë—ç–∫–µ–Ω–¥ –æ–±—Ä–∞–±–æ—Ç–∫–∏: 'pipeline', 'vlm-transformers' –∏–ª–∏ 'vlm-sglang-client'.
        method (str): –ú–µ—Ç–æ–¥ –ø–∞—Ä—Å–∏–Ω–≥–∞: 'auto' (–æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏), 'txt' (—Ç–µ–∫—Å—Ç–æ–≤—ã–π —Å–ª–æ–π) –∏–ª–∏ 'ocr' (—Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ).
        lang (str): –Ø–∑—ã–∫ –¥–ª—è OCR (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 'east_slavic' –¥–ª—è –≤–æ—Å—Ç–æ—á–Ω–æ—Å–ª–∞–≤—è–Ω—Å–∫–∏—Ö —è–∑—ã–∫–æ–≤).
        sglang_url (Optional[str]): URL —Å–µ—Ä–≤–µ—Ä–∞ sglang (—Ç–æ–ª—å–∫–æ –¥–ª—è backend='vlm-sglang-client').
        device (str): –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏: 'cpu' –∏–ª–∏ 'cuda'.

    Returns:
        bool: True –µ—Å–ª–∏ –≤—Å–µ —Ñ–∞–π–ª—ã –±—ã–ª–∏ —É—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω—ã, False –µ—Å–ª–∏ —Ö–æ—Ç—è –±—ã –æ–¥–∏–Ω —Ñ–∞–π–ª –Ω–µ –±—ã–ª –æ–±—Ä–∞–±–æ—Ç–∞–Ω.
        
    Raises:
        FileNotFoundError: –ï—Å–ª–∏ –≤—Ö–æ–¥–Ω–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç.
    """
    input_path = Path(input_pdf_dir)
    output_root_path = Path(output_md_dir)
    output_root_path.mkdir(parents=True, exist_ok=True)

    if not input_path.exists() or not input_path.is_dir():
        raise FileNotFoundError(f"–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è —Å PDF –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {input_pdf_dir}")

    pdf_files = list(input_path.glob("*.pdf"))
    if not pdf_files:
        logger.info(f"–í –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ {input_pdf_dir} –Ω–µ –Ω–∞–π–¥–µ–Ω–æ PDF —Ñ–∞–π–ª–æ–≤.")
        return True # –ù–µ—Ç —Ñ–∞–π–ª–æ–≤ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ - —Å—á–∏—Ç–∞–µ–º —É—Å–ø–µ—à–Ω—ã–º

    logger.info(f"–ù–∞–π–¥–µ–Ω–æ {len(pdf_files)} PDF —Ñ–∞–π–ª–æ–≤ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏.")
    logger.info("–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –º–µ—Ç–æ–¥ –æ–±—Ä–∞–±–æ—Ç–∫–∏: Subprocess (–≤—ã–∑–æ–≤ –∫–æ–º–∞–Ω–¥—ã)")

    # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è –¥–ª—è MinerU
    env = setup_env(model_source, models_dir, enable_formula_parsing, enable_table_parsing)
    
    # –§–ª–∞–≥ –¥–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è —É—Å–ø–µ—à–Ω–æ—Å—Ç–∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –≤—Å–µ—Ö —Ñ–∞–π–ª–æ–≤
    all_successful = True

    for pdf_file in pdf_files:
        temp_output_dir = None
        try:
            pdf_stem = pdf_file.stem
            logger.info(f"–û–±—Ä–∞–±–æ—Ç–∫–∞ —Ñ–∞–π–ª–∞ (Subprocess): {pdf_file.name}")

            # –°–æ–∑–¥–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –¥–ª—è –≤—ã–≤–æ–¥–∞ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ —Ñ–∞–π–ª–∞
            temp_output_dir = output_root_path / f"temp_{pdf_stem}"
            final_output_dir = output_root_path / pdf_stem
            
            # –û—á–∏—â–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é, –µ—Å–ª–∏ –æ–Ω–∞ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
            if temp_output_dir.exists():
                shutil.rmtree(temp_output_dir)
            temp_output_dir.mkdir(parents=True, exist_ok=True)
            
            # –û—á–∏—â–∞–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é, –µ—Å–ª–∏ –æ–Ω–∞ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
            if final_output_dir.exists():
                logger.info(f"  -> –£–¥–∞–ª–µ–Ω–∏–µ —Å—Ç–∞—Ä–æ–π –ø–∞–ø–∫–∏: {final_output_dir}")
                shutil.rmtree(final_output_dir)

            # –ó–∞–ø—É—Å–∫ subprocess
            result = run_subprocess(pdf_file, temp_output_dir, backend, method, lang, sglang_url, env)

            if result.returncode != 0:
                logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –≤—ã–∑–æ–≤–µ MinerU –¥–ª—è —Ñ–∞–π–ª–∞ {pdf_file.name}:")
                logger.error(f"–ö–æ–º–∞–Ω–¥–∞: {' '.join(['mineru', '-p', str(pdf_file), '-o', str(temp_output_dir), '-b', backend, '--output-format', 'md'])}")
                logger.error(f"–ö–æ–¥ –≤–æ–∑–≤—Ä–∞—Ç–∞: {result.returncode}")
                if result.stdout:
                    logger.error(f"Stdout: {result.stdout}")
                if result.stderr:
                    logger.error(f"Stderr: {result.stderr}") 
                all_successful = False
                continue

            # –ü–æ—Å—Ç-–æ–±—Ä–∞–±–æ—Ç–∫–∞ –≤—ã–≤–æ–¥–∞
            if not postprocess_output(pdf_file, pdf_stem, temp_output_dir, final_output_dir, output_root_path):
                all_successful = False
                continue

        except subprocess.TimeoutExpired:
            logger.exception(f"–û—à–∏–±–∫–∞: –¢–∞–π–º–∞—É—Ç –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ —Ñ–∞–π–ª–∞ {pdf_file.name}.")
            all_successful = False
            continue
        except FileNotFoundError as e:
            if "mineru" in str(e).lower() or "[winerror 2]" in str(e).lower() or "No such file or directory" in str(e):
                 logger.exception(f"–ö–æ–º–∞–Ω–¥–∞ 'mineru' –Ω–µ –Ω–∞–π–¥–µ–Ω–∞. –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ –ø–∞–∫–µ—Ç 'mineru' —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω: {e}")
            else:
                 logger.exception(f"–û—à–∏–±–∫–∞ FileNotFoundError –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ —Ñ–∞–π–ª–∞ {pdf_file.name}: {e}")
            all_successful = False
            continue
        except subprocess.CalledProcessError as e:
            logger.exception(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –≤—ã–∑–æ–≤–µ MinerU –¥–ª—è —Ñ–∞–π–ª–∞ {pdf_file.name}: {e}")
            all_successful = False
            continue
        except Exception as e:
            logger.exception(f"–ù–µ–æ–∂–∏–¥–∞–Ω–Ω–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ —Ñ–∞–π–ª–∞ {pdf_file.name}: {e}")
            all_successful = False
            continue
        finally:
            # –£–¥–∞–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –≤ –ª—é–±–æ–º —Å–ª—É—á–∞–µ
            if temp_output_dir and temp_output_dir.exists():
                shutil.rmtree(temp_output_dir)

    logger.info("–û–±—Ä–∞–±–æ—Ç–∫–∞ PDF —Ñ–∞–π–ª–æ–≤ –∑–∞–≤–µ—Ä—à–µ–Ω–∞.")
    return all_successful
====================================================================================================
==================== –ö–û–ù–ï–¶ –§–ê–ô–õ–ê: core\converting\pdf_to_md_chunker.py ====================



==================== –ù–ê–ß–ê–õ–û –§–ê–ô–õ–ê: core\converting\__init__.py ====================
# –§–∞–π–ª: core\converting\__init__.py
====================================================================================================
 

====================================================================================================
==================== –ö–û–ù–ï–¶ –§–ê–ô–õ–ê: core\converting\__init__.py ====================



==================== –ù–ê–ß–ê–õ–û –§–ê–ô–õ–ê: core\embedding\embeddings.py ====================
# –§–∞–π–ª: core\embedding\embeddings.py
====================================================================================================
"""–ú–æ–¥—É–ª—å –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä–∞–º–∏ (—ç–º–±–µ–¥–¥–µ—Ä–∞–º–∏)."""

import logging

from config.settings import Config
from core.embedding.embedding_manager import EmbeddingManager

# –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º GGUF —ç–º–±–µ–¥–¥–µ—Ä
try:
    from core.embedding.gguf_embeddings import GGUFEmbeddings
    GGUF_AVAILABLE = True
except ImportError:
    GGUF_AVAILABLE = False
    # raise ImportError("GGUF —ç–º–±–µ–¥–¥–µ—Ä –Ω–µ –¥–æ—Å—Ç—É–ø–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ ctransformers –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å GGUF –º–æ–¥–µ–ª—è–º–∏.")
    # –í—Ä–µ–º–µ–Ω–Ω–æ –æ—Å—Ç–∞–≤–ª—è–µ–º print –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏, –Ω–æ –≤ –±—É–¥—É—â–µ–º –ª—É—á—à–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å raise

logger = logging.getLogger(__name__)

# Get singleton instance of EmbeddingManager
embedding_manager = EmbeddingManager.get_instance()


def get_device(config_device: str) -> str:
    """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –¥–ª—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏ –∏–∑ –∫–æ–Ω—Ñ–∏–≥–∞."""
    return embedding_manager.get_device(config_device)


def get_search_device(search_device_param: str) -> str:
    """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –¥–ª—è –ø–æ–∏—Å–∫–∞ –∏–∑ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞ —Ñ–æ—Ä–º—ã."""
    return embedding_manager.get_search_device(search_device_param)


# --- –ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ HuggingFaceEmbeddings ---
def get_dense_embedder(config: Config, device=None):
    """–ü–æ–ª—É—á–∞–µ—Ç –∏–ª–∏ —Å–æ–∑–¥–∞–µ—Ç –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä —ç–º–±–µ–¥–¥–µ—Ä–∞."""
    return embedding_manager.get_embedder(config, device)


def clear_embedder_cache():
    """–û—á–∏—â–∞–µ—Ç –∫—ç—à —ç–º–±–µ–¥–¥–µ—Ä–æ–≤."""
    embedding_manager.clear_cache()
====================================================================================================
==================== –ö–û–ù–ï–¶ –§–ê–ô–õ–ê: core\embedding\embeddings.py ====================



==================== –ù–ê–ß–ê–õ–û –§–ê–ô–õ–ê: core\embedding\embedding_manager.py ====================
# –§–∞–π–ª: core\embedding\embedding_manager.py
====================================================================================================
"""Module for centralized embedding management."""

import logging
from collections import OrderedDict
from typing import Any, Dict, List, Optional

import torch
from langchain_core.embeddings import Embeddings
from langchain_huggingface import HuggingFaceEmbeddings

from config.config_manager import ConfigManager
from config.settings import Config

# –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º GGUF —ç–º–±–µ–¥–¥–µ—Ä
try:
    from core.embedding.gguf_embeddings import GGUFEmbeddings
    GGUF_AVAILABLE = True
except ImportError:
    GGUF_AVAILABLE = False
    logger = logging.getLogger(__name__)
    logger.warning("GGUF —ç–º–±–µ–¥–¥–µ—Ä –Ω–µ –¥–æ—Å—Ç—É–ø–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ llama-cpp-python –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å GGUF –º–æ–¥–µ–ª—è–º–∏.")

logger = logging.getLogger(__name__)

# Get singleton instance of ConfigManager
config_manager = ConfigManager.get_instance()


class EmbeddingError(Exception):
    """Custom exception for embedding management errors."""
    pass


class EmbeddingManager:
    """Centralized manager for embeddings with caching."""
    
    _instance: Optional['EmbeddingManager'] = None
    MAX_CACHE_SIZE = 1  # –ú–∞–∫—Å–∏–º—É–º 1 –º–æ–¥–µ–ª—å –≤ –∫—ç—à–µ –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ
    
    def __init__(self):
        """Initialize EmbeddingManager."""
        self._embedder_cache: OrderedDict = OrderedDict()
        
    @classmethod
    def get_instance(cls) -> 'EmbeddingManager':
        """Get singleton instance of EmbeddingManager.
        
        Returns:
            EmbeddingManager: Singleton instance
        """
        if cls._instance is None:
            cls._instance = cls()
        return cls._instance
    
    def get_device(self, config_device: str) -> str:
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –¥–ª—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏ –∏–∑ –∫–æ–Ω—Ñ–∏–≥–∞."""
        if config_device == "cuda" and torch.cuda.is_available():
            return "cuda"
        if config_device == "cpu":
            return "cpu"
        return "cuda" if torch.cuda.is_available() else "cpu"
    
    def get_search_device(self, search_device_param: str) -> str:
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –¥–ª—è –ø–æ–∏—Å–∫–∞ –∏–∑ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞ —Ñ–æ—Ä–º—ã."""
        if search_device_param == "cuda" and torch.cuda.is_available():
            return "cuda"
        return "cpu"
    
    def get_embedder(self, config: Config, device: Optional[str] = None) -> Embeddings:
        """–ü–æ–ª—É—á–∞–µ—Ç –∏–ª–∏ —Å–æ–∑–¥–∞–µ—Ç –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä —ç–º–±–µ–¥–¥–µ—Ä–∞.
        
        Args:
            config: Configuration object
            device: Device to use for embeddings (optional)
            
        Returns:
            Embeddings: Embedder instance
            
        Raises:
            EmbeddingError: If there's an error creating the embedder
        """
        try:
            model_name = config.current_hf_model
            batch_size = config.embedding_batch_size
            if device is None:
                device = self.get_device(config.device)
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ –º–æ–¥–µ–ª—å GGUF
            if model_name.lower().endswith('.gguf'):
                if GGUF_AVAILABLE:
                    return self._get_gguf_embedder(config, device)
                else:
                    raise EmbeddingError("GGUF —ç–º–±–µ–¥–¥–µ—Ä –Ω–µ –¥–æ—Å—Ç—É–ø–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ llama-cpp-python –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å GGUF –º–æ–¥–µ–ª—è–º–∏.")
            
            # –ö–ª—é—á –∫—ç—à–∞ - –∫–æ—Ä—Ç–µ–∂ (–º–æ–¥–µ–ª—å, —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ)
            cache_key = (model_name, device)
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ –≤ –∫—ç—à–µ –º–æ–¥–µ–ª—å —Å —Ç–∞–∫–∏–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
            if cache_key in self._embedder_cache:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç –ª–∏ batch_size
                cached_embedder = self._embedder_cache[cache_key]
                if getattr(cached_embedder, "_batch_size", None) == batch_size:
                    # –ü–µ—Ä–µ–º–µ—â–∞–µ–º –º–æ–¥–µ–ª—å –≤ –Ω–∞—á–∞–ª–æ OrderedDict (–æ–±–Ω–æ–≤–ª—è–µ–º –≤—Ä–µ–º—è –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è)
                    self._embedder_cache.move_to_end(cache_key, last=True)
                    return cached_embedder
            
            # –ï—Å–ª–∏ –∫—ç—à –ø–µ—Ä–µ–ø–æ–ª–Ω–µ–Ω, —É–¥–∞–ª—è–µ–º —Å–∞–º—É—é —Å—Ç–∞—Ä—É—é –∑–∞–ø–∏—Å—å
            if len(self._embedder_cache) >= self.MAX_CACHE_SIZE:
                # –£–¥–∞–ª—è–µ–º —Å–∞–º—É—é —Å—Ç–∞—Ä—É—é –∑–∞–ø–∏—Å—å (–ø–µ—Ä–≤—É—é –≤ OrderedDict)
                oldest_key, _ = self._embedder_cache.popitem(last=False)
                logger.info(f"–£–¥–∞–ª–µ–Ω–∞ —Å—Ç–∞—Ä–∞—è –º–æ–¥–µ–ª—å –∏–∑ –∫—ç—à–∞: {oldest_key}")
            
            # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏
            model_kwargs = {"device": device}
            encode_kwargs = {"batch_size": batch_size}
            # –ï—Å–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–µ–º CUDA, —É–∫–∞–∑—ã–≤–∞–µ–º dtype —á–µ—Ä–µ–∑ encode_kwargs –¥–ª—è –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞—é—â–∏—Ö –º–æ–¥–µ–ª–µ–π
            # –£–±–∏—Ä–∞–µ–º torch_dtype –¥–ª—è –º–æ–¥–µ–ª–µ–π, –∫–æ—Ç–æ—Ä—ã–µ –µ–≥–æ –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞—é—Ç
            if device == "cuda":
                # –î–æ–±–∞–≤–ª—è–µ–º torch_dtype —Ç–æ–ª—å–∫–æ –¥–ª—è –º–æ–¥–µ–ª–µ–π, –∫–æ—Ç–æ—Ä—ã–µ —ç—Ç–æ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞—é—Ç
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–æ –Ω–∞–∑–≤–∞–Ω–∏—é –º–æ–¥–µ–ª–∏ –∏–ª–∏ –ø–æ –¥—Ä—É–≥–∏–º –∫—Ä–∏—Ç–µ—Ä–∏—è–º
                unsupported_models = ["qwen", "qwen3", "google/mt5-base", "t5", "mt5"]
                model_lower = model_name.lower()
                
                # –ï—Å–ª–∏ –º–æ–¥–µ–ª—å –Ω–µ –≤ —Å–ø–∏—Å–∫–µ –Ω–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞—é—â–∏—Ö, –¥–æ–±–∞–≤–ª—è–µ–º torch_dtype
                if not any(unsupported in model_lower for unsupported in unsupported_models):
                    encode_kwargs["torch_dtype"] = torch.float16
                else:
                    logger.debug(f"–ú–æ–¥–µ–ª—å {model_name} –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç torch_dtype, –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π —Ç–∏–ø –¥–∞–Ω–Ω—ã—Ö")
            
            # –ï—Å–ª–∏ —É–∫–∞–∑–∞–Ω —Ç–æ–∫–µ–Ω HuggingFace, –¥–æ–±–∞–≤–ª—è–µ–º –µ–≥–æ –≤ model_kwargs
            if config.huggingface_token:
                model_kwargs["token"] = config.huggingface_token

            # –ï—Å–ª–∏ –º–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –≤ –∫–µ—à–µ –∏–ª–∏ batch_size –∏–∑–º–µ–Ω–∏–ª—Å—è, —Å–æ–∑–¥–∞–µ–º –Ω–æ–≤—É—é
            embedder = HuggingFaceEmbeddings(
                model_name=model_name,
                model_kwargs=model_kwargs,
                encode_kwargs=encode_kwargs
            )

            # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º batch_size –¥–ª—è –ø–æ—Å–ª–µ–¥—É—é—â–µ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
            embedder._batch_size = batch_size  # –î–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å –≤ –∫—ç—à
            self._embedder_cache[cache_key] = embedder
            
            return embedder
            
        except Exception as e:
            logger.exception(f"Error creating embedder: {e}")
            raise EmbeddingError(f"Failed to create embedder: {e}")
    
    def _get_gguf_embedder(self, config: Config, device: str) -> Embeddings:
        """–ü–æ–ª—É—á–∞–µ—Ç GGUF —ç–º–±–µ–¥–¥–µ—Ä.
        
        Args:
            config: Configuration object
            device: Device to use for embeddings
            
        Returns:
            Embeddings: GGUF embedder instance
        """
        if not GGUF_AVAILABLE:
            raise EmbeddingError("GGUF —ç–º–±–µ–¥–¥–µ—Ä –Ω–µ –¥–æ—Å—Ç—É–ø–µ–Ω")
            
        model_name = config.current_hf_model
        
        # –ö–ª—é—á –∫—ç—à–∞ - –∫–æ—Ä—Ç–µ–∂ (–º–æ–¥–µ–ª—å, —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ)
        cache_key = (model_name, device)
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ –≤ –∫—ç—à–µ –º–æ–¥–µ–ª—å —Å —Ç–∞–∫–∏–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
        if cache_key in self._embedder_cache:
            # –ü–µ—Ä–µ–º–µ—â–∞–µ–º –º–æ–¥–µ–ª—å –≤ –Ω–∞—á–∞–ª–æ OrderedDict (–æ–±–Ω–æ–≤–ª—è–µ–º –≤—Ä–µ–º—è –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è)
            self._embedder_cache.move_to_end(cache_key, last=True)
            return self._embedder_cache[cache_key]
        
        # –ï—Å–ª–∏ –∫—ç—à –ø–µ—Ä–µ–ø–æ–ª–Ω–µ–Ω, —É–¥–∞–ª—è–µ–º —Å–∞–º—É—é —Å—Ç–∞—Ä—É—é –∑–∞–ø–∏—Å—å
        if len(self._embedder_cache) >= self.MAX_CACHE_SIZE:
            # –£–¥–∞–ª—è–µ–º —Å–∞–º—É—é —Å—Ç–∞—Ä—É—é –∑–∞–ø–∏—Å—å (–ø–µ—Ä–≤—É—é –≤ OrderedDict)
            oldest_key, _ = self._embedder_cache.popitem(last=False)
            logger.info(f"–£–¥–∞–ª–µ–Ω–∞ —Å—Ç–∞—Ä–∞—è –º–æ–¥–µ–ª—å –∏–∑ –∫—ç—à–∞: {oldest_key}")
        
        # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–π GGUF —ç–º–±–µ–¥–¥–µ—Ä
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø—É—Ç—å –∫ –º–æ–¥–µ–ª–∏ (–ø—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ–º, —á—Ç–æ –æ–Ω –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –≤ –∫–∞—Ç–∞–ª–æ–≥–µ –º–æ–¥–µ–ª–µ–π)
        model_path = model_name  # –ü—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ–º, —á—Ç–æ model_name —É–∂–µ —Å–æ–¥–µ—Ä–∂–∏—Ç –ø–æ–ª–Ω—ã–π –ø—É—Ç—å
        
        embedder = GGUFEmbeddings(
            model_path=model_path,
            device=device
        )
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å –≤ –∫—ç—à
        self._embedder_cache[cache_key] = embedder
        
        return embedder
    
    def clear_cache(self) -> None:
        """–û—á–∏—â–∞–µ—Ç –∫—ç—à —ç–º–±–µ–¥–¥–µ—Ä–æ–≤."""
        self._embedder_cache.clear()
        logger.info("–ö—ç—à —ç–º–±–µ–¥–¥–µ—Ä–æ–≤ –æ—á–∏—â–µ–Ω")
        
        # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ –æ—á–∏—â–∞–µ–º –ø–∞–º—è—Ç—å PyTorch
        try:
            import torch
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
        except ImportError:
            pass
    
    def get_cache_info(self) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–∞–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –∫—ç—à–µ —ç–º–±–µ–¥–¥–µ—Ä–æ–≤.
        
        Returns:
            Dict[str, Any]: Cache information
        """
        return {
            "cache_size": len(self._embedder_cache),
            "max_cache_size": self.MAX_CACHE_SIZE,
            "cached_models": list(self._embedder_cache.keys())
        }
    
    def embed_query(self, text: str, config: Optional[Config] = None, device: Optional[str] = None) -> List[float]:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–∞ –¥–ª—è –æ–¥–Ω–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞.
        
        Args:
            text: –¢–µ–∫—Å—Ç –¥–ª—è –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è
            config: Configuration object (optional, –µ—Å–ª–∏ –Ω–µ —É–∫–∞–∑–∞–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –∫–æ–Ω—Ñ–∏–≥ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é)
            device: Device to use for embeddings (optional)
            
        Returns:
            List[float]: –≠–º–±–µ–¥–¥–∏–Ω–≥ —Ç–µ–∫—Å—Ç–∞
            
        Raises:
            EmbeddingError: –ï—Å–ª–∏ –≤–æ–∑–Ω–∏–∫–∞–µ—Ç –æ—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞
        """
        try:
            # –ï—Å–ª–∏ –∫–æ–Ω—Ñ–∏–≥ –Ω–µ –ø–µ—Ä–µ–¥–∞–Ω, –ø–æ–ª—É—á–∞–µ–º –µ–≥–æ –∏–∑ config_manager
            if config is None:
                config = config_manager.get()
            
            # –ü–æ–ª—É—á–∞–µ–º —ç–º–±–µ–¥–¥–µ—Ä
            embedder = self.get_embedder(config, device)
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ —É —ç–º–±–µ–¥–¥–µ—Ä–∞ –º–µ—Ç–æ–¥ embed_query
            if hasattr(embedder, 'embed_query'):
                return embedder.embed_query(text)
            else:
                # –ï—Å–ª–∏ –º–µ—Ç–æ–¥–∞ embed_query –Ω–µ—Ç, –∏—Å–ø–æ–ª—å–∑—É–µ–º embed_documents —Å –æ–¥–Ω–∏–º —Ç–µ–∫—Å—Ç–æ–º
                embeddings = embedder.embed_documents([text])
                return embeddings[0] if embeddings else []
                
        except Exception as e:
            logger.exception(f"Error generating embedding for query: {e}")
            raise EmbeddingError(f"Failed to generate embedding for query: {e}")
    
    def embed_texts(self, texts: List[str], config: Optional[Config] = None, device: Optional[str] = None) -> List[List[float]]:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –¥–ª—è —Å–ø–∏—Å–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤.
        
        Args:
            texts: –°–ø–∏—Å–æ–∫ —Ç–µ–∫—Å—Ç–æ–≤ –¥–ª—è –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è
            config: Configuration object (optional, –µ—Å–ª–∏ –Ω–µ —É–∫–∞–∑–∞–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –∫–æ–Ω—Ñ–∏–≥ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é)
            device: Device to use for embeddings (optional)
            
        Returns:
            List[List[float]]: –°–ø–∏—Å–æ–∫ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ —Ç–µ–∫—Å—Ç–æ–≤
            
        Raises:
            EmbeddingError: –ï—Å–ª–∏ –≤–æ–∑–Ω–∏–∫–∞–µ—Ç –æ—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
        """
        try:
            # –ï—Å–ª–∏ –∫–æ–Ω—Ñ–∏–≥ –Ω–µ –ø–µ—Ä–µ–¥–∞–Ω, –ø–æ–ª—É—á–∞–µ–º –µ–≥–æ –∏–∑ config_manager
            if config is None:
                config = config_manager.get()
            
            # –ü–æ–ª—É—á–∞–µ–º —ç–º–±–µ–¥–¥–µ—Ä
            embedder = self.get_embedder(config, device)
            
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –º–µ—Ç–æ–¥ embed_documents –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
            return embedder.embed_documents(texts)
                
        except Exception as e:
            logger.exception(f"Error generating embeddings for texts: {e}")
            raise EmbeddingError(f"Failed to generate embeddings for texts: {e}")
====================================================================================================
==================== –ö–û–ù–ï–¶ –§–ê–ô–õ–ê: core\embedding\embedding_manager.py ====================



==================== –ù–ê–ß–ê–õ–û –§–ê–ô–õ–ê: core\embedding\gguf_embeddings.py ====================
# –§–∞–π–ª: core\embedding\gguf_embeddings.py
====================================================================================================
"""–ú–æ–¥—É–ª—å –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å GGUF —ç–º–±–µ–¥–¥–µ—Ä–∞–º–∏."""

import logging
import os
from typing import List

from langchain_core.embeddings import Embeddings

from config.config_manager import ConfigManager
from config.settings import Config

logger = logging.getLogger(__name__)

# –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º llama_cpp –¥–ª—è –ø—Ä–∞–≤–∏–ª—å–Ω–æ–π —Ä–∞–±–æ—Ç—ã —Ç–µ—Å—Ç–æ–≤
try:
    import importlib.util
    LLAMA_CPP_AVAILABLE = importlib.util.find_spec("llama_cpp") is not None
except ImportError:
    LLAMA_CPP_AVAILABLE = False


class GGUFEmbeddings(Embeddings):
    """–ö–ª–∞—Å—Å –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å GGUF –º–æ–¥–µ–ª—è–º–∏ —á–µ—Ä–µ–∑ llama.cpp Python –±–∏–Ω–¥–∏–Ω–≥–∏."""
    
    def __init__(self, model_path: str, device: str = "cpu"):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è GGUF —ç–º–±–µ–¥–¥–µ—Ä–∞.
        
        Args:
            model_path: –ü—É—Ç—å –∫ GGUF –º–æ–¥–µ–ª–∏
            device: –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏–π ("cpu" –∏–ª–∏ "cuda")
        """
        try:
            from llama_cpp import Llama
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
            config_manager = ConfigManager.get_instance()
            config = config_manager.get()
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –ª–∏ —Ñ–∞–π–ª –º–æ–¥–µ–ª–∏
            if not os.path.exists(model_path):
                # –ï—Å–ª–∏ —Ñ–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω, –ø—ã—Ç–∞–µ–º—Å—è –Ω–∞–π—Ç–∏ –µ–≥–æ –≤ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã—Ö –º–µ—Å—Ç–∞—Ö
                models_dir = os.getenv("MODELS_DIR", "")
                possible_paths = [
                    model_path,
                    os.path.join("models", model_path),
                    os.path.join("data", model_path),
                    os.path.join("..", "models", model_path),
                ]
                
                # –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç—å –∏–∑ –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π –æ–∫—Ä—É–∂–µ–Ω–∏—è MODELS_DIR, –µ—Å–ª–∏ –æ–Ω–∞ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞
                if models_dir:
                    possible_paths.append(os.path.join(models_dir, model_path))
                
                found = False
                for path in possible_paths:
                    if os.path.exists(path):
                        model_path = path
                        found = True
                        break
                
                if not found:
                    raise FileNotFoundError(f"–§–∞–π–ª –º–æ–¥–µ–ª–∏ {model_path} –Ω–µ –Ω–∞–π–¥–µ–Ω. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ —É–∫–∞–∑–∞—Ç—å –ø–æ–ª–Ω—ã–π –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É.")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫—ç—à —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ –≤–µ–∫—Ç–æ—Ä–∞
            global _embedding_dim_cache
            if model_path in _embedding_dim_cache:
                self.expected_dim = _embedding_dim_cache[model_path]
                # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –º–æ–¥–µ–ª—å –±–µ–∑ —Ç–µ—Å—Ç–æ–≤–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞
                n_gpu_layers = -1 if device == "cuda" else 0
                self.model = Llama(
                    model_path=model_path,
                    n_gpu_layers=n_gpu_layers,  # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–µ–≤ –Ω–∞ GPU (-1 –¥–ª—è –≤—Å–µ—Ö —Å–ª–æ–µ–≤)
                    n_ctx=config.gguf_model_n_ctx,  # –†–∞–∑–º–µ—Ä –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –∏–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
                    embedding=True,  # –í–∫–ª—é—á–∞–µ–º —Ä–µ–∂–∏–º —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
                    verbose=False  # –û—Ç–∫–ª—é—á–∞–µ–º –ø–æ–¥—Ä–æ–±–Ω—ã–π –≤—ã–≤–æ–¥
                )
            else:
                # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –º–æ–¥–µ–ª—å –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ –≤–µ–∫—Ç–æ—Ä–∞
                n_gpu_layers = -1 if device == "cuda" else 0
                self.model = Llama(
                    model_path=model_path,
                    n_gpu_layers=n_gpu_layers,  # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–µ–≤ –Ω–∞ GPU (-1 –¥–ª—è –≤—Å–µ—Ö —Å–ª–æ–µ–≤)
                    n_ctx=config.gguf_model_n_ctx,  # –†–∞–∑–º–µ—Ä –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –∏–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
                    embedding=True,  # –í–∫–ª—é—á–∞–µ–º —Ä–µ–∂–∏–º —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
                    verbose=False  # –û—Ç–∫–ª—é—á–∞–µ–º –ø–æ–¥—Ä–æ–±–Ω—ã–π –≤—ã–≤–æ–¥
                )
                # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –æ–∂–∏–¥–∞–µ–º—É—é —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –≤–µ–∫—Ç–æ—Ä–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–µ—Å—Ç–æ–≤–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞
                test_embedding = self.model.create_embedding("test")
                self.expected_dim = len(test_embedding['data'][0]['embedding'])
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –≤ –∫—ç—à
                _embedding_dim_cache[model_path] = self.expected_dim
            
            self.device = device
        except ImportError:
            logger.exception("–ù–µ —É–¥–∞–ª–æ—Å—å –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å llama_cpp. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ –µ–≥–æ —Å –ø–æ–º–æ—â—å—é `pip install llama-cpp-python`.")
            raise
        except Exception as e:
            logger.exception(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ GGUF –º–æ–¥–µ–ª–∏: {e}")
            raise
    
    def embed_documents(self, texts: List[str]) -> List[List[float]]:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –¥–ª—è —Å–ø–∏—Å–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤."""
        embeddings = []
        for text in texts:
            embedding = self.embed_query(text)
            embeddings.append(embedding)
        return embeddings
    
    def embed_query(self, text: str) -> List[float]:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–∞ –¥–ª—è –æ–¥–Ω–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞."""
        try:
            # –ü–æ–ª—É—á–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥ —á–µ—Ä–µ–∑ llama.cpp
            embedding = self.model.create_embedding(text)
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –≤–µ–∫—Ç–æ—Ä –∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
            vector = embedding.get('data', [{}])[0].get('embedding', []) or getattr(embedding, 'embedding', [])
            if not vector:
                raise ValueError("Invalid embedding format")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –≤–µ–∫—Ç–æ—Ä–∞
            if len(vector) != self.expected_dim:
                # logger.warning(f"Embedding dimension mismatch: {len(vector)} vs {self.expected_dim}")
                raise ValueError(f"Embedding dimension mismatch: {len(vector)} vs {self.expected_dim}")
            
            return vector
        except Exception as e:
            raise Exception(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞: {e}")


# –ö—ç—à –¥–ª—è GGUF embedder'–∞
_gguf_embedder_cache = {}
# –ö—ç—à –¥–ª—è —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ –≤–µ–∫—Ç–æ—Ä–∞
_embedding_dim_cache = {}


def get_gguf_embedder(config: Config, device=None):
    """–ü–æ–ª—É—á–∞–µ—Ç –∏–ª–∏ —Å–æ–∑–¥–∞–µ—Ç –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä GGUFEmbeddings."""
    global _gguf_embedder_cache, _embedding_dim_cache
    model_name = config.current_hf_model
    batch_size = config.embedding_batch_size
    if device is None:
        device = "cpu"  # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é –∏—Å–ø–æ–ª—å–∑—É–µ–º CPU –¥–ª—è GGUF
    
    # –ö–ª—é—á –∫—ç—à–∞ - –∫–æ—Ä—Ç–µ–∂ (–º–æ–¥–µ–ª—å, —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ)
    cache_key = (model_name, device)
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ –≤ –∫—ç—à–µ –º–æ–¥–µ–ª—å —Å —Ç–∞–∫–∏–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
    if cache_key in _gguf_embedder_cache:
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç –ª–∏ batch_size
        cached_embedder = _gguf_embedder_cache[cache_key]
        if getattr(cached_embedder, "_batch_size", None) == batch_size:
            return cached_embedder
    
    # –ï—Å–ª–∏ –º–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –≤ –∫—ç—à–µ –∏–ª–∏ batch_size –∏–∑–º–µ–Ω–∏–ª—Å—è, —Å–æ–∑–¥–∞–µ–º –Ω–æ–≤—É—é
    embedder = GGUFEmbeddings(
        model_path=model_name,
        device=device
    )
    embedder._batch_size = batch_size  # –î–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å –≤ –∫—ç—à
    _gguf_embedder_cache[cache_key] = embedder
    
    return embedder
====================================================================================================
==================== –ö–û–ù–ï–¶ –§–ê–ô–õ–ê: core\embedding\gguf_embeddings.py ====================



==================== –ù–ê–ß–ê–õ–û –§–ê–ô–õ–ê: core\embedding\sparse_embedding_adapter.py ====================
# –§–∞–π–ª: core\embedding\sparse_embedding_adapter.py
====================================================================================================
"""–ê–¥–∞–ø—Ç–µ—Ä –¥–ª—è —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω—ã—Ö —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ —Å native BM25."""

import logging
import re
from collections import Counter
from typing import Dict, List, Tuple, Any

from qdrant_client.models import SparseVector
from config.config_manager import ConfigManager

logger = logging.getLogger(__name__)


class SparseEmbeddingAdapter:
    """–ê–¥–∞–ø—Ç–µ—Ä –¥–ª—è sparse —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤, –∑–∞–º–µ–Ω—è—é—â–∏–π Fastembed –Ω–∞ native BM25."""
    
    def __init__(self, config=None):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç –∞–¥–∞–ø—Ç–µ—Ä."""
        self.config = config or ConfigManager.get_instance().get()
        if not self.config.use_bm25:
            raise ValueError("BM25 not enabled in config")

    def encode(self, texts: List[str], return_sparse: bool = True) -> List[Dict[str, Any]]:
        """
        –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç sparse vectors –¥–ª—è —Ç–µ–∫—Å—Ç–æ–≤ (native BM25-like: —Ç–æ–∫–µ–Ω—ã + TF weights).
        Returns raw dict for Qdrant JSON: {"indices": [int], "values": [float]}.
        
        Args:
            texts: List of texts.
            return_sparse: Always True for sparse.
            
        Returns:
            List of dict (for SparseVector).
        """
        sparse_vectors = []
        for text in texts:
            # Advanced tokenization to better match Qdrant native BM25
            # Use more sophisticated tokenization similar to what Qdrant uses
            tokens = self._tokenize_text(text)
            tokens = [t for t in tokens if len(t) >= self.config.bm25_min_token_len]
            
            # TF weights (frequency)
            token_counts = Counter(tokens)
            total_terms = len(tokens) or 1  # Avoid div/0
            indices = [abs(hash(token)) % (2**31 - 1) for token in token_counts.keys()]  # Positive unique-ish ints
            values = [count / total_terms for count in token_counts.values()]
            
            # Ensure unique/sorted (Qdrant req)
            unique_pairs = sorted(set(zip(indices, values)), key=lambda x: x[0])
            indices = [idx for idx, _ in unique_pairs]
            values = [val for _, val in unique_pairs]
            
            sparse_vectors.append({"indices": indices, "values": values})
        
        logger.debug(f"Generated {len(sparse_vectors)} sparse vectors for BM25")
        return sparse_vectors

    def _tokenize_text(self, text: str) -> List[str]:
        """
        Advanced tokenization that better matches Qdrant native BM25.
        Qdrant typically uses word-level tokenization with some normalization.
        """
        # Convert to lowercase
        text = text.lower()
        
        # Use regex to find word-like tokens (letters, numbers, and hyphens within words)
        # This should better match the Qdrant native tokenization
        tokens = re.findall(r'\b\w+(?:-\w+)*\b', text)
        
        return tokens

    def embed_query(self, query: str) -> Dict[str, Any]:
        """Single query embedding."""
        return self.encode([query], return_sparse=True)[0]
====================================================================================================
==================== –ö–û–ù–ï–¶ –§–ê–ô–õ–ê: core\embedding\sparse_embedding_adapter.py ====================



==================== –ù–ê–ß–ê–õ–û –§–ê–ô–õ–ê: core\embedding\__init__.py ====================
# –§–∞–π–ª: core\embedding\__init__.py
====================================================================================================
 

====================================================================================================
==================== –ö–û–ù–ï–¶ –§–ê–ô–õ–ê: core\embedding\__init__.py ====================



==================== –ù–ê–ß–ê–õ–û –§–ê–ô–õ–ê: core\indexing\chunker.py ====================
# –§–∞–π–ª: core\indexing\chunker.py
====================================================================================================
"""–ú–æ–¥—É–ª—å –¥–ª—è –Ω–∞—Ä–µ–∑–∫–∏ —Ç–µ–∫—Å—Ç–∞ –Ω–∞ —á–∞–Ω–∫–∏."""

import logging
from functools import lru_cache

from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_text_splitters.base import TextSplitter

from config.config_manager import ConfigManager
from config.settings import Config
from core.indexing.paragraph_chunker import ParagraphTextSplitter
from core.indexing.sentence_chunker import SentenceTextSplitter

logger = logging.getLogger(__name__)


def create_text_splitter(config: Config) -> TextSplitter:
    """
    –°–æ–∑–¥–∞–µ—Ç –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —ç–∫–∑–µ–º–ø–ª—è—Ä TextSplitter –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏.
    
    Args:
        config (Config): –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è
        
    Returns:
        TextSplitter: –≠–∫–∑–µ–º–ø–ª—è—Ä –ø–æ–¥—Ö–æ–¥—è—â–µ–≥–æ —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—è —Ç–µ–∫—Å—Ç–∞
    """
    if config.chunking_strategy == "paragraph":
        logger.debug(
            f"–°–æ–∑–¥–∞–Ω–∏–µ paragraph text splitter —Å paragraphs_per_chunk={config.paragraphs_per_chunk}, "
            f"paragraph_overlap={config.paragraph_overlap}"
        )
        return ParagraphTextSplitter(
            paragraphs_per_chunk=config.paragraphs_per_chunk,
            paragraph_overlap=config.paragraph_overlap
        )
    elif config.chunking_strategy == "sentence":
        logger.debug(
            f"–°–æ–∑–¥–∞–Ω–∏–µ sentence text splitter —Å sentences_per_chunk={config.sentences_per_chunk}, "
            f"sentence_overlap={config.sentence_overlap}"
        )
        return SentenceTextSplitter(
            sentences_per_chunk=config.sentences_per_chunk,
            sentence_overlap=config.sentence_overlap
        )
    else:  # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å—Ç—Ä–∞—Ç–µ–≥–∏—é –ø–æ —Å–∏–º–≤–æ–ª–∞–º
        logger.debug(
            f"–°–æ–∑–¥–∞–Ω–∏–µ character text splitter —Å chunk_size={config.chunk_size}, "
            f"chunk_overlap={config.chunk_overlap}"
        )
        return RecursiveCharacterTextSplitter(
            chunk_size=config.chunk_size,
            chunk_overlap=config.chunk_overlap
        )


@lru_cache(maxsize=32)  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º —Ä–∞–∑–º–µ—Ä –∫—ç—à–∞ –¥–æ 32 —ç–ª–µ–º–µ–Ω—Ç–æ–≤
def get_text_splitter(
    chunk_size: int = None, 
    chunk_overlap: int = None,
    chunking_strategy: str = None,
    paragraphs_per_chunk: int = None,
    paragraph_overlap: int = None,
    sentences_per_chunk: int = None,
    sentence_overlap: int = None
) -> TextSplitter:
    """
    –°–æ–∑–¥–∞–µ—Ç –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —ç–∫–∑–µ–º–ø–ª—è—Ä TextSplitter –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –∏–ª–∏ –ø–µ—Ä–µ–¥–∞–Ω–Ω—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤.
    
    Args:
        chunk_size (int, optional): –†–∞–∑–º–µ—Ä —á–∞–Ω–∫–∞ –≤ —Å–∏–º–≤–æ–ª–∞—Ö
        chunk_overlap (int, optional): –ü–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ —á–∞–Ω–∫–æ–≤ –≤ —Å–∏–º–≤–æ–ª–∞—Ö
        chunking_strategy (str, optional): –°—Ç—Ä–∞—Ç–µ–≥–∏—è —á–∞–Ω–∫–∏–Ω–≥–∞ ("character", "paragraph" –∏–ª–∏ "sentence")
        paragraphs_per_chunk (int, optional): –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∞–±–∑–∞—Ü–µ–≤ –≤ –æ–¥–Ω–æ–º —á–∞–Ω–∫–µ
        paragraph_overlap (int, optional): –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∞–±–∑–∞—Ü–µ–≤ –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏—è –º–µ–∂–¥—É —á–∞–Ω–∫–∞–º–∏
        sentences_per_chunk (int, optional): –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π –≤ –æ–¥–Ω–æ–º —á–∞–Ω–∫–µ
        sentence_overlap (int, optional): –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏—è –º–µ–∂–¥—É —á–∞–Ω–∫–∞–º–∏
        
    Returns:
        TextSplitter: –≠–∫–∑–µ–º–ø–ª—è—Ä –ø–æ–¥—Ö–æ–¥—è—â–µ–≥–æ —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—è —Ç–µ–∫—Å—Ç–∞
    """
    config_manager = ConfigManager.get_instance()
    config: Config = config_manager.get()
    
    # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–µ—Ä–µ–¥–∞–Ω–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã, –µ—Å–ª–∏ –æ–Ω–∏ –µ—Å—Ç—å, –∏–Ω–∞—á–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∏–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
    strategy = chunking_strategy if chunking_strategy is not None else config.chunking_strategy
    size = chunk_size if chunk_size is not None else config.chunk_size
    overlap = chunk_overlap if chunk_overlap is not None else config.chunk_overlap
    para_per_chunk = paragraphs_per_chunk if paragraphs_per_chunk is not None else config.paragraphs_per_chunk
    para_overlap = paragraph_overlap if paragraph_overlap is not None else config.paragraph_overlap
    sent_per_chunk = sentences_per_chunk if sentences_per_chunk is not None else config.sentences_per_chunk
    sent_overlap = sentence_overlap if sentence_overlap is not None else config.sentence_overlap
    
    if strategy == "paragraph":
        logger.debug(
            f"–°–æ–∑–¥–∞–Ω–∏–µ paragraph text splitter —Å paragraphs_per_chunk={para_per_chunk}, "
            f"paragraph_overlap={para_overlap}"
        )
        return ParagraphTextSplitter(
            paragraphs_per_chunk=para_per_chunk,
            paragraph_overlap=para_overlap
        )
    elif strategy == "sentence":
        logger.debug(
            f"–°–æ–∑–¥–∞–Ω–∏–µ sentence text splitter —Å sentences_per_chunk={sent_per_chunk}, "
            f"sentence_overlap={sent_overlap}"
        )
        return SentenceTextSplitter(
            sentences_per_chunk=sent_per_chunk,
            sentence_overlap=sent_overlap
        )
    else:  # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å—Ç—Ä–∞—Ç–µ–≥–∏—é –ø–æ —Å–∏–º–≤–æ–ª–∞–º
        logger.debug(
            f"–°–æ–∑–¥–∞–Ω–∏–µ character text splitter —Å chunk_size={size}, "
            f"chunk_overlap={overlap}"
        )
        return RecursiveCharacterTextSplitter(
            chunk_size=size,
            chunk_overlap=overlap
        )
====================================================================================================
==================== –ö–û–ù–ï–¶ –§–ê–ô–õ–ê: core\indexing\chunker.py ====================



==================== –ù–ê–ß–ê–õ–û –§–ê–ô–õ–ê: core\indexing\document_loader.py ====================
# –§–∞–π–ª: core\indexing\document_loader.py
====================================================================================================
"""–ú–æ–¥—É–ª—å –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤."""

import logging
from pathlib import Path
from typing import List

from langchain_community.document_loaders import TextLoader
from langchain_core.documents import Document

logger = logging.getLogger(__name__)


class DocumentLoader:
    """–ö–ª–∞—Å—Å –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∏–∑ —Ñ–∞–π–ª–æ–≤."""

    def __init__(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∑–∞–≥—Ä—É–∑—á–∏–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤."""
        pass

    def load_text_file(self, filepath: Path) -> List[Document]:
        """
        –ó–∞–≥—Ä—É–∑–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ —Ñ–∞–π–ª–∞ (.txt –∏–ª–∏ .md).
        
        Args:
            filepath (Path): –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É.
            
        Returns:
            List[Document]: –°–ø–∏—Å–æ–∫ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤.
        """
        try:
            loader = TextLoader(str(filepath), encoding="utf-8")
            return loader.load()
        except Exception as e:
            logger.exception(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ —Ñ–∞–π–ª–∞ {filepath}: {e}")
            raise


class IndexingError(Exception):
    """–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–æ–µ –∏—Å–∫–ª—é—á–µ–Ω–∏–µ –¥–ª—è –æ—à–∏–±–æ–∫ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏."""
    pass
====================================================================================================
==================== –ö–û–ù–ï–¶ –§–ê–ô–õ–ê: core\indexing\document_loader.py ====================



==================== –ù–ê–ß–ê–õ–û –§–ê–ô–õ–ê: core\indexing\indexer.py ====================
# –§–∞–π–ª: core\indexing\indexer.py
====================================================================================================
"""–ú–æ–¥—É–ª—å –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–µ–π –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ Qdrant."""

import asyncio
import logging
from pathlib import Path
from typing import List, Tuple

from langchain_core.documents import Document

from config.config_manager import ConfigManager
from config.settings import Config
from core.indexing.document_loader import DocumentLoader, IndexingError
from core.indexing.indexer_component import Indexer
from core.indexing.metadata_manager import metadata_manager
from core.indexing.multilevel_indexer import MultiLevelIndexer
from core.indexing.text_splitter import TextSplitter

logger = logging.getLogger(__name__)

# Get singleton instance of ConfigManager
config_manager = ConfigManager.get_instance()


async def run_indexing_logic(client=None, pre_chunked_documents=None) -> Tuple[bool, str]:
    """
    –û—Å–Ω–æ–≤–Ω–∞—è –ª–æ–≥–∏–∫–∞ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤.
    
    Args:
        client (QdrantClient, optional): –ö–ª–∏–µ–Ω—Ç Qdrant. –ï—Å–ª–∏ –Ω–µ —É–∫–∞–∑–∞–Ω, —Å–æ–∑–¥–∞–µ—Ç—Å—è –Ω–æ–≤—ã–π.
        pre_chunked_documents (List[Document], optional): –ü—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ –Ω–∞—Ä–µ–∑–∞–Ω–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã. 
            –ï—Å–ª–∏ —É–∫–∞–∑–∞–Ω—ã, –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –≤–º–µ—Å—Ç–æ –∑–∞–≥—Ä—É–∑–∫–∏ –∏ –Ω–∞—Ä–µ–∑–∫–∏ —Ñ–∞–π–ª–æ–≤.
        
    Returns:
        Tuple[bool, str]: (—É—Å–ø–µ—Ö, —Å—Ç–∞—Ç—É—Å)
    """
    try:
        config: Config = config_manager.get()
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–µ–∂–∏–º–æ–≤ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏
        index_dense = getattr(config, 'index_dense', True)
        index_bm25 = getattr(config, 'index_bm25', False)
        index_hybrid = getattr(config, 'index_hybrid', False)

        if not (index_dense or index_bm25 or index_hybrid):
            return False, "no_index_type"
        
        # –ï—Å–ª–∏ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω—ã –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ –Ω–∞—Ä–µ–∑–∞–Ω–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã, –∏—Å–ø–æ–ª—å–∑—É–µ–º –∏—Ö
        if pre_chunked_documents is not None:
            logger.info(f"–ò—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ –Ω–∞—Ä–µ–∑–∞–Ω–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã: {len(pre_chunked_documents)} —á–∞–Ω–∫–æ–≤")
            all_documents = pre_chunked_documents
            folder_path_resolved = Path(config.folder_path).resolve()
            
            # –°–æ–∑–¥–∞–µ–º –∏–Ω–¥–µ–∫—Å–µ—Ä –∏ –∏–Ω–¥–µ–∫—Å–∏—Ä—É–µ–º –¥–æ–∫—É–º–µ–Ω—Ç—ã
            indexer = Indexer(config)
            success, status = await indexer.index_documents(all_documents, client, folder_path_resolved)
        else:
            folder_path = Path(config.folder_path)
            folder_path_resolved = folder_path.resolve()  # –î–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã—Ö –ø—É—Ç–µ–π
            if not folder_path.is_dir():
                return False, "folder_not_found"
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω—É–∂–Ω–æ –ª–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –º–Ω–æ–≥–æ—É—Ä–æ–≤–Ω–µ–≤—ã–π —á–∞–Ω–∫–∏–Ω–≥
            use_multilevel_chunking = getattr(config, 'use_multilevel_chunking', False)
            
            if use_multilevel_chunking:
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º –º–Ω–æ–≥–æ—É—Ä–æ–≤–Ω–µ–≤—ã–π –∏–Ω–¥–µ–∫—Å–∞—Ç–æ—Ä
                logger.info("–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –º–Ω–æ–≥–æ—É—Ä–æ–≤–Ω–µ–≤—ã–π –∏–Ω–¥–µ–∫—Å–∞—Ç–æ—Ä")
                multilevel_indexer = MultiLevelIndexer(config)
                
                # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã
                all_documents = []
                document_loader = DocumentLoader()
                
                # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º .txt —Ñ–∞–π–ª—ã
                for filepath in folder_path.rglob("*.txt"):
                    try:
                        loaded_docs = document_loader.load_text_file(filepath)
                        # –î–ª—è –º–Ω–æ–≥–æ—É—Ä–æ–≤–Ω–µ–≤–æ–≥–æ —á–∞–Ω–∫–∏–Ω–≥–∞ –Ω–µ –Ω—É–∂–Ω–æ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ —Ä–∞–∑–±–∏–≤–∞—Ç—å –Ω–∞ —á–∞–Ω–∫–∏
                        processed_docs = _process_documents_without_chunking(loaded_docs, filepath, folder_path_resolved, config)
                        all_documents.extend(processed_docs)
                    except Exception as e:
                        logger.exception(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ —Ñ–∞–π–ª–∞ {filepath}: {e}")
                        raise IndexingError(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ —Ñ–∞–π–ª–∞ {filepath}: {e}")
                
                # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º .md —Ñ–∞–π–ª—ã
                for filepath in folder_path.rglob("*.md"):
                    try:
                        logger.info(f"–ó–∞–≥—Ä—É–∑–∫–∞ .md —Ñ–∞–π–ª–∞: {filepath}")
                        loaded_docs = document_loader.load_text_file(filepath)
                        logger.info(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(loaded_docs)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∏–∑ —Ñ–∞–π–ª–∞: {filepath}")
                        # –î–ª—è –º–Ω–æ–≥–æ—É—Ä–æ–≤–Ω–µ–≤–æ–≥–æ —á–∞–Ω–∫–∏–Ω–≥–∞ –Ω–µ –Ω—É–∂–Ω–æ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ —Ä–∞–∑–±–∏–≤–∞—Ç—å –Ω–∞ —á–∞–Ω–∫–∏
                        processed_docs = _process_documents_without_chunking(loaded_docs, filepath, folder_path_resolved, config)
                        all_documents.extend(processed_docs)
                        logger.info(f"–ü–æ—Å–ª–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ñ–∞–π–ª–∞ {filepath} –≤—Å–µ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {len(all_documents)}")
                    except Exception as e:
                        logger.exception(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ —Ñ–∞–π–ª–∞ {filepath}: {e}")
                        raise IndexingError(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ —Ñ–∞–π–ª–∞ {filepath}: {e}")
                
                logger.info(f"–í—Å–µ–≥–æ —Å–æ–±—Ä–∞–Ω–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {len(all_documents)}")
                
                # –ò–Ω–¥–µ–∫—Å–∏—Ä—É–µ–º –≤—Å–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –º–Ω–æ–≥–æ—É—Ä–æ–≤–Ω–µ–≤–æ–≥–æ –∏–Ω–¥–µ–∫—Å–∞—Ç–æ—Ä–∞
                # –í—Ä–µ–º–µ–Ω–Ω–æ –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –≤—ã–∑–æ–≤, —Ç–∞–∫ –∫–∞–∫ multilevel_indexer.index_documents_multilevel –Ω–µ async
                stats = multilevel_indexer.index_documents_multilevel(all_documents)
                logger.info(f"–ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞: {stats}")
                
                success = True
                status = "indexed_successfully"
            else:
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º –æ–±—ã—á–Ω—ã–π –∏–Ω–¥–µ–∫—Å–∞—Ç–æ—Ä
                logger.info("–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –æ–±—ã—á–Ω—ã–π –∏–Ω–¥–µ–∫—Å–∞—Ç–æ—Ä")
                
                # –°–æ–∑–¥–∞–µ–º –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã
                document_loader = DocumentLoader()
                text_splitter = TextSplitter(config)
                indexer = Indexer(config)
                
                # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã
                all_documents = []
                
                # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º .txt —Ñ–∞–π–ª—ã
                for filepath in folder_path.rglob("*.txt"):
                    try:
                        loaded_docs = document_loader.load_text_file(filepath)
                        chunks = text_splitter.split_documents(loaded_docs)
                        processed_chunks = _process_chunks(chunks, filepath, folder_path_resolved, config)
                        all_documents.extend(processed_chunks)
                    except Exception as e:
                        logger.exception(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ —Ñ–∞–π–ª–∞ {filepath}: {e}")
                        raise IndexingError(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ —Ñ–∞–π–ª–∞ {filepath}: {e}")
                
                # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º .md —Ñ–∞–π–ª—ã
                for filepath in folder_path.rglob("*.md"):
                    try:
                        loaded_docs = document_loader.load_text_file(filepath)
                        chunks = text_splitter.split_documents(loaded_docs)
                        processed_chunks = _process_chunks(chunks, filepath, folder_path_resolved, config)
                        all_documents.extend(processed_chunks)
                    except Exception as e:
                        logger.exception(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ —Ñ–∞–π–ª–∞ {filepath}: {e}")
                        raise IndexingError(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ —Ñ–∞–π–ª–∞ {filepath}: {e}")
                
                # –ò–Ω–¥–µ–∫—Å–∏—Ä—É–µ–º –≤—Å–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã
                success, status = await indexer.index_documents(all_documents, client, folder_path_resolved)
        
        # –û–±–Ω–æ–≤–ª—è–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é (—Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –Ω–µ –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ –Ω–∞—Ä–µ–∑–∞–Ω–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã)
        if success and pre_chunked_documents is None:
            config.is_indexed = True
            config_manager.save(config)
        
        return success, status
        
    except IndexingError as e:
        logger.exception(f"–û—à–∏–±–∫–∞ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏: {e}")
        return False, f"indexing_error: {str(e)}"
    except Exception as e:
        logger.exception(f"–ù–µ–æ–∂–∏–¥–∞–Ω–Ω–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏: {e}")
        return False, f"indexing_error: {str(e)}"


def _process_chunks(chunks: List[Document], filepath: Path, folder_path_resolved: Path, config: Config) -> List[Document]:
    """
    –û–±—Ä–∞–±–æ—Ç–∫–∞ —á–∞–Ω–∫–æ–≤: –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –∏ –≤—ã—á–∏—Å–ª–µ–Ω–∏–µ –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã—Ö –ø—É—Ç–µ–π.
    
    Args:
        chunks (List[Document]): –°–ø–∏—Å–æ–∫ —á–∞–Ω–∫–æ–≤.
        filepath (Path): –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É.
        folder_path_resolved (Path): –ü—É—Ç—å –∫ –∫–æ—Ä–Ω–µ–≤–æ–π –ø–∞–ø–∫–µ –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏—è –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã—Ö –ø—É—Ç–µ–π.
        config (Config): –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è.
        
    Returns:
        List[Document]: –°–ø–∏—Å–æ–∫ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö —á–∞–Ω–∫–æ–≤.
    """
    try:
        # –ü–æ–ª—É—á–µ–Ω–∏–µ –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ–≥–æ –ø—É—Ç–∏ —Ñ–∞–π–ª–∞ –æ—Ç –∫–æ—Ä–Ω–µ–≤–æ–π –ø–∞–ø–∫–∏
        abs_filepath = filepath.resolve()
        relative_source_path = abs_filepath.relative_to(folder_path_resolved)
    except ValueError:
        # –ï—Å–ª–∏ —Ñ–∞–π–ª –Ω–µ –≤ –∫–æ—Ä–Ω–µ–≤–æ–π –ø–∞–ø–∫–µ, –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–ª—å–∫–æ –∏–º—è —Ñ–∞–π–ª–∞
        logger.warning(f"–§–∞–π–ª {filepath} –Ω–µ –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –≤–Ω—É—Ç—Ä–∏ {folder_path_resolved}. –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Ç–æ–ª—å–∫–æ –∏–º—è —Ñ–∞–π–ª–∞.")
        relative_source_path = abs_filepath.name
    
    # –î–æ–±–∞–≤–ª—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –∫ —á–∞–Ω–∫–∞–º —Å –ø–æ–º–æ—â—å—é MetadataManager
    # –£—á–∏—Ç—ã–≤–∞–µ–º –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –∏–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
    if config.enable_metadata_extraction:
        for chunk in chunks:
            # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–µ –ø–æ–ª—è –∏–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
            chunk = metadata_manager.add_metadata_to_chunk(chunk, filepath, config.metadata_custom_fields)
            # –û–±–Ω–æ–≤–ª—è–µ–º source –≤ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
            chunk.metadata["source"] = str(relative_source_path)
    else:
        # –ï—Å–ª–∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –æ—Ç–∫–ª—é—á–µ–Ω–æ, –¥–æ–±–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ –±–∞–∑–æ–≤—ã–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
        for chunk in chunks:
            chunk.metadata["source"] = str(relative_source_path)
    
    return chunks


def _process_documents_without_chunking(documents: List[Document], filepath: Path, folder_path_resolved: Path, config: Config) -> List[Document]:
    """
    –û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –±–µ–∑ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–≥–æ —Ä–∞–∑–±–∏–µ–Ω–∏—è –Ω–∞ —á–∞–Ω–∫–∏ (–¥–ª—è –º–Ω–æ–≥–æ—É—Ä–æ–≤–Ω–µ–≤–æ–≥–æ —á–∞–Ω–∫–∏–Ω–≥–∞).
    
    Args:
        documents (List[Document]): –°–ø–∏—Å–æ–∫ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤.
        filepath (Path): –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É.
        folder_path_resolved (Path): –ü—É—Ç—å –∫ –∫–æ—Ä–Ω–µ–≤–æ–π –ø–∞–ø–∫–µ –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏—è –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã—Ö –ø—É—Ç–µ–π.
        config (Config): –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è.
        
    Returns:
        List[Document]: –°–ø–∏—Å–æ–∫ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤.
    """
    try:
        # –ü–æ–ª—É—á–µ–Ω–∏–µ –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ–≥–æ –ø—É—Ç–∏ —Ñ–∞–π–ª–∞ –æ—Ç –∫–æ—Ä–Ω–µ–≤–æ–π –ø–∞–ø–∫–∏
        abs_filepath = filepath.resolve()
        relative_source_path = abs_filepath.relative_to(folder_path_resolved)
    except ValueError:
        # –ï—Å–ª–∏ —Ñ–∞–π–ª –Ω–µ –≤ –∫–æ—Ä–Ω–µ–≤–æ–π –ø–∞–ø–∫–µ, –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–ª—å–∫–æ –∏–º—è —Ñ–∞–π–ª–∞
        logger.warning(f"–§–∞–π–ª {filepath} –Ω–µ –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –≤–Ω—É—Ç—Ä–∏ {folder_path_resolved}. –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Ç–æ–ª—å–∫–æ –∏–º—è —Ñ–∞–π–ª–∞.")
        relative_source_path = abs_filepath.name
    
    # –î–æ–±–∞–≤–ª—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –∫ –¥–æ–∫—É–º–µ–Ω—Ç–∞–º —Å –ø–æ–º–æ—â—å—é MetadataManager
    # –£—á–∏—Ç—ã–≤–∞–µ–º –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –∏–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
    if config.enable_metadata_extraction:
        for doc in documents:
            # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–µ –ø–æ–ª—è –∏–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
            doc = metadata_manager.add_metadata_to_chunk(doc, filepath, config.metadata_custom_fields)
            # –û–±–Ω–æ–≤–ª—è–µ–º source –≤ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
            doc.metadata["source"] = str(relative_source_path)
    else:
        # –ï—Å–ª–∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –æ—Ç–∫–ª—é—á–µ–Ω–æ, –¥–æ–±–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ –±–∞–∑–æ–≤—ã–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
        for doc in documents:
            doc.metadata["source"] = str(relative_source_path)
    
    return documents


def run_indexing_from_config():
    """
    –ó–∞–ø—É—Å–∫–∞–µ—Ç –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—é, –∏—Å–ø–æ–ª—å–∑—É—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –∏–∑ config.json.
    
    Returns:
        Tuple[bool, str]: (—É—Å–ø–µ—Ö, —Å—Ç–∞—Ç—É—Å)
    """
    try:
        # –í—ã–∑–æ–≤ –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–π —Ñ—É–Ω–∫—Ü–∏–∏ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏
        result = asyncio.run(run_indexing_logic())
        return result
    except Exception as e:
        logger.exception(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏: {e}")
        return False, f"indexing_error: {str(e)}"
====================================================================================================
==================== –ö–û–ù–ï–¶ –§–ê–ô–õ–ê: core\indexing\indexer.py ====================



==================== –ù–ê–ß–ê–õ–û –§–ê–ô–õ–ê: core\indexing\indexer_additional.py ====================
# –§–∞–π–ª: core\indexing\indexer_additional.py
====================================================================================================
"""–î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ –¥–ª—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏."""

import logging
from typing import Tuple

from core.indexer import run_indexing_logic

logger = logging.getLogger(__name__)


def run_indexing_from_config() -> Tuple[bool, str]:
    """
    –ó–∞–ø—É—Å–∫–∞–µ—Ç –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—é, –∏—Å–ø–æ–ª—å–∑—É—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –∏–∑ config.json.
    
    Returns:
        Tuple[bool, str]: (—É—Å–ø–µ—Ö, —Å—Ç–∞—Ç—É—Å)
    """
    try:
        # –í—ã–∑–æ–≤ —Ñ—É–Ω–∫—Ü–∏–∏ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏ —Å –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏ –∏–∑ –∫–æ–Ω—Ñ–∏–≥–∞
        success, status = run_indexing_logic()
        return success, status
    except Exception as e:
        logger.exception(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏: {e}")
        return False, f"indexing_error: {str(e)}"
====================================================================================================
==================== –ö–û–ù–ï–¶ –§–ê–ô–õ–ê: core\indexing\indexer_additional.py ====================



==================== –ù–ê–ß–ê–õ–û –§–ê–ô–õ–ê: core\indexing\indexer_component.py ====================
# –§–∞–π–ª: core\indexing\indexer_component.py
====================================================================================================
"""–ú–æ–¥—É–ª—å –¥–ª—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ Qdrant."""

import logging
from pathlib import Path
from typing import List, Optional, Tuple

import psutil
from langchain_core.documents import Document
from langchain_qdrant import QdrantVectorStore, RetrievalMode
from qdrant_client import QdrantClient

from config.settings import Config
from core.embedding.embeddings import get_dense_embedder, get_device
from core.embedding.sparse_embedding_adapter import SparseEmbeddingAdapter
from core.qdrant.qdrant_client import aget_qdrant_client

logger = logging.getLogger(__name__)


class Indexer:
    """–ö–ª–∞—Å—Å –¥–ª—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ Qdrant."""

    def __init__(self, config: Config):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∏–Ω–¥–µ–∫—Å–∞—Ç–æ—Ä–∞.
        
        Args:
            config (Config): –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è.
        """
        self.config = config
        self.index_dense = getattr(config, 'index_dense', True)
        self.index_bm25 = getattr(config, 'index_bm25', False)
        self.index_hybrid = getattr(config, 'index_hybrid', False)
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–ª—è –≥–∏–±—Ä–∏–¥–Ω–æ–≥–æ —Ä–µ–∂–∏–º–∞
        if self.index_hybrid and not self.index_dense:
            raise IndexingError("Hybrid mode requires index_dense=True")
        
        # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–∞ –±–∞—Ç—á–∞
        mem = psutil.virtual_memory().available
        self.batch_size = (
            config.indexing_batch_size 
            if mem > config.memory_threshold 
            else config.indexing_batch_size // 2
        )
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è sparse embedding –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
        self.sparse_emb = None
        if config.use_bm25 and (self.index_bm25 or self.index_hybrid):
            try:
                from core.embedding.sparse_embedding_adapter import SparseEmbeddingAdapter
                self.sparse_emb = SparseEmbeddingAdapter(config)
                logger.info(f"Native BM25 sparse embedding adapter initialized")
            except ImportError:
                logger.warning("–ù–µ —É–¥–∞–ª–æ—Å—å –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å native BM25: –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—è BM25/hybrid –±—É–¥–µ—Ç –ø—Ä–æ–ø—É—â–µ–Ω–∞ –ª–∏–±–æ –≤—ã–ø–æ–ª–Ω–∏—Ç—Å—è —Ç–æ–ª—å–∫–æ dense —á–∞—Å—Ç—å.")
                self.sparse_emb = None
            except Exception as e:
                logger.exception(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ native BM25 sparse embedding adapter: {e}")
                self.sparse_emb = None

    async def index_documents(
        self, 
        documents: List[Document], 
        client: Optional[QdrantClient] = None,
        folder_path_resolved: Optional[Path] = None
    ) -> Tuple[bool, str]:
        """
        –ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è —Å–ø–∏—Å–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤.
        
        Args:
            documents (List[Document]): –°–ø–∏—Å–æ–∫ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏.
            client (QdrantClient, optional): –ö–ª–∏–µ–Ω—Ç Qdrant. –ï—Å–ª–∏ –Ω–µ —É–∫–∞–∑–∞–Ω, —Å–æ–∑–¥–∞–µ—Ç—Å—è –Ω–æ–≤—ã–π.
            folder_path_resolved (Path, optional): –ü—É—Ç—å –∫ –∫–æ—Ä–Ω–µ–≤–æ–π –ø–∞–ø–∫–µ –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏—è –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã—Ö –ø—É—Ç–µ–π.
            
        Returns:
            Tuple[bool, str]: (—É—Å–ø–µ—Ö, —Å—Ç–∞—Ç—É—Å)
        """
        if not documents:
            return True, "indexed_successfully_no_docs"
        
        # –ï—Å–ª–∏ –∫–ª–∏–µ–Ω—Ç –Ω–µ –ø–µ—Ä–µ–¥–∞–Ω, —Å–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–π
        if client is None:
            client = await aget_qdrant_client(self.config)
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –∫–æ–ª–ª–µ–∫—Ü–∏–∏ –∏ —É–¥–∞–ª—è–µ–º –µ—ë, –µ—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω–∞ –ø–µ—Ä–µ–∑–∞–ø–∏—Å—å
        try:
            client.get_collection(self.config.collection_name)
            # –ï—Å–ª–∏ –∫–æ–ª–ª–µ–∫—Ü–∏—è —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –∏ –≤–∫–ª—é—á–µ–Ω–∞ –ø–µ—Ä–µ–∑–∞–ø–∏—Å—å, —É–¥–∞–ª—è–µ–º –µ—ë
            if self.config.force_recreate:
                client.delete_collection(self.config.collection_name)
        except Exception as e:
            # –ö–æ–ª–ª–µ–∫—Ü–∏—è –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –∏–ª–∏ –ø—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–æ–≤–µ—Ä–∫–µ ‚Äî –ª–æ–≥–∏—Ä—É–µ–º –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏ –∏ –ø—Ä–æ–¥–æ–ª–∂–∞–µ–º
            logger.debug(f"–ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–æ–ª–ª–µ–∫—Ü–∏–∏ –∑–∞–≤–µ—Ä—à–∏–ª–∞—Å—å —Å –æ—à–∏–±–∫–æ–π –∏–ª–∏ –∫–æ–ª–ª–µ–∫—Ü–∏—è –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç: {e}")
        
        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —ç–º–±–µ–¥–¥–µ—Ä–∞
        device = get_device(self.config.device)
        dense_embedder = get_dense_embedder(self.config, device)
        
        # –û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –ø–∞—Ä—Ç–∏—è–º–∏
        docs_batch = []
        collection_created = False
        docs_processed = 0
        
        for doc in documents:
            docs_batch.append(doc)
            docs_processed += 1
            
            # –ï—Å–ª–∏ –Ω–∞–±—Ä–∞–ª–∞—Å—å –ø–æ–ª–Ω–∞—è –ø–∞—Ä—Ç–∏—è, –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –µ—ë
            if len(docs_batch) >= self.batch_size:
                try:
                    await self._process_batch(
                        docs_batch, 
                        dense_embedder, 
                        client, 
                        collection_created,
                        folder_path_resolved
                    )
                    collection_created = True
                    docs_batch = []
                except Exception as e:
                    logger.exception(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –ø–∞—Ä—Ç–∏–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {e}")
                    raise IndexingError(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –ø–∞—Ä—Ç–∏–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {e}")
        
        return True, "indexed_successfully"

    async def _process_batch(
        self, 
        docs_batch: List[Document], 
        dense_embedder, 
        client: QdrantClient, 
        collection_created: bool,
        folder_path_resolved: Optional[Path] = None
    ):
        """
        –û–±—Ä–∞–±–æ—Ç–∫–∞ –ø–∞—Ä—Ç–∏–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤.
        
        Args:
            docs_batch (List[Document]): –ü–∞—Ä—Ç–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤.
            dense_embedder: –≠–º–±–µ–¥–¥–µ—Ä –¥–ª—è dense –≤–µ–∫—Ç–æ—Ä–æ–≤.
            client (QdrantClient): –ö–ª–∏–µ–Ω—Ç Qdrant.
            collection_created (bool): –§–ª–∞–≥, —É–∫–∞–∑—ã–≤–∞—é—â–∏–π, –±—ã–ª–∞ –ª–∏ –∫–æ–ª–ª–µ–∫—Ü–∏—è —É–∂–µ —Å–æ–∑–¥–∞–Ω–∞.
            folder_path_resolved (Path, optional): –ü—É—Ç—å –∫ –∫–æ—Ä–Ω–µ–≤–æ–π –ø–∞–ø–∫–µ –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏—è –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã—Ö –ø—É—Ç–µ–π.
        """
        if not collection_created:
            # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—É—é –∫–æ–ª–ª–µ–∫—Ü–∏—é –¥–ª—è –ø–µ—Ä–≤–æ–π –ø–∞—Ä—Ç–∏–∏
            await self._create_collection(docs_batch, dense_embedder, client)
        else:
            # –î–æ–±–∞–≤–ª—è–µ–º –∫ —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–π –∫–æ–ª–ª–µ–∫—Ü–∏–∏
            await self._add_to_collection(docs_batch, dense_embedder, client)

    async def _create_collection(self, docs_batch: List[Document], dense_embedder, client: QdrantClient):
        """
        –°–æ–∑–¥–∞–Ω–∏–µ –∫–æ–ª–ª–µ–∫—Ü–∏–∏ –∏ –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ –ø–µ—Ä–≤–æ–π –ø–∞—Ä—Ç–∏–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤.
        
        Args:
            docs_batch (List[Document]): –ü–∞—Ä—Ç–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤.
            dense_embedder: –≠–º–±–µ–¥–¥–µ—Ä –¥–ª—è dense –≤–µ–∫—Ç–æ—Ä–æ–≤.
            client (QdrantClient): –ö–ª–∏–µ–Ω—Ç Qdrant.
        """
        if self.index_hybrid and self.sparse_emb:
            logger.info(f"Creating hybrid Qdrant collection '{self.config.collection_name}' with dense and sparse embeddings")
            # –£–¥–∞–ª—è–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â—É—é –∫–æ–ª–ª–µ–∫—Ü–∏—é, –µ—Å–ª–∏ –æ–Ω–∞ –µ—Å—Ç—å
            try:
                self.client.delete_collection(self.config.collection_name)
            except Exception:
                pass  # –ö–æ–ª–ª–µ–∫—Ü–∏—è –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –∏–ª–∏ –æ—à–∏–±–∫–∞ –ø—Ä–∏ —É–¥–∞–ª–µ–Ω–∏–∏
            
            QdrantVectorStore.from_documents(
                documents=docs_batch,
                url=self.config.qdrant_url,
                collection_name=self.config.collection_name,
                embedding=dense_embedder if self.index_dense else None,
                vector_name="dense_vector",
                sparse_embedding=self.sparse_emb if (self.config.use_bm25 and (self.index_bm25 or self.index_hybrid)) else None,
                sparse_vector_name=self.config.sparse_vector_name if (self.config.use_bm25 and (self.index_bm25 or self.index_hybrid)) else "sparse_vector",
                retrieval_mode=RetrievalMode.HYBRID,
                batch_size=self.config.indexing_batch_size,
            )
        elif self.index_bm25 and self.sparse_emb and not self.index_dense:
            logger.info(f"Creating sparse-only Qdrant collection '{self.config.collection_name}' using native BM25 sparse embeddings")
            # –£–¥–∞–ª—è–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â—É—é –∫–æ–ª–ª–µ–∫—Ü–∏—é, –µ—Å–ª–∏ –æ–Ω–∞ –µ—Å—Ç—å
            try:
                self.client.delete_collection(self.config.collection_name)
            except Exception:
                pass  # –ö–æ–ª–ª–µ–∫—Ü–∏—è –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –∏–ª–∏ –æ—à–∏–±–∫–∞ –ø—Ä–∏ —É–¥–∞–ª–µ–Ω–∏–∏
            
            QdrantVectorStore.from_documents(
                documents=docs_batch,
                url=self.config.qdrant_url,
                collection_name=self.config.collection_name,
                embedding=dense_embedder if self.index_dense else None,
                vector_name="dense_vector",
                sparse_embedding=self.sparse_emb if (self.config.use_bm25 and self.index_bm25) else None,
                sparse_vector_name=self.config.sparse_vector_name if (self.config.use_bm25 and self.index_bm25) else "sparse_vector",
                retrieval_mode=RetrievalMode.SPARSE,
                batch_size=self.config.indexing_batch_size,
            )
        elif self.index_dense:
            logger.info(f"Creating dense-only Qdrant collection '{self.config.collection_name}'")
            # –£–¥–∞–ª—è–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â—É—é –∫–æ–ª–ª–µ–∫—Ü–∏—é, –µ—Å–ª–∏ –æ–Ω–∞ –µ—Å—Ç—å
            try:
                self.client.delete_collection(self.config.collection_name)
            except Exception:
                pass  # –ö–æ–ª–ª–µ–∫—Ü–∏—è –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –∏–ª–∏ –æ—à–∏–±–∫–∞ –ø—Ä–∏ —É–¥–∞–ª–µ–Ω–∏–∏
            
            QdrantVectorStore.from_documents(
                documents=docs_batch,
                url=self.config.qdrant_url,
                collection_name=self.config.collection_name,
                embedding=dense_embedder,
                vector_name="dense_vector",
                batch_size=self.config.indexing_batch_size,
            )
        else:
            # –ù–µ—á–µ–≥–æ –∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞—Ç—å
            logger.warning("–ù–µ—Ç –ø–æ–¥—Ö–æ–¥—è—â–µ–≥–æ —Ä–µ–∂–∏–º–∞ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏ –¥–ª—è –ø–µ—Ä–≤–æ–π –ø–∞—Ä—Ç–∏–∏ (–ø—Ä–æ–ø—É—Å–∫–∞–µ–º)")

    async def _add_to_collection(self, docs_batch: List[Document], dense_embedder, client: QdrantClient):
        """
        –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –ø–∞—Ä—Ç–∏–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∫ —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–π –∫–æ–ª–ª–µ–∫—Ü–∏–∏.
        
        Args:
            docs_batch (List[Document]): –ü–∞—Ä—Ç–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤.
            dense_embedder: –≠–º–±–µ–¥–¥–µ—Ä –¥–ª—è dense –≤–µ–∫—Ç–æ—Ä–æ–≤.
            client (QdrantClient): –ö–ª–∏–µ–Ω—Ç Qdrant.
        """
        logger.info(f"Adding documents to collection '{self.config.collection_name}' (hybrid={self.index_hybrid and bool(self.sparse_emb)})")
        if self.index_hybrid and self.sparse_emb:
            qdrant_store = QdrantVectorStore(
                client=client,
                collection_name=self.config.collection_name,
                embedding=dense_embedder if self.index_dense else None,
                vector_name="dense_vector",
                sparse_embedding=self.sparse_emb if (self.config.use_bm25 and (self.index_bm25 or self.index_hybrid)) else None,
                sparse_vector_name=self.config.sparse_vector_name if (self.config.use_bm25 and (self.index_bm25 or self.index_hybrid)) else "sparse_vector",
                retrieval_mode=RetrievalMode.HYBRID
            )
        elif self.index_bm25 and self.sparse_emb and not self.index_dense:
            qdrant_store = QdrantVectorStore(
                client=client,
                collection_name=self.config.collection_name,
                embedding=None,
                vector_name=None,
                sparse_embedding=self.sparse_emb if (self.config.use_bm25 and self.index_bm25) else None,
                sparse_vector_name=self.config.sparse_vector_name if (self.config.use_bm25 and self.index_bm25) else "sparse_vector",
                retrieval_mode=RetrievalMode.SPARSE
            )
        else:
            qdrant_store = QdrantVectorStore(
                client=client,
                collection_name=self.config.collection_name,
                embedding=dense_embedder if self.index_dense else None,
                vector_name="dense_vector"
            )
        try:
            await qdrant_store.aadd_documents(docs_batch)
        except Exception as e:
            # –ï—Å–ª–∏ Qdrant –∂–∞–ª—É–µ—Ç—Å—è –Ω–∞ –æ—Ç—Å—É—Ç—Å—Ç–≤–∏–µ sparse –≤–µ–∫—Ç–æ—Ä–æ–≤, –ø—Ä–æ–±—É–µ–º –ø–µ—Ä–µ—Å–æ–∑–¥–∞—Ç—å –∫–æ–ª–ª–µ–∫—Ü–∏—é
            err_msg = str(e)
            logger.exception(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –¥–æ–±–∞–≤–ª–µ–Ω–∏–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ –∫–æ–ª–ª–µ–∫—Ü–∏—é: {err_msg}")
            if 'does not contain sparse vectors' in err_msg or 'does not contain sparse' in err_msg:
                logger.info("–ü–æ—Ö–æ–∂–µ, –≤ –∫–æ–ª–ª–µ–∫—Ü–∏–∏ –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç sparse-–≤–µ–∫—Ç–æ—Ä—ã ‚Äî –ø–µ—Ä–µ—Å–æ–∑–¥–∞—ë–º –∫–æ–ª–ª–µ–∫—Ü–∏—é –∏ –ø–æ–≤—Ç–æ—Ä—è–µ–º –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ")
                # –ü–µ—Ä–µ—Å–æ–∑–¥–∞—ë–º –∫–æ–ª–ª–µ–∫—Ü–∏—é, —É–¥–∞–ª–∏–≤ —Å—É—â–µ—Å—Ç–≤—É—é—â—É—é
                try:
                    if self.index_hybrid and self.sparse_emb:
                        # –£–¥–∞–ª—è–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â—É—é –∫–æ–ª–ª–µ–∫—Ü–∏—é, –µ—Å–ª–∏ –æ–Ω–∞ –µ—Å—Ç—å
                        try:
                            self.client.delete_collection(self.config.collection_name)
                        except Exception:
                            pass  # –ö–æ–ª–ª–µ–∫—Ü–∏—è –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –∏–ª–∏ –æ—à–∏–±–∫–∞ –ø—Ä–∏ —É–¥–∞–ª–µ–Ω–∏–∏
                        
                        QdrantVectorStore.from_documents(
                            documents=docs_batch,
                            url=self.config.qdrant_url,
                            collection_name=self.config.collection_name,
                            embedding=dense_embedder if self.index_dense else None,
                            vector_name="dense_vector",
                            sparse_embedding=self.sparse_emb if (self.config.use_bm25 and (self.index_bm25 or self.index_hybrid)) else None,
                            sparse_vector_name=self.config.sparse_vector_name if (self.config.use_bm25 and (self.index_bm25 or self.index_hybrid)) else "sparse_vector",
                            retrieval_mode=RetrievalMode.HYBRID,
                            batch_size=self.config.indexing_batch_size,
                        )
                    elif self.index_bm25 and self.sparse_emb and not self.index_dense:
                        # –£–¥–∞–ª—è–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â—É—é –∫–æ–ª–ª–µ–∫—Ü–∏—é, –µ—Å–ª–∏ –æ–Ω–∞ –µ—Å—Ç—å
                        try:
                            self.client.delete_collection(self.config.collection_name)
                        except Exception:
                            pass  # –ö–æ–ª–ª–µ–∫—Ü–∏—è –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –∏–ª–∏ –æ—à–∏–±–∫–∞ –ø—Ä–∏ —É–¥–∞–ª–µ–Ω–∏–∏
                        
                        QdrantVectorStore.from_documents(
                            documents=docs_batch,
                            url=self.config.qdrant_url,
                            collection_name=self.config.collection_name,
                            embedding=None,
                            vector_name=None,
                            sparse_embedding=self.sparse_emb if (self.config.use_bm25 and self.index_bm25) else None,
                            sparse_vector_name=self.config.sparse_vector_name if (self.config.use_bm25 and self.index_bm25) else "sparse_vector",
                            retrieval_mode=RetrievalMode.SPARSE,
                            batch_size=self.config.indexing_batch_size,
                        )
                    else:
                        # –£–¥–∞–ª—è–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â—É—é –∫–æ–ª–ª–µ–∫—Ü–∏—é, –µ—Å–ª–∏ –æ–Ω–∞ –µ—Å—Ç—å
                        try:
                            self.client.delete_collection(self.config.collection_name)
                        except Exception:
                            pass  # –ö–æ–ª–ª–µ–∫—Ü–∏—è –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –∏–ª–∏ –æ—à–∏–±–∫–∞ –ø—Ä–∏ —É–¥–∞–ª–µ–Ω–∏–∏
                        
                        QdrantVectorStore.from_documents(
                            documents=docs_batch,
                            url=self.config.qdrant_url,
                            collection_name=self.config.collection_name,
                            embedding=dense_embedder if self.index_dense else None,
                            vector_name="dense_vector",
                            batch_size=self.config.indexing_batch_size,
                        )
                except Exception as recreate_error:
                    logger.exception("–ù–µ —É–¥–∞–ª–æ—Å—å –ø–µ—Ä–µ—Å–æ–∑–¥–∞—Ç—å –∫–æ–ª–ª–µ–∫—Ü–∏—é –¥–ª—è –¥–æ–±–∞–≤–ª–µ–Ω–∏—è sparse-–≤–µ–∫—Ç–æ—Ä–æ–≤")
                    raise IndexingError(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø–µ—Ä–µ—Å–æ–∑–¥–∞—Ç—å –∫–æ–ª–ª–µ–∫—Ü–∏—é: {recreate_error}")
            else:
                # –î–ª—è –¥—Ä—É–≥–∏—Ö –æ—à–∏–±–æ–∫ –ø—Ä–æ—Å—Ç–æ –ø—Ä–æ–±—Ä–∞—Å—ã–≤–∞–µ–º –∏—Å–∫–ª—é—á–µ–Ω–∏–µ
                raise


class IndexingError(Exception):
    """–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–æ–µ –∏—Å–∫–ª—é—á–µ–Ω–∏–µ –¥–ª—è –æ—à–∏–±–æ–∫ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏."""
    pass
====================================================================================================
==================== –ö–û–ù–ï–¶ –§–ê–ô–õ–ê: core\indexing\indexer_component.py ====================



==================== –ù–ê–ß–ê–õ–û –§–ê–ô–õ–ê: core\indexing\metadata_manager.py ====================
# –§–∞–π–ª: core\indexing\metadata_manager.py
====================================================================================================
"""–ú–æ–¥—É–ª—å –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏ –¥–ª—è —á–∞–Ω–∫–æ–≤."""

import hashlib
import logging
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, Optional

try:
    from PIL import Image
    from PIL.ExifTags import TAGS
    HAS_PIL = True
except ImportError:
    HAS_PIL = False

try:
    from PyPDF2 import PdfReader
    HAS_PYPDF2 = True
except ImportError:
    HAS_PYPDF2 = False

try:
    from docx import Document
    HAS_PYTHON_DOCX = True
except ImportError:
    HAS_PYTHON_DOCX = False

logger = logging.getLogger(__name__)

class MetadataManager:
    """–ö–ª–∞—Å—Å –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏ –¥–ª—è —á–∞–Ω–∫–æ–≤."""
    
    def __init__(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è MetadataManager."""
        self.custom_schemas = {}
        
    def add_metadata_to_chunk(self, chunk: Any, file_path: Path, custom_fields: Optional[Dict[str, Any]] = None) -> Any:
        """
        –î–æ–±–∞–≤–ª—è–µ—Ç –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –∫ —á–∞–Ω–∫—É.
        
        Args:
            chunk (Any): –ß–∞–Ω–∫ –¥–æ–∫—É–º–µ–Ω—Ç–∞.
            file_path (Path): –ü—É—Ç—å –∫ –∏—Å—Ö–æ–¥–Ω–æ–º—É —Ñ–∞–π–ª—É.
            custom_fields (Optional[Dict[str, Any]]): –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–µ –ø–æ–ª—è –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö.
            
        Returns:
            Any: –ß–∞–Ω–∫ —Å –¥–æ–±–∞–≤–ª–µ–Ω–Ω—ã–º–∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏.
        """
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ, –µ—Å–ª–∏ –∏—Ö –Ω–µ—Ç
        if not hasattr(chunk, 'metadata') or chunk.metadata is None:
            chunk.metadata = {}
            
        # –î–æ–±–∞–≤–ª—è–µ–º –±–∞–∑–æ–≤—ã–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
        chunk.metadata["source"] = str(file_path)
        chunk.metadata["file_name"] = file_path.name
        chunk.metadata["file_extension"] = file_path.suffix.lower()
        chunk.metadata["file_size"] = file_path.stat().st_size if file_path.exists() else 0
        chunk.metadata["file_modified"] = datetime.fromtimestamp(file_path.stat().st_mtime).isoformat() if file_path.exists() else None
        
        # –î–æ–±–∞–≤–ª—è–µ–º —Ö—ç—à —Ñ–∞–π–ª–∞ –¥–ª—è –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏–∏
        chunk.metadata["file_hash"] = self._get_file_hash(file_path)
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –∏–∑ —Ñ–∞–π–ª–∞
        file_metadata = self.extract_metadata_from_file(file_path)
        chunk.metadata.update(file_metadata)
        
        # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–µ –ø–æ–ª—è
        if custom_fields:
            chunk.metadata.update(custom_fields)
            
        return chunk
    
    def extract_metadata_from_file(self, file_path: Path) -> Dict[str, Any]:
        """
        –ò–∑–≤–ª–µ–∫–∞–µ—Ç –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –∏–∑ –∏—Å—Ö–æ–¥–Ω–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞.
        
        Args:
            file_path (Path): –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É.
            
        Returns:
            Dict[str, Any]: –°–ª–æ–≤–∞—Ä—å —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏.
        """
        metadata = {}
        
        try:
            if file_path.suffix.lower() == '.pdf' and HAS_PYPDF2:
                metadata.update(self._extract_pdf_metadata(file_path))
            elif file_path.suffix.lower() in ['.jpg', '.jpeg', '.png', '.tiff', '.tif'] and HAS_PIL:
                metadata.update(self._extract_image_metadata(file_path))
            elif file_path.suffix.lower() == '.docx' and HAS_PYTHON_DOCX:
                metadata.update(self._extract_docx_metadata(file_path))
            else:
                logger.debug(f"–ù–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π —Ç–∏–ø —Ñ–∞–π–ª–∞ –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö: {file_path}")
        except Exception as e:
            logger.warning(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏–∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –∏–∑ —Ñ–∞–π–ª–∞ {file_path}: {e}")
            
        return metadata
    
    def _extract_pdf_metadata(self, file_path: Path) -> Dict[str, Any]:
        """
        –ò–∑–≤–ª–µ–∫–∞–µ—Ç –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –∏–∑ PDF —Ñ–∞–π–ª–∞.
        
        Args:
            file_path (Path): –ü—É—Ç—å –∫ PDF —Ñ–∞–π–ª—É.
            
        Returns:
            Dict[str, Any]: –°–ª–æ–≤–∞—Ä—å —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏ PDF.
        """
        metadata = {}
        try:
            with open(file_path, 'rb') as f:
                pdf = PdfReader(f)
                if pdf.metadata:
                    # –ò–∑–≤–ª–µ–∫–∞–µ–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
                    pdf_metadata = {
                        "title": pdf.metadata.get("/Title"),
                        "author": pdf.metadata.get("/Author"),
                        "subject": pdf.metadata.get("/Subject"),
                        "creator": pdf.metadata.get("/Creator"),
                        "producer": pdf.metadata.get("/Producer"),
                        "creation_date": pdf.metadata.get("/CreationDate"),
                        "modification_date": pdf.metadata.get("/ModDate"),
                    }
                    # –§–∏–ª—å—Ç—Ä—É–µ–º None –∑–Ω–∞—á–µ–Ω–∏—è
                    metadata.update({k: v for k, v in pdf_metadata.items() if v is not None})
                    
                    # –î–æ–±–∞–≤–ª—è–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–∞–Ω–∏—Ü
                    metadata["page_count"] = len(pdf.pages)
        except Exception as e:
            logger.warning(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏–∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –∏–∑ PDF {file_path}: {e}")
            
        return metadata
    
    def _extract_image_metadata(self, file_path: Path) -> Dict[str, Any]:
        """
        –ò–∑–≤–ª–µ–∫–∞–µ—Ç –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –∏–∑ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è.
        
        Args:
            file_path (Path): –ü—É—Ç—å –∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—é.
            
        Returns:
            Dict[str, Any]: –°–ª–æ–≤–∞—Ä—å —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è.
        """
        metadata = {}
        try:
            image = Image.open(file_path)
            metadata["image_format"] = image.format
            metadata["image_mode"] = image.mode
            metadata["image_size"] = image.size  # (width, height)
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º EXIF –¥–∞–Ω–Ω—ã–µ
            if hasattr(image, '_getexif') and image._getexif():
                exif_data = image._getexif()
                if exif_data:
                    exif = {}
                    for tag_id, value in exif_data.items():
                        tag = TAGS.get(tag_id, tag_id)
                        exif[tag] = value
                    metadata["exif"] = exif
        except Exception as e:
            logger.warning(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏–∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –∏–∑ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è {file_path}: {e}")
            
        return metadata
    
    def _extract_docx_metadata(self, file_path: Path) -> Dict[str, Any]:
        """
        –ò–∑–≤–ª–µ–∫–∞–µ—Ç –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –∏–∑ DOCX —Ñ–∞–π–ª–∞.
        
        Args:
            file_path (Path): –ü—É—Ç—å –∫ DOCX —Ñ–∞–π–ª—É.
            
        Returns:
            Dict[str, Any]: –°–ª–æ–≤–∞—Ä—å —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏ DOCX.
        """
        metadata = {}
        try:
            doc = Document(file_path)
            # –ò–∑–≤–ª–µ–∫–∞–µ–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
            core_props = doc.core_properties
            doc_metadata = {
                "title": core_props.title,
                "author": core_props.author,
                "subject": core_props.subject,
                "creator": core_props.creator,
                "keywords": core_props.keywords,
                "description": core_props.description,
                "category": core_props.category,
                "created": core_props.created.isoformat() if core_props.created else None,
                "modified": core_props.modified.isoformat() if core_props.modified else None,
            }
            # –§–∏–ª—å—Ç—Ä—É–µ–º None –∑–Ω–∞—á–µ–Ω–∏—è
            metadata.update({k: v for k, v in doc_metadata.items() if v is not None})
            
            # –î–æ–±–∞–≤–ª—è–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–∞–Ω–∏—Ü (–ø—Ä–∏–±–ª–∏–∑–∏—Ç–µ–ª—å–Ω–æ)
            paragraph_count = len(doc.paragraphs)
            metadata["approximate_paragraph_count"] = paragraph_count
        except Exception as e:
            logger.warning(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏–∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –∏–∑ DOCX {file_path}: {e}")
            
        return metadata
    
    def _get_file_hash(self, file_path: Path) -> str:
        """
        –í—ã—á–∏—Å–ª—è–µ—Ç —Ö—ç—à —Ñ–∞–π–ª–∞.
        
        Args:
            file_path (Path): –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É.
            
        Returns:
            str: –•—ç—à —Ñ–∞–π–ª–∞.
        """
        hash_sha256 = hashlib.sha256()
        try:
            with open(file_path, "rb") as f:
                for chunk in iter(lambda: f.read(4096), b""):
                    hash_sha256.update(chunk)
            return hash_sha256.hexdigest()
        except Exception as e:
            logger.warning(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –≤—ã—á–∏—Å–ª–µ–Ω–∏–∏ —Ö—ç—à–∞ —Ñ–∞–π–ª–∞ {file_path}: {e}")
            return ""
    
    def validate_and_normalize_metadata(self, metadata: Dict[str, Any]) -> Dict[str, Any]:
        """
        –í–∞–ª–∏–¥–∏—Ä—É–µ—Ç –∏ –Ω–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ.
        
        Args:
            metadata (Dict[str, Any]): –°–ª–æ–≤–∞—Ä—å —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏.
            
        Returns:
            Dict[str, Any]: –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–π —Å–ª–æ–≤–∞—Ä—å —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏.
        """
        normalized = {}
        
        for key, value in metadata.items():
            # –ü—Ä–∏–≤–æ–¥–∏–º –∫–ª—é—á–∏ –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É –∏ –∑–∞–º–µ–Ω—è–µ–º –ø—Ä–æ–±–µ–ª—ã –Ω–∞ –ø–æ–¥—á–µ—Ä–∫–∏–≤–∞–Ω–∏—è
            normalized_key = key.lower().replace(" ", "_")
            
            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∑–Ω–∞—á–µ–Ω–∏—è –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ç–∏–ø–∞
            if isinstance(value, datetime):
                normalized[normalized_key] = value.isoformat()
            elif isinstance(value, (str, int, float, bool)) or value is None:
                normalized[normalized_key] = value
            else:
                # –î–ª—è —Å–ª–æ–∂–Ω—ã—Ö –æ–±—ä–µ–∫—Ç–æ–≤ –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ —Å—Ç—Ä–æ–∫—É
                normalized[normalized_key] = str(value)
                
        return normalized
    
    def register_custom_schema(self, schema_name: str, schema: Dict[str, Any]):
        """
        –†–µ–≥–∏—Å—Ç—Ä–∏—Ä—É–µ—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫—É—é —Å—Ö–µ–º—É –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö.
        
        Args:
            schema_name (str): –ò–º—è —Å—Ö–µ–º—ã.
            schema (Dict[str, Any]): –°—Ö–µ–º–∞ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö.
        """
        self.custom_schemas[schema_name] = schema
        logger.info(f"–ó–∞—Ä–µ–≥–∏—Å—Ç—Ä–∏—Ä–æ–≤–∞–Ω–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∞—è —Å—Ö–µ–º–∞ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö: {schema_name}")
    
    def get_custom_schema(self, schema_name: str) -> Optional[Dict[str, Any]]:
        """
        –ü–æ–ª—É—á–∞–µ—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫—É—é —Å—Ö–µ–º—É –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö.
        
        Args:
            schema_name (str): –ò–º—è —Å—Ö–µ–º—ã.
            
        Returns:
            Optional[Dict[str, Any]]: –°—Ö–µ–º–∞ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –∏–ª–∏ None.
        """
        return self.custom_schemas.get(schema_name)

# –°–æ–∑–¥–∞–µ–º –≥–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä MetadataManager
metadata_manager = MetadataManager()
====================================================================================================
==================== –ö–û–ù–ï–¶ –§–ê–ô–õ–ê: core\indexing\metadata_manager.py ====================



==================== –ù–ê–ß–ê–õ–û –§–ê–ô–õ–ê: core\indexing\multilevel_chunker.py ====================
# –§–∞–π–ª: core\indexing\multilevel_chunker.py
====================================================================================================
"""–ú–æ–¥—É–ª—å –¥–ª—è –º–Ω–æ–≥–æ—É—Ä–æ–≤–Ω–µ–≤–æ–≥–æ —á–∞–Ω–∫–∏–Ω–≥–∞ –∏ –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏."""

import logging
from typing import Any, Dict, List, Union

from langchain_core.documents import Document

from core.indexing.chunker import get_text_splitter
from core.indexing.paragraph_chunker import ParagraphTextSplitter
from core.indexing.sentence_chunker import SentenceTextSplitter

logger = logging.getLogger(__name__)


class MultiLevelChunker:
    """–ú–Ω–æ–≥–æ—É—Ä–æ–≤–Ω–µ–≤—ã–π —á–∞–Ω–∫–µ—Ä –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–∏—Ö —á–∞–Ω–∫–æ–≤."""
    
    def __init__(
        self,
        macro_chunk_strategy: str = "character",
        macro_chunk_size: int = 10000,
        macro_chunk_overlap: int = 1000,
        macro_paragraphs_per_chunk: int = 5,
        macro_paragraph_overlap: int = 1,
        macro_sentences_per_chunk: int = 10,
        macro_sentence_overlap: int = 1,
        micro_chunk_strategy: str = "character",
        micro_chunk_size: int = 1000,
        micro_chunk_overlap: int = 100,
        micro_paragraphs_per_chunk: int = 3,
        micro_paragraph_overlap: int = 1,
        micro_sentences_per_chunk: int = 5,
        micro_sentence_overlap: int = 1
    ):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–Ω–æ–≥–æ—É—Ä–æ–≤–Ω–µ–≤–æ–≥–æ —á–∞–Ω–∫–µ—Ä–∞ —Å –ø–æ–ª–Ω–æ—Å—Ç—å—é –Ω–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏.
        
        Args:
            macro_chunk_strategy (str): –°—Ç—Ä–∞—Ç–µ–≥–∏—è –º–∞–∫—Ä–æ-—á–∞–Ω–∫–∏–Ω–≥–∞ ("character", "paragraph", "sentence")
            macro_chunk_size (int): –†–∞–∑–º–µ—Ä –º–∞–∫—Ä–æ-—á–∞–Ω–∫–æ–≤ –≤ —Å–∏–º–≤–æ–ª–∞—Ö
            macro_chunk_overlap (int): –ü–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ –º–∞–∫—Ä–æ-—á–∞–Ω–∫–æ–≤ –≤ —Å–∏–º–≤–æ–ª–∞—Ö
            macro_paragraphs_per_chunk (int): –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∞–±–∑–∞—Ü–µ–≤ –≤ –º–∞–∫—Ä–æ-—á–∞–Ω–∫–µ
            macro_paragraph_overlap (int): –ü–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ –∞–±–∑–∞—Ü–µ–≤ –≤ –º–∞–∫—Ä–æ-—á–∞–Ω–∫–∞—Ö
            macro_sentences_per_chunk (int): –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π –≤ –º–∞–∫—Ä–æ-—á–∞–Ω–∫–µ
            macro_sentence_overlap (int): –ü–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π –≤ –º–∞–∫—Ä–æ-—á–∞–Ω–∫–∞—Ö
            micro_chunk_strategy (str): –°—Ç—Ä–∞—Ç–µ–≥–∏—è –º–∏–∫—Ä–æ-—á–∞–Ω–∫–∏–Ω–≥–∞ ("character", "paragraph", "sentence")
            micro_chunk_size (int): –†–∞–∑–º–µ—Ä –º–∏–∫—Ä–æ-—á–∞–Ω–∫–æ–≤ –≤ —Å–∏–º–≤–æ–ª–∞—Ö
            micro_chunk_overlap (int): –ü–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ –º–∏–∫—Ä–æ-—á–∞–Ω–∫–æ–≤ –≤ —Å–∏–º–≤–æ–ª–∞—Ö
            micro_paragraphs_per_chunk (int): –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∞–±–∑–∞—Ü–µ–≤ –≤ –º–∏–∫—Ä–æ-—á–∞–Ω–∫–µ
            micro_paragraph_overlap (int): –ü–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ –∞–±–∑–∞—Ü–µ–≤ –≤ –º–∏–∫—Ä–æ-—á–∞–Ω–∫–∞—Ö
            micro_sentences_per_chunk (int): –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π –≤ –º–∏–∫—Ä–æ-—á–∞–Ω–∫–µ
            micro_sentence_overlap (int): –ü–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π –≤ –º–∏–∫—Ä–æ-—á–∞–Ω–∫–∞—Ö
        """
        self.macro_chunk_strategy = macro_chunk_strategy
        self.macro_chunk_size = macro_chunk_size
        self.macro_chunk_overlap = macro_chunk_overlap
        self.macro_paragraphs_per_chunk = macro_paragraphs_per_chunk
        self.macro_paragraph_overlap = macro_paragraph_overlap
        self.macro_sentences_per_chunk = macro_sentences_per_chunk
        self.macro_sentence_overlap = macro_sentence_overlap
        
        self.micro_chunk_strategy = micro_chunk_strategy
        self.micro_chunk_size = micro_chunk_size
        self.micro_chunk_overlap = micro_chunk_overlap
        self.micro_paragraphs_per_chunk = micro_paragraphs_per_chunk
        self.micro_paragraph_overlap = micro_paragraph_overlap
        self.micro_sentences_per_chunk = micro_sentences_per_chunk
        self.micro_sentence_overlap = micro_sentence_overlap
        
        # –°–æ–∑–¥–∞–µ–º –º–∞–∫—Ä–æ-—á–∞–Ω–∫–µ—Ä
        if macro_chunk_strategy == "paragraph":
            self.macro_chunker = ParagraphTextSplitter(
                paragraphs_per_chunk=macro_paragraphs_per_chunk,
                paragraph_overlap=macro_paragraph_overlap
            )
        elif macro_chunk_strategy == "sentence":
            self.macro_chunker = SentenceTextSplitter(
                sentences_per_chunk=macro_sentences_per_chunk,
                sentence_overlap=macro_sentence_overlap
            )
        else:  # character
            self.macro_chunker = get_text_splitter(
                chunking_strategy="character",
                chunk_size=macro_chunk_size,
                chunk_overlap=macro_chunk_overlap
            )
        
        # –°–æ–∑–¥–∞–µ–º –º–∏–∫—Ä–æ-—á–∞–Ω–∫–µ—Ä
        if micro_chunk_strategy == "paragraph":
            self.micro_chunker = ParagraphTextSplitter(
                paragraphs_per_chunk=micro_paragraphs_per_chunk,
                paragraph_overlap=micro_paragraph_overlap
            )
        elif micro_chunk_strategy == "sentence":
            self.micro_chunker = SentenceTextSplitter(
                sentences_per_chunk=micro_sentences_per_chunk,
                sentence_overlap=micro_sentence_overlap
            )
        else:  # character
            self.micro_chunker = get_text_splitter(
                chunking_strategy="character",
                chunk_size=micro_chunk_size,
                chunk_overlap=micro_chunk_overlap
            )
    
    def create_multilevel_chunks(self, documents: List[Document]) -> List[Dict[str, Any]]:
        """
        –°–æ–∑–¥–∞–Ω–∏–µ –º–Ω–æ–≥–æ—É—Ä–æ–≤–Ω–µ–≤—ã—Ö —á–∞–Ω–∫–æ–≤ –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤.
        
        Args:
            documents (List[Document]): –°–ø–∏—Å–æ–∫ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è —á–∞–Ω–∫–∏–Ω–≥–∞
            
        Returns:
            List[Dict[str, Any]]: –°–ø–∏—Å–æ–∫ –º–Ω–æ–≥–æ—É—Ä–æ–≤–Ω–µ–≤—ã—Ö —á–∞–Ω–∫–æ–≤ —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏
        """
        logger.info(f"–ù–∞—á–∞–ª–æ —Å–æ–∑–¥–∞–Ω–∏—è –º–Ω–æ–≥–æ—É—Ä–æ–≤–Ω–µ–≤—ã—Ö —á–∞–Ω–∫–æ–≤ –∏–∑ {len(documents)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
        
        multilevel_chunks = []
        
        # –°–æ–∑–¥–∞–µ–º –º–∞–∫—Ä–æ-—á–∞–Ω–∫–∏
        macro_chunks = self.macro_chunker.split_documents(documents)
        
        logger.info(f"–°–æ–∑–¥–∞–Ω–æ {len(macro_chunks)} –º–∞–∫—Ä–æ-—á–∞–Ω–∫–æ–≤")
        
        for i, macro_chunk in enumerate(macro_chunks):
            # –°–æ–∑–¥–∞–µ–º –º–∏–∫—Ä–æ-—á–∞–Ω–∫–∏ –∏–∑ –º–∞–∫—Ä–æ-—á–∞–Ω–∫–∞
            micro_chunks = self.micro_chunker.split_text(macro_chunk.page_content)
            
            # –°–æ–∑–¥–∞–µ–º –∑–∞–ø–∏—Å—å –¥–ª—è –º–∞–∫—Ä–æ-—á–∞–Ω–∫–∞ —Å —Å—Å—ã–ª–∫–∞–º–∏ –Ω–∞ –º–∏–∫—Ä–æ-—á–∞–Ω–∫–∏
            multilevel_chunk = {
                "macro_chunk": macro_chunk,
                "micro_chunks": micro_chunks,
                "chunk_index": i,
                "total_macro_chunks": len(macro_chunks),
                "total_micro_chunks": len(micro_chunks)
            }
            
            multilevel_chunks.append(multilevel_chunk)
            
            logger.debug(f"–ú–∞–∫—Ä–æ-—á–∞–Ω–∫ {i+1}: {len(micro_chunks)} –º–∏–∫—Ä–æ-—á–∞–Ω–∫–æ–≤")
        
        logger.info(f"–í—Å–µ–≥–æ —Å–æ–∑–¥–∞–Ω–æ {len(multilevel_chunks)} –º–Ω–æ–≥–æ—É—Ä–æ–≤–Ω–µ–≤—ã—Ö —á–∞–Ω–∫–æ–≤")
        return multilevel_chunks
    
    def get_all_vectors_for_chunk(self, macro_chunk_content: str) -> List[str]:
        """
        –ü–æ–ª—É—á–µ–Ω–∏–µ –≤—Å–µ—Ö —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π –¥–ª—è –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏ –æ–¥–Ω–æ–≥–æ –º–∞–∫—Ä–æ-—á–∞–Ω–∫–∞.
        
        Args:
            macro_chunk_content (str): –°–æ–¥–µ—Ä–∂–∏–º–æ–µ –º–∞–∫—Ä–æ-—á–∞–Ω–∫–∞
            
        Returns:
            List[str]: –°–ø–∏—Å–æ–∫ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π –¥–ª—è –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏
        """
        # –û—Å–Ω–æ–≤–Ω–æ–π —Ç–µ–∫—Å—Ç –º–∞–∫—Ä–æ-—á–∞–Ω–∫–∞
        vectors = [macro_chunk_content]
        
        # –ú–∏–∫—Ä–æ-—á–∞–Ω–∫–∏
        micro_chunks = self.micro_chunker.split_text(macro_chunk_content)
        vectors.extend(micro_chunks)
        
        return vectors


def create_multilevel_chunker_from_config(config: Dict[str, Any]) -> MultiLevelChunker:
    """
    –°–æ–∑–¥–∞–Ω–∏–µ –º–Ω–æ–≥–æ—É—Ä–æ–≤–Ω–µ–≤–æ–≥–æ —á–∞–Ω–∫–µ—Ä–∞ –∏–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏.
    
    Args:
        config (Dict[str, Any]): –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
        
    Returns:
        MultiLevelChunker: –≠–∫–∑–µ–º–ø–ª—è—Ä –º–Ω–æ–≥–æ—É—Ä–æ–≤–Ω–µ–≤–æ–≥–æ —á–∞–Ω–∫–µ—Ä–∞
    """
    return MultiLevelChunker(
        macro_chunk_strategy=config.get('multilevel_macro_strategy', 'character'),
        macro_chunk_size=config.get('multilevel_macro_chunk_size', 10000),
        macro_chunk_overlap=config.get('multilevel_macro_chunk_overlap', 1000),
        macro_paragraphs_per_chunk=config.get('multilevel_macro_paragraphs_per_chunk', 5),
        macro_paragraph_overlap=config.get('multilevel_macro_paragraph_overlap', 1),
        macro_sentences_per_chunk=config.get('multilevel_macro_sentences_per_chunk', 10),
        macro_sentence_overlap=config.get('multilevel_macro_sentence_overlap', 1),
        micro_chunk_strategy=config.get('multilevel_micro_strategy', 'character'),
        micro_chunk_size=config.get('multilevel_micro_chunk_size', 1000),
        micro_chunk_overlap=config.get('multilevel_micro_chunk_overlap', 100),
        micro_paragraphs_per_chunk=config.get('multilevel_micro_paragraphs_per_chunk', 3),
        micro_paragraph_overlap=config.get('multilevel_micro_paragraph_overlap', 1),
        micro_sentences_per_chunk=config.get('multilevel_micro_sentences_per_chunk', 5),
        micro_sentence_overlap=config.get('multilevel_micro_sentence_overlap', 1)
    )


def create_flexible_multilevel_chunker(
    macro_strategy: str = "character",
    macro_size: Union[int, str] = 10000,
    micro_strategy: str = "character",
    micro_size: Union[int, str] = 1000
) -> MultiLevelChunker:
    """
    –°–æ–∑–¥–∞–Ω–∏–µ –≥–∏–±–∫–æ–≥–æ –º–Ω–æ–≥–æ—É—Ä–æ–≤–Ω–µ–≤–æ–≥–æ —á–∞–Ω–∫–µ—Ä–∞ —Å –ø—Ä–æ—Å—Ç—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏.
    
    Args:
        macro_strategy (str): –°—Ç—Ä–∞—Ç–µ–≥–∏—è –º–∞–∫—Ä–æ-—á–∞–Ω–∫–∏–Ω–≥–∞
        macro_size (Union[int, str]): –†–∞–∑–º–µ—Ä –º–∞–∫—Ä–æ-—á–∞–Ω–∫–æ–≤
        micro_strategy (str): –°—Ç—Ä–∞—Ç–µ–≥–∏—è –º–∏–∫—Ä–æ-—á–∞–Ω–∫–∏–Ω–≥–∞
        micro_size (Union[int, str]): –†–∞–∑–º–µ—Ä –º–∏–∫—Ä–æ-—á–∞–Ω–∫–æ–≤
        
    Returns:
        MultiLevelChunker: –≠–∫–∑–µ–º–ø–ª—è—Ä –º–Ω–æ–≥–æ—É—Ä–æ–≤–Ω–µ–≤–æ–≥–æ —á–∞–Ω–∫–µ—Ä–∞
    """
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏–π –∏ —Ä–∞–∑–º–µ—Ä–æ–≤
    macro_params = {}
    micro_params = {}
    
    if macro_strategy == "character":
        macro_params = {
            "macro_chunk_strategy": "character",
            "macro_chunk_size": int(macro_size) if isinstance(macro_size, (int, float)) else 10000,
            "macro_chunk_overlap": int(int(macro_size) * 0.1) if isinstance(macro_size, (int, float)) else 1000
        }
    elif macro_strategy == "paragraph":
        macro_params = {
            "macro_chunk_strategy": "paragraph",
            "macro_paragraphs_per_chunk": int(macro_size) if isinstance(macro_size, (int, float)) else 5,
            "macro_paragraph_overlap": max(1, int(int(macro_size) * 0.2)) if isinstance(macro_size, (int, float)) else 1
        }
    elif macro_strategy == "sentence":
        macro_params = {
            "macro_chunk_strategy": "sentence",
            "macro_sentences_per_chunk": int(macro_size) if isinstance(macro_size, (int, float)) else 10,
            "macro_sentence_overlap": max(1, int(int(macro_size) * 0.1)) if isinstance(macro_size, (int, float)) else 1
        }
    
    if micro_strategy == "character":
        micro_params = {
            "micro_chunk_strategy": "character",
            "micro_chunk_size": int(micro_size) if isinstance(micro_size, (int, float)) else 1000,
            "micro_chunk_overlap": int(int(micro_size) * 0.1) if isinstance(micro_size, (int, float)) else 100
        }
    elif micro_strategy == "paragraph":
        micro_params = {
            "micro_chunk_strategy": "paragraph",
            "micro_paragraphs_per_chunk": int(micro_size) if isinstance(micro_size, (int, float)) else 3,
            "micro_paragraph_overlap": max(1, int(int(micro_size) * 0.3)) if isinstance(micro_size, (int, float)) else 1
        }
    elif micro_strategy == "sentence":
        micro_params = {
            "micro_chunk_strategy": "sentence",
            "micro_sentences_per_chunk": int(micro_size) if isinstance(micro_size, (int, float)) else 5,
            "micro_sentence_overlap": max(1, int(int(micro_size) * 0.2)) if isinstance(micro_size, (int, float)) else 1
        }
    
    # –û–±—ä–µ–¥–∏–Ω—è–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
    params = {**macro_params, **micro_params}
    
    return MultiLevelChunker(**params)
====================================================================================================
==================== –ö–û–ù–ï–¶ –§–ê–ô–õ–ê: core\indexing\multilevel_chunker.py ====================



==================== –ù–ê–ß–ê–õ–û –§–ê–ô–õ–ê: core\indexing\multilevel_indexer.py ====================
# –§–∞–π–ª: core\indexing\multilevel_indexer.py
====================================================================================================
"""–ú–æ–¥—É–ª—å –¥–ª—è –º–Ω–æ–≥–æ—É—Ä–æ–≤–Ω–µ–≤–æ–π –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ Qdrant."""

import logging
import uuid  # –î–æ–±–∞–≤–ª—è–µ–º –∏–º–ø–æ—Ä—Ç uuid
from typing import Any, Dict, List

from langchain_core.documents import Document
from qdrant_client.http.models import SparseVectorParams
from qdrant_client.models import (
    Distance,
    MultiVectorComparator,
    MultiVectorConfig,
    PointStruct,
    VectorParams,
)

from config.settings import Config
from core.embedding.embedding_manager import EmbeddingManager
from core.embedding.sparse_embedding_adapter import SparseEmbeddingAdapter
from core.indexing.multilevel_chunker import (
    MultiLevelChunker,
)
from core.qdrant.qdrant_client import get_qdrant_client
from core.utils.constants import DEFAULT_COLLECTION_NAME

# –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º SparseEmbeddingAdapter
try:
    from core.embedding.sparse_embedding_adapter import SparseEmbeddingAdapter
    SPARSE_AVAILABLE = True
except ImportError:
    SPARSE_AVAILABLE = False
    logger = logging.getLogger(__name__)
    logger.warning("Sparse embedding adapter –Ω–µ –¥–æ—Å—Ç—É–ø–µ–Ω.")

logger = logging.getLogger(__name__)


class MultiLevelIndexer:
    """–ò–Ω–¥–µ–∫—Å–∞—Ç–æ—Ä —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π –º–Ω–æ–≥–æ—É—Ä–æ–≤–Ω–µ–≤–æ–≥–æ —á–∞–Ω–∫–∏–Ω–≥–∞."""
    
    def __init__(self, config: Config):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–Ω–æ–≥–æ—É—Ä–æ–≤–Ω–µ–≤–æ–≥–æ –∏–Ω–¥–µ–∫—Å–∞—Ç–æ—Ä–∞.
        
        Args:
            config (Config): –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è
        """
        self.config = config
        self.client = get_qdrant_client()
        self.embedding_manager = EmbeddingManager.get_instance()
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è sparse embedding –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
        self.sparse_emb = None
        if getattr(config, 'use_bm25', False) and (getattr(config, 'index_bm25', False) or getattr(config, 'index_hybrid', False)):
            if SPARSE_AVAILABLE:
                try:
                    self.sparse_emb = SparseEmbeddingAdapter(config)
                    logger.info(f"Sparse embedding adapter initialized with native BM25")
                except Exception as e:
                    logger.exception(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ native sparse embedding adapter: {e}")
                    self.sparse_emb = None
            else:
                logger.warning("Native BM25 sparse embedding –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω: sparse embedding –±—É–¥–µ—Ç –ø—Ä–æ–ø—É—â–µ–Ω.")
        
        # –°–æ–∑–¥–∞–µ–º –º–Ω–æ–≥–æ—É—Ä–æ–≤–Ω–µ–≤—ã–π —á–∞–Ω–∫–µ—Ä
        self.multilevel_chunker = self._create_multilevel_chunker()
    
    def _ensure_collection_exists(self):
        """–£–±–µ–¥–∏—Ç—å—Å—è, —á—Ç–æ –∫–æ–ª–ª–µ–∫—Ü–∏—è —Å—É—â–µ—Å—Ç–≤—É–µ—Ç —Å –ø—Ä–∞–≤–∏–ª—å–Ω–æ–π –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–µ–π."""
        collection_name = getattr(self.config, 'collection_name', DEFAULT_COLLECTION_NAME)
        
        # –ü–æ–ª—É—á–∞–µ–º —Ä–∞–∑–º–µ—Ä –≤–µ–∫—Ç–æ—Ä–∞ –æ—Ç —ç–º–±–µ–¥–¥–µ—Ä–∞
        sample_vector = self.embedding_manager.embed_query("test")
        vector_size = len(sample_vector)
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ –∫–æ–ª–ª–µ–∫—Ü–∏–∏
        try:
            self.client.get_collection(collection_name)
            logger.info(f"–ö–æ–ª–ª–µ–∫—Ü–∏—è {collection_name} —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç")
            # TODO: –ü—Ä–æ–≤–µ—Ä–∏—Ç—å, —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç –ª–∏ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –∫–æ–ª–ª–µ–∫—Ü–∏–∏ –Ω–∞—à–∏–º —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è–º
        except Exception:
            # –ö–æ–ª–ª–µ–∫—Ü–∏—è –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç, —Å–æ–∑–¥–∞–µ–º –Ω–æ–≤—É—é
            logger.info(f"–°–æ–∑–¥–∞–Ω–∏–µ –∫–æ–ª–ª–µ–∫—Ü–∏–∏ {collection_name}")
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –≤–µ–∫—Ç–æ—Ä–æ–≤ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ä–µ–∂–∏–º–∞
            vectors_config = {}
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω—É–∂–Ω–æ –ª–∏ –∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞—Ç—å dense –≤–µ–∫—Ç–æ—Ä—ã
            index_dense = getattr(self.config, 'index_dense', True)
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω—É–∂–Ω–æ –ª–∏ –∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞—Ç—å sparse –≤–µ–∫—Ç–æ—Ä—ã
            index_sparse = (
                self.sparse_emb and 
                (getattr(self.config, 'index_bm25', False) or getattr(self.config, 'index_hybrid', False))
            )
            
            # Dense –≤–µ–∫—Ç–æ—Ä—ã —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π –º—É–ª—å—Ç–∏–≤–µ–∫—Ç–æ—Ä–æ–≤
            if index_dense:
                vectors_config["dense_vector"] = VectorParams(
                    size=vector_size,
                    distance=Distance.COSINE,
                    multivector_config=MultiVectorConfig(
                        comparator=MultiVectorComparator.MAX_SIM
                    )
                )
                
            # Sparse –≤–µ–∫—Ç–æ—Ä—ã (–µ—Å–ª–∏ –Ω—É–∂–Ω—ã)
            sparse_vectors_config = {}
            if index_sparse:
                from qdrant_client.models import SparseVectorParams, Modifier, SparseIndexParams
                sparse_vectors_config[getattr(self.config, 'sparse_vector_name', 'sparse_vector')] = SparseVectorParams(
                    modifier=Modifier.IDF,  # Enable BM25-like IDF
                    index=SparseIndexParams(on_disk=True)  # For large collections
                )
            
            logger.debug(f"Vectors config: {vectors_config}")
            logger.debug(f"Sparse vectors config: {sparse_vectors_config}")
            
            # –ü–æ–ø—Ä–æ–±—É–µ–º –ø–µ—Ä–µ–¥–∞—Ç—å –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –≤ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–º —Ñ–æ—Ä–º–∞—Ç–µ
            try:
                if sparse_vectors_config:
                    # –°–æ–∑–¥–∞–µ–º –∫–æ–ª–ª–µ–∫—Ü–∏—é —Å dense –∏ sparse –≤–µ–∫—Ç–æ—Ä–∞–º–∏
                    self.client.create_collection(
                        collection_name=collection_name,
                        vectors_config=vectors_config,
                        sparse_vectors_config=sparse_vectors_config
                    )
                else:
                    # –°–æ–∑–¥–∞–µ–º –∫–æ–ª–ª–µ–∫—Ü–∏—é —Ç–æ–ª—å–∫–æ —Å dense –≤–µ–∫—Ç–æ—Ä–∞–º–∏ (–µ—Å–ª–∏ –æ–Ω–∏ –Ω—É–∂–Ω—ã) –∏–ª–∏ –±–µ–∑ –≤–µ–∫—Ç–æ—Ä–æ–≤
                    if vectors_config:
                        self.client.create_collection(
                            collection_name=collection_name,
                            vectors_config=vectors_config
                        )
                    else:
                        # –ï—Å–ª–∏ –Ω–∏ –æ–¥–∏–Ω —Ç–∏–ø –≤–µ–∫—Ç–æ—Ä–æ–≤ –Ω–µ –Ω—É–∂–µ–Ω, —Å–æ–∑–¥–∞–µ–º –∫–æ–ª–ª–µ–∫—Ü–∏—é –±–µ–∑ –≤–µ–∫—Ç–æ—Ä–æ–≤
                        # –≠—Ç–æ –º–∞–ª–æ–≤–µ—Ä–æ—è—Ç–Ω—ã–π —Å–ª—É—á–∞–π, –Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–µ–º –µ–≥–æ –¥–ª—è –ø–æ–ª–Ω–æ—Ç—ã
                        self.client.create_collection(
                            collection_name=collection_name
                        )
                        
            except Exception as e:
                logger.error(f"Failed to create collection {collection_name}: {e}")
                raise
    
    def _create_multilevel_chunker(self) -> MultiLevelChunker:
        """–°–æ–∑–¥–∞–Ω–∏–µ –º–Ω–æ–≥–æ—É—Ä–æ–≤–Ω–µ–≤–æ–≥–æ —á–∞–Ω–∫–µ—Ä–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏."""
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Å—Ç—Ä–∞—Ç–µ–≥–∏—é –º–∏–∫—Ä–æ-—á–∞–Ω–∫–∏–Ω–≥–∞
        micro_strategy = getattr(self.config, 'multilevel_micro_strategy', 'character')
        
        if micro_strategy == 'paragraph':
            return MultiLevelChunker(
                macro_chunk_strategy=getattr(self.config, 'multilevel_macro_strategy', 'character'),
                macro_chunk_size=getattr(self.config, 'multilevel_macro_chunk_size', 10000),
                macro_chunk_overlap=getattr(self.config, 'multilevel_macro_chunk_overlap', 1000),
                macro_paragraphs_per_chunk=getattr(self.config, 'multilevel_macro_paragraphs_per_chunk', 5),
                macro_paragraph_overlap=getattr(self.config, 'multilevel_macro_paragraph_overlap', 1),
                micro_chunk_strategy='paragraph',
                micro_paragraphs_per_chunk=getattr(self.config, 'multilevel_micro_paragraphs_per_chunk', 3),
                micro_paragraph_overlap=getattr(self.config, 'multilevel_micro_paragraph_overlap', 1)
            )
        elif micro_strategy == 'sentence':
            return MultiLevelChunker(
                macro_chunk_strategy=getattr(self.config, 'multilevel_macro_strategy', 'character'),
                macro_chunk_size=getattr(self.config, 'multilevel_macro_chunk_size', 10000),
                macro_chunk_overlap=getattr(self.config, 'multilevel_macro_chunk_overlap', 1000),
                macro_sentences_per_chunk=getattr(self.config, 'multilevel_macro_sentences_per_chunk', 10),
                macro_sentence_overlap=getattr(self.config, 'multilevel_macro_sentence_overlap', 1),
                micro_chunk_strategy='sentence',
                micro_sentences_per_chunk=getattr(self.config, 'multilevel_micro_sentences_per_chunk', 5),
                micro_sentence_overlap=getattr(self.config, 'multilevel_micro_sentence_overlap', 1)
            )
        else:  # character
            return MultiLevelChunker(
                macro_chunk_strategy=getattr(self.config, 'multilevel_macro_strategy', 'character'),
                macro_chunk_size=getattr(self.config, 'multilevel_macro_chunk_size', 10000),
                macro_chunk_overlap=getattr(self.config, 'multilevel_macro_chunk_overlap', 1000),
                micro_chunk_strategy='character',
                micro_chunk_size=getattr(self.config, 'multilevel_micro_chunk_size', 1000),
                micro_chunk_overlap=getattr(self.config, 'multilevel_micro_chunk_overlap', 100)
            )
    
    def index_documents_multilevel(self, documents: List[Document]) -> Dict[str, Any]:
        """
        –ú–Ω–æ–≥–æ—É—Ä–æ–≤–Ω–µ–≤–∞—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤.
        
        Args:
            documents (List[Document]): –°–ø–∏—Å–æ–∫ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏
            
        Returns:
            Dict[str, Any]: –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏
        """
        logger.info(f"–ù–∞—á–∞–ª–æ –º–Ω–æ–≥–æ—É—Ä–æ–≤–Ω–µ–≤–æ–π –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏ {len(documents)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
        
        # –£–±–µ–¥–∏—Ç—å—Å—è, —á—Ç–æ –∫–æ–ª–ª–µ–∫—Ü–∏—è —Å—É—â–µ—Å—Ç–≤—É–µ—Ç —Å –ø—Ä–∞–≤–∏–ª—å–Ω–æ–π –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–µ–π
        self._ensure_collection_exists()
        
        # –°–æ–∑–¥–∞–µ–º –º–Ω–æ–≥–æ—É—Ä–æ–≤–Ω–µ–≤—ã–µ —á–∞–Ω–∫–∏
        multilevel_chunks = self.multilevel_chunker.create_multilevel_chunks(documents)
        
        logger.info(f"–ü–æ—Å–ª–µ —Å–æ–∑–¥–∞–Ω–∏—è –º–Ω–æ–≥–æ—É—Ä–æ–≤–Ω–µ–≤—ã—Ö —á–∞–Ω–∫–æ–≤: {len(multilevel_chunks)} —á–∞–Ω–∫–æ–≤")
        
        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏
        points_to_upload = []
        total_vectors = 0
        
        logger.info(f"–ù–∞—á–∞–ª–æ –æ–±—Ä–∞–±–æ—Ç–∫–∏ {len(multilevel_chunks)} —á–∞–Ω–∫–æ–≤")
        
        for i, chunk_data in enumerate(multilevel_chunks):
            logger.info(f"–û–±—Ä–∞–±–æ—Ç–∫–∞ —á–∞–Ω–∫–∞ {i+1}/{len(multilevel_chunks)}")
            macro_chunk = chunk_data["macro_chunk"]
            micro_chunks = chunk_data["micro_chunks"]
            chunk_index = chunk_data["chunk_index"]
            
            logger.info(f"–ú–∞–∫—Ä–æ-—á–∞–Ω–∫ {i+1}: {len(micro_chunks)} –º–∏–∫—Ä–æ-—á–∞–Ω–∫–æ–≤")
            
            # –°–æ–∑–¥–∞–µ–º —Ç–æ—á–∫—É –¥–ª—è Qdrant –±–µ–∑ —è–≤–Ω–æ–≥–æ —É–∫–∞–∑–∞–Ω–∏—è ID (Qdrant —Å–≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç UUID –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏)
            
            # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ dense –≤–µ–∫—Ç–æ—Ä—ã –¥–ª—è –º–∞–∫—Ä–æ-—á–∞–Ω–∫–∞ (–º–∞–∫—Ä–æ + –º–∏–∫—Ä–æ)
            all_texts = [macro_chunk.page_content] + micro_chunks
            dense_vectors = self.embedding_manager.embed_texts(all_texts)
            
            # –ü–æ–ª—É—á–∞–µ–º sparse –≤–µ–∫—Ç–æ—Ä –¥–ª—è –º–∞–∫—Ä–æ-—á–∞–Ω–∫–∞
            sparse_vector = None
            if self.sparse_emb:
                try:
                    sparse_vector = self.sparse_emb.embed_query(macro_chunk.page_content)
                    # sparse_vector —É–∂–µ –≤ —Ñ–æ—Ä–º–∞—Ç–µ {"indices": [...], "values": [...]} –æ—Ç –∞–¥–∞–ø—Ç–µ—Ä–∞
                except Exception as e:
                    logger.warning(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ sparse –≤–µ–∫—Ç–æ—Ä–∞: {e}")
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –≤–µ–∫—Ç–æ—Ä –¥–ª—è —Ç–æ—á–∫–∏ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ä–µ–∂–∏–º–∞
            point_vector = {}  # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –ø—É—Å—Ç–æ–π —Å–ª–æ–≤–∞—Ä—å –¥–ª—è –≤–µ–∫—Ç–æ—Ä–æ–≤
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω—É–∂–Ω–æ –ª–∏ –∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞—Ç—å dense –≤–µ–∫—Ç–æ—Ä—ã
            index_dense = getattr(self.config, 'index_dense', True)
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω—É–∂–Ω–æ –ª–∏ –∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞—Ç—å sparse –≤–µ–∫—Ç–æ—Ä—ã
            index_sparse = (
                self.sparse_emb and 
                (getattr(self.config, 'index_bm25', False) or getattr(self.config, 'index_hybrid', False))
            )
            
            if index_dense:
                # –î–æ–±–∞–≤–ª—è–µ–º dense –≤–µ–∫—Ç–æ—Ä—ã (–º—É–ª—å—Ç–∏–≤–µ–∫—Ç–æ—Ä—ã)
                point_vector["dense_vector"] = dense_vectors
                
            # –î–æ–±–∞–≤–ª—è–µ–º sparse –≤–µ–∫—Ç–æ—Ä—ã –≤ –æ—Å–Ω–æ–≤–Ω–æ–π –≤–µ–∫—Ç–æ—Ä, –µ—Å–ª–∏ –æ–Ω–∏ –µ—Å—Ç—å
            if index_sparse and sparse_vector:
                point_vector[getattr(self.config, 'sparse_vector_name', 'sparse_vector')] = sparse_vector
            
            # –°–æ–∑–¥–∞–µ–º —Ç–æ—á–∫—É –¥–ª—è Qdrant —Å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–º UUID
            point = PointStruct(
                id=str(uuid.uuid4()),
                vector=point_vector,
                payload={
                    "content": macro_chunk.page_content,
                    "metadata": macro_chunk.metadata,
                    "chunk_index": chunk_index,
                    "total_micro_chunks": len(micro_chunks),
                    "micro_contents": micro_chunks,  # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ –º–∏–∫—Ä–æ-—á–∞–Ω–∫–æ–≤ –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
                    "source": macro_chunk.metadata.get("source", "unknown")  # –î–æ–±–∞–≤–ª—è–µ–º source –≤ payload –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
                }
            )
        
            # –î–æ–±–∞–≤–ª—è–µ–º —Ç–æ—á–∫—É –≤ —Å–ø–∏—Å–æ–∫ –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏
            points_to_upload.append(point)
            total_vectors += len(dense_vectors) if index_dense else 0
            
            logger.info(f"–î–æ–±–∞–≤–ª–µ–Ω–∞ —Ç–æ—á–∫–∞ {i+1}, –≤—Å–µ–≥–æ —Ç–æ—á–µ–∫: {len(points_to_upload)}")
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–æ—á–∫–∏ –≤ Qdrant
        collection_name = getattr(self.config, 'collection_name', DEFAULT_COLLECTION_NAME)
        
        # –ï—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω–∞ –ø–µ—Ä–µ–∑–∞–ø–∏—Å—å, —É–¥–∞–ª—è–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â—É—é –∫–æ–ª–ª–µ–∫—Ü–∏—é
        if getattr(self.config, 'force_recreate', False):
            try:
                self.client.delete_collection(collection_name)
                self._ensure_collection_exists()
            except Exception as e:
                logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å —É–¥–∞–ª–∏—Ç—å –∫–æ–ª–ª–µ–∫—Ü–∏—é {collection_name}: {e}")
        
        self.client.upload_points(
            collection_name=collection_name,
            points=points_to_upload,
            wait=True
        )
        
        logger.info(f"–£—Å–ø–µ—à–Ω–æ –ø—Ä–æ–∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω–æ {len(points_to_upload)} –º–∞–∫—Ä–æ-—á–∞–Ω–∫–æ–≤ —Å {total_vectors} dense –≤–µ–∫—Ç–æ—Ä–∞–º–∏")
        
        return {
            "macro_chunks": len(points_to_upload),
            "total_vectors": total_vectors,
            "micro_vectors_per_chunk": total_vectors - len(points_to_upload) if len(points_to_upload) > 0 and index_dense else 0
        }
    
    def search_multilevel(self, query: str, k: int = 5) -> List[Dict[str, Any]]:
        """
        –ú–Ω–æ–≥–æ—É—Ä–æ–≤–Ω–µ–≤—ã–π –ø–æ–∏—Å–∫.
        
        Args:
            query (str): –ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
            k (int): –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            
        Returns:
            List[Dict[str, Any]]: –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞
        """
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ä–µ–∂–∏–º –ø–æ–∏—Å–∫–∞
        index_dense = getattr(self.config, 'index_dense', True)
        index_bm25 = getattr(self.config, 'index_bm25', False)
        index_hybrid = getattr(self.config, 'index_hybrid', False)
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω—É–∂–Ω–æ –ª–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å sparse –≤–µ–∫—Ç–æ—Ä—ã
        use_sparse = self.sparse_emb and (index_bm25 or index_hybrid)
        
        logger.info(f"Search mode: index_dense={index_dense}, index_bm25={index_bm25}, index_hybrid={index_hybrid}")
        logger.info(f"Use sparse: {use_sparse}")
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –≤–µ–∫—Ç–æ—Ä –¥–ª—è –ø–æ–∏—Å–∫–∞ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ä–µ–∂–∏–º–∞
        search_vector = None
        
        if index_hybrid and index_dense and use_sparse:
            # –ì–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–∏—Å–∫ - –∏—Å–ø–æ–ª—å–∑—É–µ–º –æ–±–∞ –≤–µ–∫—Ç–æ—Ä–∞
            # –í–µ–∫—Ç–æ—Ä–∏–∑—É–µ–º –∑–∞–ø—Ä–æ—Å –¥–ª—è dense –ø–æ–∏—Å–∫–∞
            dense_query_vector = self.embedding_manager.embed_query(query)
            
            # –í–µ–∫—Ç–æ—Ä–∏–∑—É–µ–º –∑–∞–ø—Ä–æ—Å –¥–ª—è sparse –ø–æ–∏—Å–∫–∞
            sparse_vector = self.sparse_emb.embed_query(query)
            # sparse_vector —É–∂–µ –≤ —Ñ–æ—Ä–º–∞—Ç–µ {"indices": [...], "values": [...]} –æ—Ç –∞–¥–∞–ø—Ç–µ—Ä–∞
            
            # –î–ª—è –≥–∏–±—Ä–∏–¥–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞ –ø–µ—Ä–µ–¥–∞–µ–º –æ–±–∞ –≤–µ–∫—Ç–æ—Ä–∞
            search_vector = {
                "dense_vector": dense_query_vector,
                getattr(self.config, 'sparse_vector_name', 'sparse_vector'): sparse_vector
            }
            logger.info("Using hybrid search mode")
        elif index_dense:
            # –¢–æ–ª—å–∫–æ dense –ø–æ–∏—Å–∫
            query_vector = self.embedding_manager.embed_query(query)
            search_vector = {"dense_vector": query_vector}
            logger.info("Using dense search mode")
        elif use_sparse:
            # –¢–æ–ª—å–∫–æ sparse –ø–æ–∏—Å–∫
            sparse_vector = self.sparse_emb.embed_query(query)
            # sparse_vector —É–∂–µ –≤ —Ñ–æ—Ä–º–∞—Ç–µ {"indices": [...], "values": [...]} –æ—Ç –∞–¥–∞–ø—Ç–µ—Ä–∞
            search_vector = {getattr(self.config, 'sparse_vector_name', 'sparse_vector'): sparse_vector}
            logger.info("Using sparse search mode")
        else:
            logger.warning("No valid search mode determined")
            return []
        
        logger.info(f"Search vector: {search_vector}")
        
        # –í—ã–ø–æ–ª–Ω—è–µ–º –ø–æ–∏—Å–∫ –≤ Qdrant —Å –∏–º–µ–Ω–æ–≤–∞–Ω–Ω—ã–º –≤–µ–∫—Ç–æ—Ä–æ–º
        collection_name = getattr(self.config, 'collection_name', DEFAULT_COLLECTION_NAME)
        
        # –î–ª—è –≥–∏–±—Ä–∏–¥–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞ –∏—Å–ø–æ–ª—å–∑—É–µ–º –º–µ—Ç–æ–¥ query_points –∏–ª–∏ —Ä–∞–∑–¥–µ–ª—å–Ω—ã–π –ø–æ–¥—Ö–æ–¥
        if index_hybrid and index_dense and use_sparse:
            # –ì–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–∏—Å–∫ - –≤—ã–ø–æ–ª–Ω–∏–º –æ—Ç–¥–µ–ª—å–Ω—ã–µ –ø–æ–∏—Å–∫–∏ –∏ –æ–±—ä–µ–¥–∏–Ω–∏–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
            logger.info("–í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –≥–∏–±—Ä–∏–¥–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞")
            
            # Dense –ø–æ–∏—Å–∫
            dense_results = self.client.search(
                collection_name=collection_name,
                query_vector=("dense_vector", dense_query_vector),
                limit=k * 2,  # –ë–µ—Ä–µ–º –±–æ–ª—å—à–µ –¥–ª—è reranking
                query_filter=search_filter,
                with_payload=True,
                with_vectors=False
            )
            
            # Sparse –ø–æ–∏—Å–∫
            sparse_results = self.client.search(
                collection_name=collection_name,
                query_vector=None,  # –î–ª—è sparse –ø–æ–∏—Å–∫–∞ query_vector=None
                sparse_vector={getattr(self.config, 'sparse_vector_name', 'sparse_vector'): sparse_vector},  # {indices: [...], values: [...]}
                vector_name=getattr(self.config, 'sparse_vector_name', 'sparse_vector'),  # –ò–º—è sparse named vector
                limit=k * 2,
                query_filter=search_filter,
                with_payload=True,
                with_vectors=False
            )
            
            # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã (–ø—Ä–æ—Å—Ç–æ–µ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ —Å reranking)
            # –°–æ–∑–¥–∞–µ–º —Å–ª–æ–≤–∞—Ä—å –¥–ª—è –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ ID
            combined_dict = {}
            
            # –î–æ–±–∞–≤–ª—è–µ–º dense —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã (—Å –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç–æ–º alpha)
            config = ConfigManager.get_instance().get()
            alpha = getattr(config, "hybrid_alpha", 0.7)  # –í–µ—Å –¥–ª—è dense —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ (–ø–æ–≤—ã—à–µ–Ω–Ω—ã–π –≤–µ—Å –¥–ª—è —Å–µ–º–∞–Ω—Ç–∏–∫–∏)
            for result in dense_results:
                result_id = result.id
                combined_dict[result_id] = {
                    'point': result,
                    'score': alpha * result.score
                }
            
            # –î–æ–±–∞–≤–ª—è–µ–º sparse —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã (—Å –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç–æ–º 1-alpha)
            beta = 1 - alpha
            for result in sparse_results:
                result_id = result.id
                if result_id in combined_dict:
                    # –ï—Å–ª–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç —É–∂–µ –µ—Å—Ç—å, –¥–æ–±–∞–≤–ª—è–µ–º –∫ —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–º—É —Å–∫–æ—Ä—É
                    combined_dict[result_id]['score'] += beta * result.score
                else:
                    # –ï—Å–ª–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç –Ω–æ–≤—ã–π, –¥–æ–±–∞–≤–ª—è–µ–º –µ–≥–æ
                    combined_dict[result_id] = {
                        'point': result,
                        'score': beta * result.score
                    }
            
            # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–Ω–æ–º—É —Å–∫–æ—Ä—É –∏ –±–µ—Ä–µ–º top-k
            sorted_results = sorted(combined_dict.items(), key=lambda x: x[1]['score'], reverse=True)[:k]
            
            # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Ç–æ–ª—å–∫–æ –æ–±—ä–µ–∫—Ç—ã PointStruct
            search_results = [item[1]['point'] for item in sorted_results]
        elif index_dense:
            # –¢–æ–ª—å–∫–æ dense –ø–æ–∏—Å–∫
            search_results = self.client.search(
                collection_name=collection_name,
                query_vector=("dense_vector", search_vector["dense_vector"]),
                limit=k,
                query_filter=search_filter,
                with_payload=True,
                with_vectors=False
            )
        elif use_sparse:
            # –¢–æ–ª—å–∫–æ sparse –ø–æ–∏—Å–∫
            search_results = self.client.search(
                collection_name=collection_name,
                query_vector=None,
                sparse_vector=search_vector,
                vector_name=getattr(self.config, 'sparse_vector_name', 'sparse_vector'),
                limit=k,
                query_filter=search_filter,
                with_payload=True,
                with_vectors=False
            )
        else:
            logger.warning("No valid search mode determined")
            return []
        
        logger.info(f"Search returned {len(search_results)} results")
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        results = []
        for result in search_results:
            processed_result = {
                "content": result.payload.get("content", ""),
                "metadata": result.payload.get("metadata", {}),
                "score": result.score,
                "chunk_index": result.payload.get("chunk_index", 0),
                "total_micro_chunks": result.payload.get("total_micro_chunks", 0),
                "micro_contents": result.payload.get("micro_contents", []),
                "source": result.payload.get("source", "unknown")
            }
            results.append(processed_result)
        
        logger.info(f"Processed {len(results)} results for return")
        return results


def create_multilevel_indexer(config: Config) -> MultiLevelIndexer:
    """–°–æ–∑–¥–∞–Ω–∏–µ –º–Ω–æ–≥–æ—É—Ä–æ–≤–Ω–µ–≤–æ–≥–æ –∏–Ω–¥–µ–∫—Å–∞—Ç–æ—Ä–∞."""
    return MultiLevelIndexer(config)
====================================================================================================
==================== –ö–û–ù–ï–¶ –§–ê–ô–õ–ê: core\indexing\multilevel_indexer.py ====================



==================== –ù–ê–ß–ê–õ–û –§–ê–ô–õ–ê: core\indexing\paragraph_chunker.py ====================
# –§–∞–π–ª: core\indexing\paragraph_chunker.py
====================================================================================================
"""Module for chunking text by paragraphs."""

import logging
from typing import List

from langchain_text_splitters.base import TextSplitter

logger = logging.getLogger(__name__)


class ParagraphTextSplitter(TextSplitter):
    """Text splitter for chunking by paragraphs."""
    
    def __init__(
        self,
        paragraphs_per_chunk: int = 3,
        paragraph_overlap: int = 1,
        keep_separator: bool = True,
        **kwargs
    ):
        """
        Initialize the paragraph text splitter.
        
        Args:
            paragraphs_per_chunk (int): Number of paragraphs per chunk
            paragraph_overlap (int): Number of paragraphs overlap between chunks
            keep_separator (bool): Whether to keep separators (newlines) in chunks
            **kwargs: Additional arguments for TextSplitter
        """
        super().__init__(**kwargs)
        self.paragraphs_per_chunk = paragraphs_per_chunk
        self.paragraph_overlap = paragraph_overlap
        self.keep_separator = keep_separator
        
    def split_text(self, text: str) -> List[str]:
        """
        Split text into chunks by paragraphs.
        
        Args:
            text (str): Input text to split
            
        Returns:
            List[str]: List of text chunks
        """
        # Split text into paragraphs by double newlines
        paragraphs = [p.strip() for p in text.split('\n\n') if p.strip()]
        
        if not paragraphs:
            return []
            
        chunks = []
        
        # Calculate window step
        step = self.paragraphs_per_chunk - self.paragraph_overlap
        # If step <= 0, set it to 1 to avoid infinite loop
        if step <= 0:
            step = 1
            
        i = 0
        while i < len(paragraphs):
            # Determine start and end of current chunk
            start_idx = i
            end_idx = min(i + self.paragraphs_per_chunk, len(paragraphs))
            
            # Form chunk from paragraphs
            if self.keep_separator:
                chunk = '\n\n'.join(paragraphs[start_idx:end_idx])
            else:
                chunk = ' '.join(paragraphs[start_idx:end_idx])
                
            chunks.append(chunk)
            
            # If we've reached the end of text, exit loop
            if end_idx >= len(paragraphs):
                break
                
            # Move to next chunk with overlap
            i += step
            
        return chunks


# Test function
def test_paragraph_chunking():
    """Test paragraph chunking functionality."""
    text = """First paragraph of text.

Second paragraph of text.

Third paragraph of text.

Fourth paragraph of text.

Fifth paragraph of text."""
    
    print("Source text:")
    print(repr(text))
    
    # Test with default settings (3 paragraphs per chunk, 1 paragraph overlap)
    splitter = ParagraphTextSplitter()
    print(f"\nParameters: paragraphs_per_chunk={splitter.paragraphs_per_chunk}, paragraph_overlap={splitter.paragraph_overlap}")
    
    chunks = splitter.split_text(text)
    print(f"\nChunks ({len(chunks)} pcs.):")
    for i, chunk in enumerate(chunks):
        print(f"\nChunk {i+1}:")
        print(repr(chunk))


if __name__ == "__main__":
    test_paragraph_chunking()
====================================================================================================
==================== –ö–û–ù–ï–¶ –§–ê–ô–õ–ê: core\indexing\paragraph_chunker.py ====================



==================== –ù–ê–ß–ê–õ–û –§–ê–ô–õ–ê: core\indexing\sentence_chunker.py ====================
# –§–∞–π–ª: core\indexing\sentence_chunker.py
====================================================================================================
"""Module for chunking text by sentences."""

import logging
import re
from typing import List

from langchain_text_splitters.base import TextSplitter

logger = logging.getLogger(__name__)


class SentenceTextSplitter(TextSplitter):
    """Text splitter for chunking by sentences."""
    
    def __init__(
        self,
        sentences_per_chunk: int = 5,
        sentence_overlap: int = 1,
        keep_separator: bool = True,
        **kwargs
    ):
        """
        Initialize the sentence text splitter.
        
        Args:
            sentences_per_chunk (int): Number of sentences per chunk
            sentence_overlap (int): Number of sentences overlap between chunks
            keep_separator (bool): Whether to keep separators (newlines) in chunks
            **kwargs: Additional arguments for TextSplitter
        """
        super().__init__(**kwargs)
        self.sentences_per_chunk = sentences_per_chunk
        self.sentence_overlap = sentence_overlap
        self.keep_separator = keep_separator
        
    def split_text(self, text: str) -> List[str]:
        """
        Split text into chunks by sentences.
        
        Args:
            text (str): Input text to split
            
        Returns:
            List[str]: List of text chunks
        """
        # More accurate sentence splitting with punctuation preservation
        # Use regex to find sentence endings with punctuation capture
        sentence_endings = re.finditer(r'[.!?]+', text)
        sentences = []
        last_end = 0
        
        for match in sentence_endings:
            # Add sentence with punctuation preserved
            sentence = text[last_end:match.end()].strip()
            if sentence:
                sentences.append(sentence)
            last_end = match.end()
        
        # Add remaining text if any
        if last_end < len(text):
            remaining = text[last_end:].strip()
            if remaining:
                sentences.append(remaining)
        
        # If no sentences found by punctuation, split by periods
        if not sentences:
            sentences = [s.strip() + '.' for s in text.split('.') if s.strip()]
            # Remove trailing period if it was added extra
            if sentences and sentences[-1].endswith('.'):
                sentences[-1] = sentences[-1][:-1]
        
        if not sentences:
            return []
            
        chunks = []
        
        # Calculate window step
        step = self.sentences_per_chunk - self.sentence_overlap
        # If step <= 0, set it to 1 to avoid infinite loop
        if step <= 0:
            step = 1
            
        i = 0
        while i < len(sentences):
            # Determine start and end of current chunk
            start_idx = i
            end_idx = min(i + self.sentences_per_chunk, len(sentences))
            
            # Form chunk from sentences
            if self.keep_separator:
                chunk = " ".join(sentences[start_idx:end_idx])
            else:
                # Remove punctuation for chunk without separators
                cleaned_sentences = [re.sub(r'[.!?]+$', '', s).strip() for s in sentences[start_idx:end_idx]]
                chunk = " ".join(cleaned_sentences)
                
            chunks.append(chunk)
            
            # If we've reached the end of text, exit loop
            if end_idx >= len(sentences):
                break
                
            # Move to next chunk with overlap
            i += step
            
        return chunks
====================================================================================================
==================== –ö–û–ù–ï–¶ –§–ê–ô–õ–ê: core\indexing\sentence_chunker.py ====================



==================== –ù–ê–ß–ê–õ–û –§–ê–ô–õ–ê: core\indexing\text_splitter.py ====================
# –§–∞–π–ª: core\indexing\text_splitter.py
====================================================================================================
"""–ú–æ–¥—É–ª—å –¥–ª—è —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞ –Ω–∞ —á–∞–Ω–∫–∏."""

import logging
from typing import List

from langchain_core.documents import Document

from config.settings import Config
from core.indexing.chunker import create_text_splitter

logger = logging.getLogger(__name__)


class TextSplitter:
    """–ö–ª–∞—Å—Å –¥–ª—è —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞ –Ω–∞ —á–∞–Ω–∫–∏."""

    def __init__(self, config: Config):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—è —Ç–µ–∫—Å—Ç–∞.
        
        Args:
            config (Config): –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è.
        """
        self.text_splitter = create_text_splitter(config)

    def split_documents(self, documents: List[Document]) -> List[Document]:
        """
        –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –Ω–∞ —á–∞–Ω–∫–∏.
        
        Args:
            documents (List[Document]): –°–ø–∏—Å–æ–∫ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è.
            
        Returns:
            List[Document]: –°–ø–∏—Å–æ–∫ —á–∞–Ω–∫–æ–≤.
        """
        try:
            return self.text_splitter.split_documents(documents)
        except Exception as e:
            logger.exception(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –Ω–∞ —á–∞–Ω–∫–∏: {e}")
            raise


class IndexingError(Exception):
    """–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–æ–µ –∏—Å–∫–ª—é—á–µ–Ω–∏–µ –¥–ª—è –æ—à–∏–±–æ–∫ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏."""
    pass
====================================================================================================
==================== –ö–û–ù–ï–¶ –§–ê–ô–õ–ê: core\indexing\text_splitter.py ====================



==================== –ù–ê–ß–ê–õ–û –§–ê–ô–õ–ê: core\indexing\__init__.py ====================
# –§–∞–π–ª: core\indexing\__init__.py
====================================================================================================
 

====================================================================================================
==================== –ö–û–ù–ï–¶ –§–ê–ô–õ–ê: core\indexing\__init__.py ====================



==================== –ù–ê–ß–ê–õ–û –§–ê–ô–õ–ê: core\qdrant\qdrant_client.py ====================
# –§–∞–π–ª: core\qdrant\qdrant_client.py
====================================================================================================
"""Centralized Qdrant client factory (sync + async helpers).

Provides a cached sync client and an async helper that creates the client in a thread
to avoid blocking the event loop.
"""
import asyncio
from functools import lru_cache
from typing import Optional

from qdrant_client import QdrantClient

from config.config_manager import ConfigManager
from config.settings import Config

# Get singleton instance of ConfigManager
config_manager = ConfigManager.get_instance()


@lru_cache
def _create_client(
    url: str, 
    timeout: Optional[int] = None,
    prefer_grpc: bool = False
) -> QdrantClient:
    """Create a QdrantClient with specified parameters.
    
    Args:
        url: Qdrant server URL
        timeout: Connection timeout in seconds
        prefer_grpc: Whether to prefer gRPC over REST API
        
    Returns:
        QdrantClient: Configured Qdrant client
    """
    # Get default config for fallback values
    config_manager.get()
    
    # Use provided timeout or default from config
    client_timeout = timeout if timeout is not None else 10
    
    return QdrantClient(
        url=url,
        timeout=client_timeout,
        prefer_grpc=prefer_grpc
    )


def get_qdrant_client(
    config: Optional[Config] = None,
    timeout: Optional[int] = None,
    prefer_grpc: bool = False
) -> QdrantClient:
    """Return a cached synchronous QdrantClient for the provided config or loaded config.
    
    Args:
        config: Configuration object (optional)
        timeout: Connection timeout in seconds (optional)
        prefer_grpc: Whether to prefer gRPC over REST API (optional)
        
    Returns:
        QdrantClient: Configured Qdrant client
    """
    if config is None:
        config = config_manager.get()
    
    # Use provided timeout or get from config
    client_timeout = timeout if timeout is not None else config.qdrant_retry_wait_time
    
    return _create_client(
        url=config.qdrant_url,
        timeout=client_timeout,
        prefer_grpc=prefer_grpc
    )


async def aget_qdrant_client(
    config: Optional[Config] = None,
    timeout: Optional[int] = None,
    prefer_grpc: bool = False
) -> QdrantClient:
    """Async helper returning QdrantClient created in a thread to avoid blocking.

    Use when creating the client from async code that shouldn't block the event loop.
    
    Args:
        config: Configuration object (optional)
        timeout: Connection timeout in seconds (optional)
        prefer_grpc: Whether to prefer gRPC over REST API (optional)
        
    Returns:
        QdrantClient: Configured Qdrant client
    """
    if config is None:
        config = config_manager.get()
    
    # Use provided timeout or get from config
    client_timeout = timeout if timeout is not None else config.qdrant_retry_wait_time
    
    loop = asyncio.get_running_loop()
    return await loop.run_in_executor(
        None, 
        lambda: _create_client(
            url=config.qdrant_url,
            timeout=client_timeout,
            prefer_grpc=prefer_grpc
        )
    )
====================================================================================================
==================== –ö–û–ù–ï–¶ –§–ê–ô–õ–ê: core\qdrant\qdrant_client.py ====================



==================== –ù–ê–ß–ê–õ–û –§–ê–ô–õ–ê: core\qdrant\qdrant_collections.py ====================
# –§–∞–π–ª: core\qdrant\qdrant_collections.py
====================================================================================================
"""–ú–æ–¥—É–ª—å –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –∫–æ–ª–ª–µ–∫—Ü–∏—è–º–∏ Qdrant."""

import logging
from typing import List

from config.config_manager import ConfigManager
from core.qdrant.qdrant_client import get_qdrant_client
from core.utils.collection_manager import CollectionManager

logger = logging.getLogger(__name__)


def get_cached_collections(client=None) -> List[str]:
    """
    –ü–æ–ª—É—á–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –∫–æ–ª–ª–µ–∫—Ü–∏–π —Å –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º.
    
    Args:
        client (QdrantClient, optional): –ö–ª–∏–µ–Ω—Ç Qdrant. –ï—Å–ª–∏ –Ω–µ —É–∫–∞–∑–∞–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –∑–∞–∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—ã–π.
        
    Returns:
        List[str]: –°–ø–∏—Å–æ–∫ –Ω–∞–∑–≤–∞–Ω–∏–π –∫–æ–ª–ª–µ–∫—Ü–∏–π.
    """
    try:
        collection_manager = CollectionManager.get_instance()
        collections_dict = collection_manager.get_collections(client)
        return list(collections_dict.keys())
    except Exception as e:
        logger.exception(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ —Å–ø–∏—Å–∫–∞ –∫–æ–ª–ª–µ–∫—Ü–∏–π: {e}")
        return []


async def refresh_collections_cache(client=None):
    """
    –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ –æ–±–Ω–æ–≤–ª—è–µ—Ç –∫—ç—à –∫–æ–ª–ª–µ–∫—Ü–∏–π.
    
    Args:
        client (QdrantClient, optional): –ö–ª–∏–µ–Ω—Ç Qdrant. –ï—Å–ª–∏ –Ω–µ —É–∫–∞–∑–∞–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –∑–∞–∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—ã–π.
    """
    try:
        collection_manager = CollectionManager.get_instance()
        collections_dict = collection_manager.refresh_collections(client)
        logger.debug(f"–ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ –æ–±–Ω–æ–≤–ª–µ–Ω –∫—ç—à –∫–æ–ª–ª–µ–∫—Ü–∏–π: {list(collections_dict.keys())}")
    except Exception as e:
        logger.exception(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ–º –æ–±–Ω–æ–≤–ª–µ–Ω–∏–∏ –∫—ç—à–∞ –∫–æ–ª–ª–µ–∫—Ü–∏–π: {e}")


def recreate_collection_from_config(force_recreate: bool = True):
    """
    –ü–µ—Ä–µ—Å–æ–∑–¥–∞–µ—Ç –∫–æ–ª–ª–µ–∫—Ü–∏—é, –∏—Å–ø–æ–ª—å–∑—É—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –∏–∑ config.json.
    
    Args:
        force_recreate (bool): –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ –ø–µ—Ä–µ—Å–æ–∑–¥–∞—Ç—å –∫–æ–ª–ª–µ–∫—Ü–∏—é.
        
    Returns:
        Tuple[bool, str]: (—É—Å–ø–µ—Ö, —Å—Ç–∞—Ç—É—Å)
    """
    try:
        config_manager = ConfigManager.get_instance()
        config = config_manager.get()
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è —Ö–æ—Ç—è –±—ã –æ–¥–Ω–æ–≥–æ —Ç–∏–ø–∞ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏
        if not (config.index_dense or config.index_bm25 or config.index_hybrid):
            return False, "no_index_type"
            
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–ª—è –≥–∏–±—Ä–∏–¥–Ω–æ–≥–æ —Ä–µ–∂–∏–º–∞
        if config.index_hybrid and not config.index_dense:
            return False, "hybrid_requires_dense"
            
        client = get_qdrant_client(config)
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º CollectionManager –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏—è –∫–æ–ª–ª–µ–∫—Ü–∏–∏
        collection_manager = CollectionManager.get_instance()
        collections_dict = collection_manager.get_collections(client)
        collection_exists = config.collection_name in collections_dict
        
        if collection_exists and force_recreate:
            # –£–¥–∞–ª–µ–Ω–∏–µ —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–π –∫–æ–ª–ª–µ–∫—Ü–∏–∏ —á–µ—Ä–µ–∑ CollectionManager
            client.delete_collection(config.collection_name)
            # –û—á–∏—â–∞–µ–º –∫—ç—à CollectionManager
            collection_manager.refresh_collections(client)
            logger.info(f"–£–¥–∞–ª–µ–Ω–∞ —Å—É—â–µ—Å—Ç–≤—É—é—â–∞—è –∫–æ–ª–ª–µ–∫—Ü–∏—è: {config.collection_name}")
            
        if not collection_exists or force_recreate:
            # –°–æ–∑–¥–∞–Ω–∏–µ –Ω–æ–≤–æ–π –∫–æ–ª–ª–µ–∫—Ü–∏–∏
            # –°–æ–∑–¥–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω—É—é –∫–æ–ª–ª–µ–∫—Ü–∏—é –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
            if config.index_dense:
                # –î–ª—è –ø–ª–æ—Ç–Ω—ã—Ö –≤–µ–∫—Ç–æ—Ä–æ–≤ —Å–æ–∑–¥–∞–µ–º –∫–æ–ª–ª–µ–∫—Ü–∏—é —Å –Ω—É–∂–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
                # –†–µ–∞–ª—å–Ω–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è –∑–∞–≤–∏—Å–∏—Ç –æ—Ç –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã—Ö —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–π
                pass
                
            logger.info(f"–°–æ–∑–¥–∞–Ω–∞ –Ω–æ–≤–∞—è –∫–æ–ª–ª–µ–∫—Ü–∏—è: {config.collection_name}")
            
        return True, "collection_created"
        
    except Exception as e:
        logger.exception(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–µ—Ä–µ—Å–æ–∑–¥–∞–Ω–∏–∏ –∫–æ–ª–ª–µ–∫—Ü–∏–∏: {e}")
        return False, f"collection_error: {str(e)}"
====================================================================================================
==================== –ö–û–ù–ï–¶ –§–ê–ô–õ–ê: core\qdrant\qdrant_collections.py ====================



==================== –ù–ê–ß–ê–õ–û –§–ê–ô–õ–ê: core\qdrant\qdrant_collections_additional.py ====================
# –§–∞–π–ª: core\qdrant\qdrant_collections_additional.py
====================================================================================================
"""Additional functions for the qdrant collections module."""

import logging
from typing import Tuple

from config.config_manager import ConfigManager
from core.qdrant_client import get_qdrant_client

logger = logging.getLogger(__name__)


def recreate_collection_from_config(force_recreate: bool = True) -> Tuple[bool, str]:
    """
    –ü–µ—Ä–µ—Å–æ–∑–¥–∞–µ—Ç –∫–æ–ª–ª–µ–∫—Ü–∏—é, –∏—Å–ø–æ–ª—å–∑—É—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –∏–∑ config.json.
    
    Args:
        force_recreate (bool): –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ –ø–µ—Ä–µ—Å–æ–∑–¥–∞—Ç—å –∫–æ–ª–ª–µ–∫—Ü–∏—é.
        
    Returns:
        Tuple[bool, str]: (—É—Å–ø–µ—Ö, —Å—Ç–∞—Ç—É—Å)
    """
    try:
        config_manager = ConfigManager.get_instance()
        config = config_manager.get()
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è —Ö–æ—Ç—è –±—ã –æ–¥–Ω–æ–≥–æ —Ç–∏–ø–∞ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏
        if not (config.index_dense or config.index_bm25 or config.index_hybrid):
            return False, "no_index_type"
            
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–ª—è –≥–∏–±—Ä–∏–¥–Ω–æ–≥–æ —Ä–µ–∂–∏–º–∞
        if config.index_hybrid and not config.index_dense:
            return False, "hybrid_requires_dense"
            
        client = get_qdrant_client(config)
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏—è –∫–æ–ª–ª–µ–∫—Ü–∏–∏
        collections = client.get_collections()
        collection_names = [c.name for c in collections.collections]
        
        if config.collection_name in collection_names and force_recreate:
            # –£–¥–∞–ª–µ–Ω–∏–µ —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–π –∫–æ–ª–ª–µ–∫—Ü–∏–∏
            client.delete_collection(config.collection_name)
            logger.info(f"–£–¥–∞–ª–µ–Ω–∞ —Å—É—â–µ—Å—Ç–≤—É—é—â–∞—è –∫–æ–ª–ª–µ–∫—Ü–∏—è: {config.collection_name}")
            
        if config.collection_name not in collection_names or force_recreate:
            # –°–æ–∑–¥–∞–Ω–∏–µ –Ω–æ–≤–æ–π –∫–æ–ª–ª–µ–∫—Ü–∏–∏
            
            # –°–æ–∑–¥–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω—É—é –∫–æ–ª–ª–µ–∫—Ü–∏—é –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
            if config.index_dense:
                # –î–ª—è –ø–ª–æ—Ç–Ω—ã—Ö –≤–µ–∫—Ç–æ—Ä–æ–≤ —Å–æ–∑–¥–∞–µ–º –∫–æ–ª–ª–µ–∫—Ü–∏—é —Å –Ω—É–∂–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
                # –†–µ–∞–ª—å–Ω–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è –∑–∞–≤–∏—Å–∏—Ç –æ—Ç –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã—Ö —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–π
                pass
                
            logger.info(f"–°–æ–∑–¥–∞–Ω–∞ –Ω–æ–≤–∞—è –∫–æ–ª–ª–µ–∫—Ü–∏—è: {config.collection_name}")
            
        return True, "collection_created"
        
    except Exception as e:
        logger.exception(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–µ—Ä–µ—Å–æ–∑–¥–∞–Ω–∏–∏ –∫–æ–ª–ª–µ–∫—Ü–∏–∏: {e}")
        return False, f"collection_error: {str(e)}"
====================================================================================================
==================== –ö–û–ù–ï–¶ –§–ê–ô–õ–ê: core\qdrant\qdrant_collections_additional.py ====================



==================== –ù–ê–ß–ê–õ–û –§–ê–ô–õ–ê: core\qdrant\__init__.py ====================
# –§–∞–π–ª: core\qdrant\__init__.py
====================================================================================================
 

====================================================================================================
==================== –ö–û–ù–ï–¶ –§–ê–ô–õ–ê: core\qdrant\__init__.py ====================



==================== –ù–ê–ß–ê–õ–û –§–ê–ô–õ–ê: core\search\collection_analyzer.py ====================
# –§–∞–π–ª: core\search\collection_analyzer.py
====================================================================================================
"""–ú–æ–¥—É–ª—å –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –∫–æ–ª–ª–µ–∫—Ü–∏–π Qdrant."""

import logging
from typing import Tuple

from qdrant_client import QdrantClient

logger = logging.getLogger(__name__)


class CollectionAnalyzer:
    """–ö–ª–∞—Å—Å –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –∫–æ–ª–ª–µ–∫—Ü–∏–π Qdrant."""
    
    @staticmethod
    def analyze_collection(client: QdrantClient, collection_name: str, sparse_name: str = None) -> Tuple[bool, bool, str]:
        """
        –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –∫–æ–ª–ª–µ–∫—Ü–∏—é Qdrant –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ç–∏–ø–æ–≤ –≤–µ–∫—Ç–æ—Ä–æ–≤.
        
        Args:
            client (QdrantClient): –ö–ª–∏–µ–Ω—Ç Qdrant.
            collection_name (str): –ù–∞–∑–≤–∞–Ω–∏–µ –∫–æ–ª–ª–µ–∫—Ü–∏–∏.
            sparse_name (str, optional): –ò–º—è sparse –≤–µ–∫—Ç–æ—Ä–∞. –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é None.
            
        Returns:
            Tuple[bool, bool, str]: (has_dense, has_sparse, sparse_vector_name)
        """
        has_dense = False
        has_sparse = False
        sparse_vector_name = sparse_name or "sparse_vector"
        
        try:
            coll_info = client.get_collection(collection_name)
            logger.debug(f"Collection info for '{collection_name}': {coll_info}")
            
            # –ü–æ–ª—É—á–∞–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∫–æ–ª–ª–µ–∫—Ü–∏–∏
            collection_config = getattr(coll_info, 'config', None)
            if collection_config:
                params = getattr(collection_config, 'params', None)
                if params:
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ dense –≤–µ–∫—Ç–æ—Ä–æ–≤
                    vectors = getattr(params, 'vectors', {})
                    # –í–µ–∫—Ç–æ—Ä—ã –º–æ–≥—É—Ç –±—ã—Ç—å –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω—ã –ø–æ-—Ä–∞–∑–Ω–æ–º—É
                    if isinstance(vectors, dict):
                        has_dense = len(vectors) > 0
                        logger.debug(f"Dense vectors config (dict): {vectors}")
                    elif hasattr(vectors, 'size'):  # –≠—Ç–æ VectorParams
                        has_dense = True
                        logger.debug(f"Dense vectors config (VectorParams): {vectors}")
                    else:
                        has_dense = bool(vectors)
                        logger.debug(f"Dense vectors config (other): {vectors}")
                    
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ sparse –≤–µ–∫—Ç–æ—Ä–æ–≤ –≤ —Ä–∞–∑–Ω—ã—Ö –≤–æ–∑–º–æ–∂–Ω—ã—Ö –º–µ—Å—Ç–∞—Ö
                    sparse_vectors = getattr(params, 'sparse_vectors', {})
                    if not sparse_vectors:
                        sparse_vectors = getattr(params, 'sparse_vectors_config', {})
                    
                    has_sparse = bool(sparse_vectors) if isinstance(sparse_vectors, dict) else False
                    logger.debug(f"Sparse vectors config: {sparse_vectors}")
                    
                    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏–º—è sparse –≤–µ–∫—Ç–æ—Ä–∞ –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –ø—Ä–∏ –ø–æ–∏—Å–∫–µ
                    if has_sparse and isinstance(sparse_vectors, dict):
                        sparse_vector_names = list(sparse_vectors.keys())
                        if sparse_vector_names:
                            sparse_vector_name = sparse_vector_names[0]  # –ë–µ—Ä–µ–º –ø–µ—Ä–≤–æ–µ –∏–º—è
                            logger.info(f"Detected sparse vector name: {sparse_vector_name}")
                        else:
                            sparse_vector_name = sparse_name or "sparse_vector"
                            logger.info(f"Using provided sparse vector name: {sparse_vector_name}")
                    else:
                        # Try to detect the sparse vector name from the collection info more thoroughly
                        try:
                            # Check if there are any sparse vectors in the collection's points structure
                            points_sample = client.scroll(
                                collection_name=collection_name,
                                limit=1,
                                with_payload=False,
                                with_vectors=True
                            )
                            if points_sample[0] if isinstance(points_sample, list) and len(points_sample) > 0 else False:
                                point = points_sample[0]
                                if hasattr(point, 'vector') and isinstance(point.vector, dict):
                                    vector_names = list(point.vector.keys())
                                    sparse_candidates = [name for name in vector_names if 'sparse' in name.lower() or 'bm25' in name.lower()]
                                    if sparse_candidates:
                                        sparse_vector_name = sparse_candidates[0]
                                        has_sparse = True
                                        logger.info(f"Detected sparse vector name from point vectors: {sparse_vector_name}")
                                    else:
                                        # Fallback to checking if 'bm25_text' exists
                                        if 'bm25_text' in vector_names:
                                            sparse_vector_name = 'bm25_text'
                                            has_sparse = True
                                            logger.info(f"Found 'bm25_text' sparse vector: {sparse_vector_name}")
                                        else:
                                            sparse_vector_name = sparse_name or "sparse_vector"
                                            logger.info(f"Using provided/fallback sparse vector name: {sparse_vector_name}")
                                else:
                                    sparse_vector_name = sparse_name or "sparse_vector"
                                    logger.info(f"Using provided/fallback sparse vector name: {sparse_vector_name}")
                        except Exception as e:
                            logger.debug(f"Could not detect sparse vector names from points: {e}")
                            sparse_vector_name = sparse_name or "sparse_vector"
                        
                    logger.debug(f"Analysis result: has_dense={has_dense}, has_sparse={has_sparse}, sparse_vector_name={sparse_vector_name}")
                else:
                    # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é –ø—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ–º, —á—Ç–æ –µ—Å—Ç—å dense –≤–µ–∫—Ç–æ—Ä—ã
                    has_dense = True
                    sparse_vector_name = sparse_name or "sparse_vector"
                    logger.debug("Using default values: has_dense=True, has_sparse=False")
            else:
                # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é –ø—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ–º, —á—Ç–æ –µ—Å—Ç—å dense –≤–µ–∫—Ç–æ—Ä—ã
                has_dense = True
                sparse_vector_name = sparse_name or "sparse_vector"
                logger.debug("Using default values: has_dense=True, has_sparse=False")
        except Exception as e:
            logger.debug(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –∫–æ–ª–ª–µ–∫—Ü–∏–∏ '{collection_name}': {e}")
            # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é –ø—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ–º, —á—Ç–æ –µ—Å—Ç—å dense –≤–µ–∫—Ç–æ—Ä—ã (—Å—Ç–∞—Ä–æ–µ –ø–æ–≤–µ–¥–µ–Ω–∏–µ)
            has_dense = True
            sparse_vector_name = sparse_name or "sparse_vector"
            logger.debug("Using default values due to error: has_dense=True, has_sparse=False")
            
        return has_dense, has_sparse, sparse_vector_name


class SearchError(Exception):
    """–ò—Å–∫–ª—é—á–µ–Ω–∏–µ –¥–ª—è –æ—à–∏–±–æ–∫ –ø–æ–∏—Å–∫–∞."""
    pass
====================================================================================================
==================== –ö–û–ù–ï–¶ –§–ê–ô–õ–ê: core\search\collection_analyzer.py ====================



==================== –ù–ê–ß–ê–õ–û –§–ê–ô–õ–ê: core\search\gguf_reranker.py ====================
# –§–∞–π–ª: core\search\gguf_reranker.py
====================================================================================================
"""–ú–æ–¥—É–ª—å –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –ª–æ–∫–∞–ª—å–Ω—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏ reranker –≤ —Ñ–æ—Ä–º–∞—Ç–µ GGUF —á–µ—Ä–µ–∑ Sentence Transformers."""

import logging
import os
from typing import List, Tuple, Any
from sentence_transformers import CrossEncoder

from config.settings import Config

logger = logging.getLogger(__name__)

# –ö—ç—à –¥–ª—è –ª–æ–∫–∞–ª—å–Ω—ã—Ö reranker'–æ–≤
_reranker_cache = {}


class LocalReranker:
    """–ö–ª–∞—Å—Å –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –ª–æ–∫–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª—å—é reranker."""
    
    def __init__(self, model_path: str):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ª–æ–∫–∞–ª—å–Ω–æ–≥–æ reranker.
        
        Args:
            model_path: –ü—É—Ç—å –∫ –ª–æ–∫–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏ reranker
        """
        try:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –ª–∏ —Ñ–∞–π–ª –º–æ–¥–µ–ª–∏
            if not os.path.exists(model_path):
                # –ï—Å–ª–∏ —Ñ–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω, –ø—ã—Ç–∞–µ–º—Å—è –Ω–∞–π—Ç–∏ –µ–≥–æ –≤ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã—Ö –º–µ—Å—Ç–∞—Ö
                models_dir = os.getenv("MODELS_DIR", "")
                possible_paths = [
                    model_path,
                    os.path.join("models", model_path),
                    os.path.join("data", model_path),
                    os.path.join("..", "models", model_path),
                ]
                
                # –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç—å –∏–∑ –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π –æ–∫—Ä—É–∂–µ–Ω–∏—è MODELS_DIR, –µ—Å–ª–∏ –æ–Ω–∞ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞
                if models_dir:
                    possible_paths.append(os.path.join(models_dir, model_path))
                
                found = False
                for path in possible_paths:
                    if os.path.exists(path):
                        model_path = path
                        found = True
                        break
                
                if not found:
                    raise FileNotFoundError(f"–§–∞–π–ª –º–æ–¥–µ–ª–∏ {model_path} –Ω–µ –Ω–∞–π–¥–µ–Ω. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ —É–∫–∞–∑–∞—Ç—å –ø–æ–ª–Ω—ã–π –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É.")
            
            # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –º–æ–¥–µ–ª—å
            self.model = CrossEncoder(model_path)
            logger.info(f"–õ–æ–∫–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å reranker –∑–∞–≥—Ä—É–∂–µ–Ω–∞: {model_path}")
        except Exception as e:
            logger.exception(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –ª–æ–∫–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏ reranker: {e}")
            raise
    
    def predict(self, pairs: List[List[str]]) -> List[float]:
        """–ü–æ–ª—É—á–∞–µ—Ç –æ—Ü–µ–Ω–∫–∏ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ –¥–ª—è –ø–∞—Ä –∑–∞–ø—Ä–æ—Å-–¥–æ–∫—É–º–µ–Ω—Ç."""
        try:
            return self.model.predict(pairs).tolist()
        except Exception as e:
            logger.exception(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–∏ reranker: {e}")
            raise


def get_local_reranker(config: Config) -> LocalReranker:
    """–ü–æ–ª—É—á–∞–µ—Ç –∏–ª–∏ —Å–æ–∑–¥–∞–µ—Ç –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä –ª–æ–∫–∞–ª—å–Ω–æ–≥–æ reranker."""
    global _reranker_cache
    model_name = config.reranker_model
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ –≤ –∫—ç—à–µ –º–æ–¥–µ–ª—å —Å —Ç–∞–∫–∏–º –∏–º–µ–Ω–µ–º
    if model_name in _reranker_cache:
        return _reranker_cache[model_name]
    
    # –ï—Å–ª–∏ –º–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –≤ –∫—ç—à–µ, —Å–æ–∑–¥–∞–µ–º –Ω–æ–≤—É—é
    reranker = LocalReranker(model_name)
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å –≤ –∫—ç—à
    _reranker_cache[model_name] = reranker
    
    return reranker
====================================================================================================
==================== –ö–û–ù–ï–¶ –§–ê–ô–õ–ê: core\search\gguf_reranker.py ====================



==================== –ù–ê–ß–ê–õ–û –§–ê–ô–õ–ê: core\search\query_cache.py ====================
# –§–∞–π–ª: core\search\query_cache.py
====================================================================================================
"""–ú–æ–¥—É–ª—å –¥–ª—è –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è –ø–æ–∏—Å–∫–æ–≤—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤ –∏ –∏—Ö —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –≤ Qdrant."""

import hashlib
import logging
from typing import List, Optional, Tuple

from config.config_manager import ConfigManager
from config.settings import Config
from core.embedding.embedding_manager import EmbeddingManager
from qdrant_client import QdrantClient
from qdrant_client.http import models

logger = logging.getLogger(__name__)

# –ü—Ä–µ—Ñ–∏–∫—Å –¥–ª—è –∫–æ–ª–ª–µ–∫—Ü–∏–π –∫—ç—à–∞
CACHE_COLLECTION_PREFIX = "query_cache_"


class QueryCache:
    """–£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º –ø–æ–∏—Å–∫–æ–≤—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤ –∏ –∏—Ö —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –≤ Qdrant."""
    
    def __init__(self, client: QdrantClient, config: Config):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è QueryCache —Å –∫–ª–∏–µ–Ω—Ç–æ–º Qdrant –∏ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–µ–π.
        
        Args:
            client: –≠–∫–∑–µ–º–ø–ª—è—Ä –∫–ª–∏–µ–Ω—Ç–∞ Qdrant
            config: –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è
        """
        self.client = client
        self.config = config
        self.embedding_manager = EmbeddingManager.get_instance()
    
    def _get_cache_collection_name(self, vector_size: int) -> str:
        """–ü–æ–ª—É—á–∏—Ç—å –∏–º—è –∫–æ–ª–ª–µ–∫—Ü–∏–∏ –∫—ç—à–∞ –¥–ª—è –∑–∞–¥–∞–Ω–Ω–æ–π —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ –≤–µ–∫—Ç–æ—Ä–∞.
        
        Args:
            vector_size: –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –≤–µ–∫—Ç–æ—Ä–∞
            
        Returns:
            str: –ò–º—è –∫–æ–ª–ª–µ–∫—Ü–∏–∏ –∫—ç—à–∞
        """
        return f"{CACHE_COLLECTION_PREFIX}{vector_size}"
    
    def _ensure_cache_collection(self, vector_size: int) -> None:
        """–£–±–µ–¥–∏—Ç—å—Å—è, —á—Ç–æ –∫–æ–ª–ª–µ–∫—Ü–∏—è –∫—ç—à–∞ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –≤ Qdrant –¥–ª—è –∑–∞–¥–∞–Ω–Ω–æ–π —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏.
        
        Args:
            vector_size: –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –≤–µ–∫—Ç–æ—Ä–∞
        """
        try:
            collection_name = self._get_cache_collection_name(vector_size)
            
            # –ü—Ä–æ–≤–µ—Ä–∏—Ç—å, —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –ª–∏ –∫–æ–ª–ª–µ–∫—Ü–∏—è
            collections = self.client.get_collections()
            collection_names = [collection.name for collection in collections.collections]
            
            if collection_name not in collection_names:
                # –°–æ–∑–¥–∞—Ç—å –∫–æ–ª–ª–µ–∫—Ü–∏—é –∫—ç—à–∞
                self.client.create_collection(
                    collection_name=collection_name,
                    vectors_config=models.VectorParams(
                        size=vector_size,
                        distance=models.Distance.COSINE
                    )
                )
                logger.info(f"–°–æ–∑–¥–∞–Ω–∞ –∫–æ–ª–ª–µ–∫—Ü–∏—è –∫—ç—à–∞ '{collection_name}' —Å —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å—é {vector_size}")
            else:
                logger.debug(f"–ö–æ–ª–ª–µ–∫—Ü–∏—è –∫—ç—à–∞ '{collection_name}' —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç")
        except Exception as e:
            logger.exception(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–æ–≤–µ—Ä–∫–µ —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏—è –∫–æ–ª–ª–µ–∫—Ü–∏–∏ –∫—ç—à–∞: {e}")
            raise
    
    def _generate_query_id(self, query_text: str, collection_name: str) -> str:
        """–°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å —É–Ω–∏–∫–∞–ª—å–Ω—ã–π ID –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –µ–≥–æ —Ç–µ–∫—Å—Ç–∞ –∏ —Ü–µ–ª–µ–≤–æ–π –∫–æ–ª–ª–µ–∫—Ü–∏–∏.
        
        Args:
            query_text: –¢–µ–∫—Å—Ç –ø–æ–∏—Å–∫–æ–≤–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞
            collection_name: –ò–º—è –∫–æ–ª–ª–µ–∫—Ü–∏–∏, –≤ –∫–æ—Ç–æ—Ä–æ–π –≤—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è –ø–æ–∏—Å–∫
            
        Returns:
            str: –£–Ω–∏–∫–∞–ª—å–Ω—ã–π —Ö—ç—à-–∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞
        """
        # –°–æ–∑–¥–∞—Ç—å —É–Ω–∏–∫–∞–ª—å–Ω—ã–π –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–µ–∫—Å—Ç–∞ –∑–∞–ø—Ä–æ—Å–∞ –∏ –∏–º–µ–Ω–∏ –∫–æ–ª–ª–µ–∫—Ü–∏–∏
        unique_string = f"{query_text}:{collection_name}"
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤—ã–µ 32 —Å–∏–º–≤–æ–ª–∞ —Ö—ç—à–∞ –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å UUID
        return hashlib.sha256(unique_string.encode('utf-8')).hexdigest()[:32]
    
    def get_cached_query(self, query_text: str, collection_name: str, vector_size: int) -> Optional[Tuple[List[float], str]]:
        """–ü–æ–ª—É—á–∏—Ç—å –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —ç–º–±–µ–¥–¥–∏–Ω–≥ –∑–∞–ø—Ä–æ—Å–∞, –µ—Å–ª–∏ –æ–Ω —Å—É—â–µ—Å—Ç–≤—É–µ—Ç.
        
        Args:
            query_text: –¢–µ–∫—Å—Ç –ø–æ–∏—Å–∫–æ–≤–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞
            collection_name: –ò–º—è –∫–æ–ª–ª–µ–∫—Ü–∏–∏, –≤ –∫–æ—Ç–æ—Ä–æ–π –≤—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è –ø–æ–∏—Å–∫
            vector_size: –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –≤–µ–∫—Ç–æ—Ä–∞
            
        Returns:
            Optional[Tuple[List[float], str]]: (–≤–µ–∫—Ç–æ—Ä —ç–º–±–µ–¥–¥–∏–Ω–≥–∞, —Ç–µ–∫—Å—Ç –∑–∞–ø—Ä–æ—Å–∞) –µ—Å–ª–∏ –Ω–∞–π–¥–µ–Ω, –∏–Ω–∞—á–µ None
        """
        try:
            cache_collection_name = self._get_cache_collection_name(vector_size)
            query_id = self._generate_query_id(query_text, collection_name)
            
            # –ü—Ä–æ–≤–µ—Ä–∏—Ç—å, —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –ª–∏ –∫–æ–ª–ª–µ–∫—Ü–∏—è –∫—ç—à–∞
            collections = self.client.get_collections()
            collection_names = [collection.name for collection in collections.collections]
            
            if cache_collection_name not in collection_names:
                # –ö–æ–ª–ª–µ–∫—Ü–∏—è –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º None
                logger.debug(f"–ö–æ–ª–ª–µ–∫—Ü–∏—è –∫—ç—à–∞ '{cache_collection_name}' –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç")
                return None
            
            # –ü–æ–∏—Å–∫ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞ –ø–æ ID
            response = self.client.retrieve(
                collection_name=cache_collection_name,
                ids=[query_id],
                with_vectors=True,
                with_payload=True
            )
            
            if response:
                point = response[0]
                vector = point.vector if hasattr(point, 'vector') else point.vectors
                payload = point.payload if hasattr(point, 'payload') else {}
                
                # –ü—Ä–æ–≤–µ—Ä–∏—Ç—å, —á—Ç–æ —Ç–µ–∫—Å—Ç –∑–∞–ø—Ä–æ—Å–∞ —Å–æ–≤–ø–∞–¥–∞–µ—Ç (–¥–≤–æ–π–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞)
                if payload.get('query_text') == query_text:
                    logger.debug(f"–ù–∞–π–¥–µ–Ω –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∑–∞–ø—Ä–æ—Å –¥–ª—è '{query_text}' –≤ –∫–æ–ª–ª–µ–∫—Ü–∏–∏ '{collection_name}'")
                    return (vector, query_text)
            
            logger.debug(f"–ö—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∑–∞–ø—Ä–æ—Å –Ω–µ –Ω–∞–π–¥–µ–Ω –¥–ª—è '{query_text}' –≤ –∫–æ–ª–ª–µ–∫—Ü–∏–∏ '{collection_name}'")
            return None
            
        except Exception as e:
            logger.exception(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞: {e}")
            return None
    
    def cache_query(self, query_text: str, collection_name: str, vector: List[float]) -> bool:
        """–ö—ç—à–∏—Ä–æ–≤–∞—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥ –∑–∞–ø—Ä–æ—Å–∞ –≤ Qdrant.
        
        Args:
            query_text: –¢–µ–∫—Å—Ç –ø–æ–∏—Å–∫–æ–≤–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞
            collection_name: –ò–º—è –∫–æ–ª–ª–µ–∫—Ü–∏–∏, –≤ –∫–æ—Ç–æ—Ä–æ–π –≤—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è –ø–æ–∏—Å–∫
            vector: –í–µ–∫—Ç–æ—Ä —ç–º–±–µ–¥–¥–∏–Ω–≥–∞ –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞
            
        Returns:
            bool: True –µ—Å–ª–∏ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–æ—à–ª–æ —É—Å–ø–µ—à–Ω–æ, False –≤ –ø—Ä–æ—Ç–∏–≤–Ω–æ–º —Å–ª—É—á–∞–µ
        """
        try:
            vector_size = len(vector)
            cache_collection_name = self._get_cache_collection_name(vector_size)
            
            # –£–±–µ–¥–∏—Ç—å—Å—è, —á—Ç–æ –∫–æ–ª–ª–µ–∫—Ü–∏—è –∫—ç—à–∞ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
            self._ensure_cache_collection(vector_size)
            
            query_id = self._generate_query_id(query_text, collection_name)
            
            # –ü–æ–¥–≥–æ—Ç–æ–≤–∏—Ç—å payload —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏
            payload = {
                "query_text": query_text,
                "collection_name": collection_name,
                "query_hash": query_id
            }
            
            # –í—Å—Ç–∞–≤–∏—Ç—å –∏–ª–∏ –æ–±–Ω–æ–≤–∏—Ç—å –∑–∞–ø—Ä–æ—Å –≤ –∫–æ–ª–ª–µ–∫—Ü–∏–∏ –∫—ç—à–∞
            self.client.upsert(
                collection_name=cache_collection_name,
                points=[
                    models.PointStruct(
                        id=query_id,
                        vector=vector,
                        payload=payload
                    )
                ]
            )
            
            logger.debug(f"–ó–∞–∫—ç—à–∏—Ä–æ–≤–∞–Ω –∑–∞–ø—Ä–æ—Å '{query_text}' –¥–ª—è –∫–æ–ª–ª–µ–∫—Ü–∏–∏ '{collection_name}' –≤ –∫–æ–ª–ª–µ–∫—Ü–∏–∏ '{cache_collection_name}'")
            return True
            
        except Exception as e:
            logger.exception(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–∏ –∑–∞–ø—Ä–æ—Å–∞: {e}")
            return False
    
    def get_or_create_query_embedding(self, query_text: str, collection_name: str, vector_size: int) -> List[float]:
        """–ü–æ–ª—É—á–∏—Ç—å –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —ç–º–±–µ–¥–¥–∏–Ω–≥ –∑–∞–ø—Ä–æ—Å–∞ –∏–ª–∏ —Å–æ–∑–¥–∞—Ç—å –Ω–æ–≤—ã–π.
        
        Args:
            query_text: –¢–µ–∫—Å—Ç –ø–æ–∏—Å–∫–æ–≤–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞
            collection_name: –ò–º—è –∫–æ–ª–ª–µ–∫—Ü–∏–∏, –≤ –∫–æ—Ç–æ—Ä–æ–π –≤—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è –ø–æ–∏—Å–∫
            vector_size: –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –≤–µ–∫—Ç–æ—Ä–∞
            
        Returns:
            List[float]: –í–µ–∫—Ç–æ—Ä —ç–º–±–µ–¥–¥–∏–Ω–≥–∞ –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞
        """
        # –°–Ω–∞—á–∞–ª–∞ –ø–æ–ø—Ä–æ–±–æ–≤–∞—Ç—å –ø–æ–ª—É—á–∏—Ç—å –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —ç–º–±–µ–¥–¥–∏–Ω–≥
        cached_result = self.get_cached_query(query_text, collection_name, vector_size)
        if cached_result:
            return cached_result[0]  # –í–µ—Ä–Ω—É—Ç—å –≤–µ–∫—Ç–æ—Ä
        
        # –ï—Å–ª–∏ –Ω–µ –∑–∞–∫—ç—à–∏—Ä–æ–≤–∞–Ω, —Å–æ–∑–¥–∞—Ç—å –Ω–æ–≤—ã–π —ç–º–±–µ–¥–¥–∏–Ω–≥
        logger.debug(f"–ó–∞–ø—Ä–æ—Å –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ –∫—ç—à–µ, —Å–æ–∑–¥–∞–Ω–∏–µ –Ω–æ–≤–æ–≥–æ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞ –¥–ª—è '{query_text}'")
        embedding = self.embedding_manager.embed_query(query_text, self.config)
        
        # –ö—ç—à–∏—Ä–æ–≤–∞—Ç—å –Ω–æ–≤—ã–π —ç–º–±–µ–¥–¥–∏–Ω–≥
        self.cache_query(query_text, collection_name, embedding)
        
        return embedding
====================================================================================================
==================== –ö–û–ù–ï–¶ –§–ê–ô–õ–ê: core\search\query_cache.py ====================



==================== –ù–ê–ß–ê–õ–û –§–ê–ô–õ–ê: core\search\reranker_manager.py ====================
# –§–∞–π–ª: core\search\reranker_manager.py
====================================================================================================
"""Module for centralized reranker management."""

import logging
from collections import OrderedDict
from typing import List, Tuple, Optional, Any

from langchain_core.documents import Document
from sentence_transformers import CrossEncoder

from config.settings import Config
from core.search.gguf_reranker import get_local_reranker, LocalReranker

logger = logging.getLogger(__name__)


class RerankerError(Exception):
    """Custom exception for reranker management errors."""
    pass


class RerankerManager:
    """Centralized manager for rerankers with caching."""
    
    _instance: Optional['RerankerManager'] = None
    MAX_CACHE_SIZE = 2  # –ú–∞–∫—Å–∏–º—É–º 2 –º–æ–¥–µ–ª–∏ –≤ –∫—ç—à–µ –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ
    
    def __init__(self):
        """Initialize RerankerManager."""
        self._reranker_cache: OrderedDict = OrderedDict()
        
    @classmethod
    def get_instance(cls) -> 'RerankerManager':
        """Get singleton instance of RerankerManager.
        
        Returns:
            RerankerManager: Singleton instance
        """
        if cls._instance is None:
            cls._instance = cls()
        return cls._instance
    
    def get_reranker(self, config: Config) -> CrossEncoder:
        """–ü–æ–ª—É—á–∞–µ—Ç –∏–ª–∏ —Å–æ–∑–¥–∞–µ—Ç –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä reranker.
        
        Args:
            config: Configuration object
            
        Returns:
            CrossEncoder or LocalReranker: Reranker instance
            
        Raises:
            RerankerError: If there's an error creating the reranker
        """
        try:
            model_name = config.reranker_model
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ –º–æ–¥–µ–ª—å –ª–æ–∫–∞–ª—å–Ω–æ–π (–ø–æ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—é .gguf)
            is_local = model_name.endswith('.gguf')
            
            # –ö–ª—é—á –∫—ç—à–∞ - –∫–æ—Ä—Ç–µ–∂ (–º–æ–¥–µ–ª—å, is_local)
            cache_key = (model_name, is_local)
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ –≤ –∫—ç—à–µ –º–æ–¥–µ–ª—å —Å —Ç–∞–∫–∏–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
            if cache_key in self._reranker_cache:
                # –ü–µ—Ä–µ–º–µ—â–∞–µ–º –º–æ–¥–µ–ª—å –≤ –Ω–∞—á–∞–ª–æ OrderedDict (–æ–±–Ω–æ–≤–ª—è–µ–º –≤—Ä–µ–º—è –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è)
                self._reranker_cache.move_to_end(cache_key, last=True)
                return self._reranker_cache[cache_key]
            
            # –ï—Å–ª–∏ –∫—ç—à –ø–µ—Ä–µ–ø–æ–ª–Ω–µ–Ω, —É–¥–∞–ª—è–µ–º —Å–∞–º—É—é —Å—Ç–∞—Ä—É—é –∑–∞–ø–∏—Å—å
            if len(self._reranker_cache) >= self.MAX_CACHE_SIZE:
                # –£–¥–∞–ª—è–µ–º —Å–∞–º—É—é —Å—Ç–∞—Ä—É—é –∑–∞–ø–∏—Å—å (–ø–µ—Ä–≤—É—é –≤ OrderedDict)
                oldest_key, _ = self._reranker_cache.popitem(last=False)
                logger.info(f"–£–¥–∞–ª–µ–Ω–∞ —Å—Ç–∞—Ä–∞—è –º–æ–¥–µ–ª—å –∏–∑ –∫—ç—à–∞: {oldest_key}")
            
            # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–π reranker
            if is_local:
                reranker = get_local_reranker(config)
            else:
                reranker = CrossEncoder(model_name)
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å –≤ –∫—ç—à
            self._reranker_cache[cache_key] = reranker
            
            return reranker
            
        except Exception as e:
            logger.exception(f"Error creating reranker: {e}")
            raise RerankerError(f"Failed to create reranker: {e}")
    
    def clear_cache(self) -> None:
        """–û—á–∏—â–∞–µ—Ç –∫—ç—à rerankers."""
        self._reranker_cache.clear()
        logger.info("–ö—ç—à rerankers –æ—á–∏—â–µ–Ω")
    
    def get_cache_info(self) -> dict:
        """–ü–æ–ª—É—á–∞–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –∫—ç—à–µ rerankers.
        
        Returns:
            dict: Cache information
        """
        return {
            "cache_size": len(self._reranker_cache),
            "max_cache_size": self.MAX_CACHE_SIZE,
            "cached_models": list(self._reranker_cache.keys())
        }
    
    def rerank_documents(self, query: str, documents: List[Tuple[Any, float]], config: Config) -> List[Tuple[Any, float]]:
        """–ü–µ—Ä–µ—Ä–∞–Ω–∂–∏—Ä—É–µ—Ç —Å–ø–∏—Å–æ–∫ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤.
        
        Args:
            query: –ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
            documents: –°–ø–∏—Å–æ–∫ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Å –æ—Ü–µ–Ω–∫–∞–º–∏ (Document, score)
            config: Configuration object
            
        Returns:
            List[Tuple[Any, float]]: –ü–µ—Ä–µ—Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã —Å –Ω–æ–≤—ã–º–∏ –æ—Ü–µ–Ω–∫–∞–º–∏
        """
        try:
            if not documents:
                return []
            
            # –ü–æ–ª—É—á–∞–µ–º reranker
            reranker = self.get_reranker(config)
            
            # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –ø–∞—Ä—ã (–∑–∞–ø—Ä–æ—Å, —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ)
            pairs = []
            doc_objects = []
            
            for doc, score in documents:
                # –ò–∑–≤–ª–µ–∫–∞–µ–º —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ
                if isinstance(doc, dict):
                    content = doc.get('content', '')
                elif hasattr(doc, 'page_content'):
                    content = doc.page_content
                else:
                    content = str(doc)
                
                if content:
                    pairs.append([query, content])
                    doc_objects.append((doc, score))
                else:
                    logger.debug(f"Document has no content: {type(doc)}, keys: {list(doc.keys()) if isinstance(doc, dict) else 'not dict'}")
            
            logger.debug(f"Prepared {len(pairs)} pairs for reranking, {len(doc_objects)} doc objects")
            
            if not pairs:
                logger.debug("No pairs to rerank, returning original documents")
                return documents
            
            # –ü–æ–ª—É—á–∞–µ–º –æ—Ü–µ–Ω–∫–∏ –æ—Ç reranker
            scores = reranker.predict(pairs)
            
            # –°–æ–∑–¥–∞–µ–º –ø–∞—Ä—ã (–¥–æ–∫—É–º–µ–Ω—Ç, –Ω–æ–≤–∞—è_–æ—Ü–µ–Ω–∫–∞) –∏ —Å–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —É–±—ã–≤–∞–Ω–∏—é –æ—Ü–µ–Ω–æ–∫
            reranked_docs = []
            for i, (doc, original_score) in enumerate(doc_objects):
                # –ï—Å–ª–∏ –¥–æ–∫—É–º–µ–Ω—Ç —É–∂–µ —è–≤–ª—è–µ—Ç—Å—è —Å–ª–æ–≤–∞—Ä–µ–º, –æ–±–Ω–æ–≤–ª—è–µ–º –µ–≥–æ, –∏–Ω–∞—á–µ —Å–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–π —Å–ª–æ–≤–∞—Ä—å
                if isinstance(doc, dict):
                    # –î–æ–±–∞–≤–ª—è–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—É—é –æ—Ü–µ–Ω–∫—É –∫ –¥–æ–∫—É–º–µ–Ω—Ç—É
                    doc_with_scores = doc.copy()
                    doc_with_scores['original_score'] = original_score
                    doc_with_scores['reranker_score'] = float(scores[i])
                    reranked_docs.append((doc_with_scores, float(scores[i])))
                else:
                    # –°–æ–∑–¥–∞–µ–º —Å–ª–æ–≤–∞—Ä—å —Å —Å–æ–¥–µ—Ä–∂–∏–º—ã–º –∏ –æ—Ü–µ–Ω–∫–∞–º–∏
                    doc_with_scores = {
                        'content': getattr(doc, 'page_content', str(doc)),
                        'metadata': getattr(doc, 'metadata', {}),
                        'original_score': original_score,
                        'reranker_score': float(scores[i])
                    }
                    reranked_docs.append((doc_with_scores, float(scores[i])))
            
            # –°–æ—Ä—Ç–∏—Ä—É–µ–º –¥–æ–∫—É–º–µ–Ω—Ç—ã –ø–æ –æ—Ü–µ–Ω–∫–∞–º reranker (–ø–æ —É–±—ã–≤–∞–Ω–∏—é)
            reranked_docs.sort(key=lambda x: x[1], reverse=True)
            
            # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Ç–æ–ø-K
            top_k = min(config.reranker_top_k, len(reranked_docs))
            return reranked_docs[:top_k]
            
        except Exception as e:
            logger.exception(f"Error reranking documents: {e}")
            # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –≤ —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏
            return documents
====================================================================================================
==================== –ö–û–ù–ï–¶ –§–ê–ô–õ–ê: core\search\reranker_manager.py ====================



==================== –ù–ê–ß–ê–õ–û –§–ê–ô–õ–ê: core\search\searcher.py ====================
# –§–∞–π–ª: core\search\searcher.py
====================================================================================================
"""–ú–æ–¥—É–ª—å –¥–ª—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –ø–æ–∏—Å–∫–∞ –≤ Qdrant."""

import logging
from typing import Any, Dict, List, Optional, Tuple
from qdrant_client.http.models import CountResult

from config.config_manager import ConfigManager
from config.settings import Config
from core.embedding.embeddings import get_dense_embedder, get_search_device
from core.qdrant.qdrant_client import aget_qdrant_client
from core.search.collection_analyzer import CollectionAnalyzer
from core.search.search_executor import SearchExecutor
from core.search.search_strategy import SearchStrategy
from core.search.query_cache import QueryCache

logger = logging.getLogger(__name__)


async def search_in_collection(query: str, collection_name: str, device: str, k: int = None, hybrid: bool = False, 
                              search_mode_override: str = None, metadata_filter: Optional[Dict[str, Any]] = None, client = None) -> Tuple[List[Tuple[Any, float]], Optional[str]]:
    """
    –í—ã–ø–æ–ª–Ω—è–µ—Ç –ø–æ–∏—Å–∫ –≤ —É–∫–∞–∑–∞–Ω–Ω–æ–π –∫–æ–ª–ª–µ–∫—Ü–∏–∏ Qdrant.
    
    Args:
        query (str): –ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å.
        collection_name (str): –ù–∞–∑–≤–∞–Ω–∏–µ –∫–æ–ª–ª–µ–∫—Ü–∏–∏.
        device (str): –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –¥–ª—è –ø–æ–∏—Å–∫–∞ ("cpu" –∏–ª–∏ "cuda").
        k (int): –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤. –ï—Å–ª–∏ None, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –∑–Ω–∞—á–µ–Ω–∏–µ –∏–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏.
        hybrid (bool): –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å hybrid search (dense + sparse).
        metadata_filter (Optional[Dict[str, Any]]): –§–∏–ª—å—Ç—Ä –ø–æ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º.
        client (QdrantClient, optional): –ö–ª–∏–µ–Ω—Ç Qdrant. –ï—Å–ª–∏ –Ω–µ —É–∫–∞–∑–∞–Ω, —Å–æ–∑–¥–∞–µ—Ç—Å—è –Ω–æ–≤—ã–π.
        
    Returns:
        Tuple[List[Tuple[Any, float]], Optional[str]]: (—Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞, –æ—à–∏–±–∫–∞)
    """
    try:
        config_manager = ConfigManager.get_instance()
        config: Config = config_manager.get()
        
        # –ï—Å–ª–∏ k –Ω–µ —É–∫–∞–∑–∞–Ω–æ, –∏—Å–ø–æ–ª—å–∑—É–µ–º –∑–Ω–∞—á–µ–Ω–∏–µ –∏–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
        if k is None:
            k = config.search_default_k
            
        # –ï—Å–ª–∏ –∫–ª–∏–µ–Ω—Ç –Ω–µ –ø–µ—Ä–µ–¥–∞–Ω, —Å–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–π (–∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ —á—Ç–æ–±—ã –Ω–µ –±–ª–æ–∫–∏—Ä–æ–≤–∞—Ç—å loop)
        if client is None:
            client = await aget_qdrant_client(config)
            
        # –ü–æ–ª—É—á–∞–µ–º —ç–º–±–µ–¥–¥–µ—Ä –¥–ª—è –ø–æ–∏—Å–∫–∞
        search_device = get_search_device(device)
        embedder = get_dense_embedder(config, search_device)
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º sparse embedding –µ—Å–ª–∏ –æ–Ω –Ω—É–∂–µ–Ω
        sparse_emb = None
        analyzer = CollectionAnalyzer()
        has_dense, has_sparse, sparse_vector_name = analyzer.analyze_collection(client, collection_name)
        
        logger.info(f"Collection analysis: has_dense={has_dense}, has_sparse={has_sparse}, sparse_vector_name={sparse_vector_name}")
        logger.info(f"Search parameters: hybrid={hybrid}, device={device}")
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º sparse embedding –µ—Å–ª–∏ –æ–Ω –Ω—É–∂–µ–Ω –∏ –≤–∫–ª—é—á–µ–Ω –≤ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
        sparse_emb = None
        if has_sparse and config.use_bm25:
            try:
                from core.embedding.sparse_embedding_adapter import (
                    SparseEmbeddingAdapter,
                )
                sparse_emb = SparseEmbeddingAdapter(config)  # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–æ–ª–Ω—ã–π config –æ–±—ä–µ–∫—Ç
                logger.info(f"Sparse embedding adapter initialized: {config.sparse_embedding}")
                
                # Debug sample doc sparse
                try:
                    sample_points = client.scroll(
                        collection_name=collection_name,
                        limit=1,
                        with_vectors=True
                    )
                    if sample_points and len(sample_points[0]) > 0:
                        sample_point = sample_points[0][0]  # First point
                        if hasattr(sample_point, 'vectors') and sample_point.vectors:
                            sparse_vec_sample = sample_point.vectors.get(config.sparse_vector_name)
                            if sparse_vec_sample:
                                logger.info(f"Sample doc sparse: indices={sparse_vec_sample.indices[:5]}..., non-zero={sum(1 for v in sparse_vec_sample.values if v > 0)}")
                            else:
                                logger.warning("No sparse vector in sample point")
                        else:
                            logger.warning("No vectors in sample point")
                except Exception as scroll_error:
                    logger.error(f"Debug scroll error: {scroll_error}")
            except ImportError:
                logger.warning("fastembed –Ω–µ –¥–æ—Å—Ç—É–ø–µ–Ω ‚Äî –æ—Ç–∫–ª—é—á–µ–Ω–∏–µ sparse search.")
                has_sparse = False
                if not has_dense:
                    return [], "–ù–µ—Ç –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –≤–µ–∫—Ç–æ—Ä–æ–≤ –¥–ª—è –ø–æ–∏—Å–∫–∞ (dense –∏ sparse –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã)"
            except Exception as e:
                logger.exception(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ sparse embedding: {e}")
                has_sparse = False
                if not has_dense:
                    return [], f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ sparse embedding: {str(e)}"
        elif has_sparse and not config.use_bm25:
            # –ï—Å–ª–∏ –∫–æ–ª–ª–µ–∫—Ü–∏—è –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç sparse, –Ω–æ BM25 –æ—Ç–∫–ª—é—á–µ–Ω –≤ –∫–æ–Ω—Ñ–∏–≥–µ, 
            # –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º sparse embedding, –Ω–æ –ø–æ–º–µ—á–∞–µ–º, —á—Ç–æ sparse –≤–µ–∫—Ç–æ—Ä—ã –µ—Å—Ç—å
            logger.info("Sparse vectors exist in collection but BM25 is disabled in config - sparse search will be skipped")
            has_sparse = False  # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ –æ—Ç–∫–ª—é—á–∞–µ–º sparse –µ—Å–ª–∏ –æ–Ω –æ—Ç–∫–ª—é—á–µ–Ω –≤ –∫–æ–Ω—Ñ–∏–≥–µ
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Å—Ç—Ä–∞—Ç–µ–≥–∏—é –ø–æ–∏—Å–∫–∞
        strategy = SearchStrategy(client, collection_name, embedder, sparse_emb)
        # –ï—Å–ª–∏ –∑–∞–¥–∞–Ω —Ä–µ–∂–∏–º –ø–æ–∏—Å–∫–∞ —è–≤–Ω–æ, –∏—Å–ø–æ–ª—å–∑—É–µ–º –µ–≥–æ, –∏–Ω–∞—á–µ –æ–ø—Ä–µ–¥–µ–ª—è–µ–º –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏
        if search_mode_override and search_mode_override in ["dense", "sparse", "hybrid"]:
            search_mode = search_mode_override
            logger.info(f"Using search mode override: {search_mode_override}")
        else:
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø –ø–æ–∏—Å–∫–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –≥–∏–±—Ä–∏–¥–Ω–æ–≥–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞ –∏ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –≤–µ–∫—Ç–æ—Ä–æ–≤
            search_type = "hybrid" if hybrid else "auto"  # "auto" –¥–ª—è —Å—Ç–∞—Ä–æ–≥–æ –ø–æ–≤–µ–¥–µ–Ω–∏—è, –µ—Å–ª–∏ –Ω–µ hybrid
            search_mode = strategy.determine_search_mode(hybrid, search_type)
        
        logger.info(f"Search mode determined: {search_mode}")
        
        # –¢–µ–ø–µ—Ä—å –≤–æ–∑–≤—Ä–∞—â–∞–µ–º client, search_mode, etc.
        client, search_mode, vector_name, sparse_params = strategy.create_qdrant_searcher(search_mode)
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –∫—ç—à –∑–∞–ø—Ä–æ—Å–æ–≤
        query_cache = QueryCache(client, config)
        
        # –ü–æ–ª—É—á–∞–µ–º —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –≤–µ–∫—Ç–æ—Ä–∞ –¥–ª—è —Ç–µ–∫—É—â–µ–π –º–æ–¥–µ–ª–∏
        # –î–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ —Å–æ–∑–¥–∞–µ–º —Ç–µ—Å—Ç–æ–≤—ã–π —ç–º–±–µ–¥–¥–∏–Ω–≥
        test_embedding = embedder.embed_query("test")
        vector_size = len(test_embedding)
        
        # –ü–æ–ª—É—á–∞–µ–º –∏–ª–∏ —Å–æ–∑–¥–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥ –∑–∞–ø—Ä–æ—Å–∞ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –∫—ç—à–∞
        query_vector = query_cache.get_or_create_query_embedding(query, collection_name, vector_size)
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ—á–µ–∫ –≤ –∫–æ–ª–ª–µ–∫—Ü–∏–∏
        count = client.count(collection_name=collection_name)
        logger.info(f"Collection '{collection_name}' has {count.count} points")
        
        # –î–ª—è –¥—Ä—É–≥–∏—Ö —Ä–µ–∂–∏–º–æ–≤: –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å SearchExecutor.execute_search(client, search_mode, vector_name, sparse_params, query, k, metadata_filter)
        logger.info(f"Calling SearchExecutor.execute_search with mode '{search_mode}', vector_name '{vector_name}', sparse_params keys: {list(sparse_params.keys()) if sparse_params else 'None'}")
        if search_mode != "dense":
            results, error = await SearchExecutor.execute_search(client, search_mode, vector_name, sparse_params, query, k, metadata_filter)
        else:
            # –í—ã–ø–æ–ª–Ω—è–µ–º –ø–æ–∏—Å–∫ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –≤–µ–∫—Ç–æ—Ä–∞
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º SearchExecutor –¥–ª—è –ø–ª–æ—Ç–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞ —Ç–æ–∂–µ
            results, error = await SearchExecutor.execute_search(client, search_mode, vector_name, sparse_params, query, k, metadata_filter)
        
        logger.info(f"SearchExecutor returned: {len(results) if results else 0} results, error: {error}")
        
        # Debug logging to check results from SearchExecutor
        if results:
            logger.info(f"Results from SearchExecutor: {len(results)} items")
            for i, (result, score) in enumerate(results[:3]):  # Log first 3 results
                logger.info(f"  Result {i}: score={score}, type={type(result)}, content_len={len(result.get('content', '')) if isinstance(result, dict) else 'N/A'}, source={result.get('source', 'N/A') if isinstance(result, dict) else 'N/A'}")
                if isinstance(result, dict):
                    logger.info(f"    Keys: {list(result.keys())}")
                    logger.info(f"    Content preview: {str(result.get('content', ''))[:100] if result.get('content') else 'NO CONTENT'}")
                    logger.info(f"    Original score: {result.get('original_score', 'NO ORIGINAL SCORE')}")
        else:
            logger.debug("No results returned from SearchExecutor")
        
        # Log the error returned from SearchExecutor
        if error:
            logger.error(f"Error from SearchExecutor: {error}")
        
        # Store original results count for comparison after further processing
        original_results_count = len(results) if results else 0
        logger.debug(f"Before additional processing: {original_results_count} results")
        
        logger.debug(f"Starting additional processing for {len(results)} results")
        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –¥–ª—è –≤–µ–±-–∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞
        logger.debug(f"Starting additional processing for {len(results)} results")
        processed_results = []
        for i, (result, score) in enumerate(results):
            logger.debug(f"Processing result {i}: type={type(result)}, score={score}")
            if isinstance(result, dict):
                # –≠—Ç–æ —É–∂–µ —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç –∏–∑ SearchExecutor
                logger.debug(f"  Dict result keys: {list(result.keys()) if isinstance(result, dict) else 'N/A'}")
                logger.debug(f"  Dict content: '{result.get('content', 'NO CONTENT')[:50]}...' if result.get('content') else 'NO CONTENT'")
                logger.debug(f"  Dict source: {result.get('source', 'NO SOURCE')}")
                logger.debug(f"  Dict original_score: {result.get('original_score', 'NO ORIGINAL SCORE')}")
                processed_results.append((result, score))
            else:
                # –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç - –ø—ã—Ç–∞–µ–º—Å—è –∏–∑–≤–ª–µ—á—å —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ
                logger.debug(f"  Processing non-dict result: {type(result)}")
                content = getattr(result, 'page_content', '')
                metadata = getattr(result, 'metadata', {})
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ payload —Å —Å–æ–¥–µ—Ä–∂–∏–º—ã–º
                if not content and hasattr(result, 'payload'):
                    payload = result.payload
                    content = payload.get('content', '')
                    if not content:
                        content = payload.get('page_content', '')
                    # –ö–æ–ø–∏—Ä—É–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –∏–∑ payload –µ—Å–ª–∏ –æ–Ω–∏ –µ—Å—Ç—å
                    if 'metadata' in payload:
                        metadata = payload['metadata']
                
                # –ï—Å–ª–∏ content –≤—Å–µ –µ—â–µ –ø—É—Å—Ç—ã–π, –ø—Ä–æ–≤–µ—Ä—è–µ–º __dict__
                if not content and hasattr(result, '__dict__'):
                    result_dict = result.__dict__
                    content = result_dict.get('content', '')
                    if not content:
                        content = result_dict.get('page_content', '')
                    if not metadata and 'metadata' in result_dict:
                        metadata = result_dict['metadata']
                
                processed_results.append(
                    (
                        {
                            'content': content if content is not None else '',
                            'metadata': metadata,
                            'original_score': score,  # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—É—é –æ—Ü–µ–Ω–∫—É
                            'score': score
                        }, 
                        score
                    )
                )
        
        logger.debug(f"Completed additional processing: {len(processed_results)} results")
        for i, (result, score) in enumerate(processed_results[:3]):
            logger.debug(f"  Processed result {i}: score={score}, content_len={len(result.get('content', ''))}, source={result.get('source', 'N/A')}, original_score={result.get('original_score', 'N/A')}")
            logger.debug(f"    Full result keys: {list(result.keys())}")
            logger.debug(f"    Content preview: {str(result.get('content', ''))[:100] if result.get('content') else 'NO CONTENT'}")
        
        # Log results before potential content fetching from Qdrant
        logger.info(f"Before Qdrant content fetch: {len(processed_results)} results")
        for i, (result, score) in enumerate(processed_results[:2]):
            logger.info(f"  Before Qdrant fetch {i}: score={score}, orig_score={result.get('original_score', 'NO ORIG')}, content_len={len(result.get('content', ''))}")
        
        # –ï—Å–ª–∏ –∫–æ–Ω—Ç–µ–Ω—Ç –ø—É—Å—Ç–æ–π, –ø—ã—Ç–∞–µ–º—Å—è –ø–æ–ª—É—á–∏—Ç—å –µ–≥–æ –Ω–∞–ø—Ä—è–º—É—é –∏–∑ Qdrant
        if all(not result.get('content', '') for result, _ in processed_results) and client:
            logger.debug("Content is empty in all results, attempting to fetch directly from Qdrant")
            try:
                # –ü–æ–ª—É—á–∞–µ–º —Ç–æ—á–∫–∏ –Ω–∞–ø—Ä—è–º—É—é –∏–∑ Qdrant –ø–æ ID —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
                # Note: This only works if we have the original point IDs available
                # Since RRF query processes the points, we need to make sure they were stored properly
                # This is a fallback mechanism for when content extraction from RRF payload fails
                point_ids = []
                # We need to extract IDs differently - since we're using RRF, the original point IDs might not be available
                # The RRF query_points returns ScoredPoint objects with IDs, but we need to maintain that info
                
                # This is tricky because after RRF processing, we lose the original point ID references
                # That's why it's important to have proper content extraction in SearchExecutor
                logger.debug("Skipping direct Qdrant retrieval as it requires original point IDs")
            except Exception as retrieve_error:
                logger.warning(f"Failed to retrieve content directly from Qdrant: {retrieve_error}")
        
        # –ü—Ä–∏–º–µ–Ω—è–µ–º reranker –µ—Å–ª–∏ –≤–∫–ª—é—á—ë–Ω (–ø–æ—Å–ª–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –∫–æ–Ω—Ç–µ–Ω—Ç–∞)
        config_manager = ConfigManager.get_instance()
        config = config_manager.get()
        if config.reranker_enabled and processed_results:
            from core.search.reranker_manager import RerankerManager
            reranker_manager = RerankerManager.get_instance()
            # –õ–æ–≥–∏—Ä—É–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –ø–µ—Ä–µ–¥ reranking –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
            logger.debug(f"Before reranking: {len(processed_results)} results")
            for i, (result, score) in enumerate(processed_results[:3]):  # –õ–æ–≥–∏—Ä—É–µ–º –ø–µ—Ä–≤—ã–µ 3
                logger.debug(f"  Result {i}: score={score}, content_len={len(result.get('content', ''))}, source={result.get('source', 'N/A')}")
            
            processed_results = reranker_manager.rerank_documents(query, processed_results, config)
            
            logger.debug(f"After reranking: {len(processed_results)} results")
            for i, (result, score) in enumerate(processed_results[:3]):  # –õ–æ–≥–∏—Ä—É–µ–º –ø–µ—Ä–≤—ã–µ 3
                logger.debug(f"  Result {i}: score={score}, content_len={len(result.get('content', ''))}, source={result.get('source', 'N/A')}")
            logger.debug(f"Reranked to {len(processed_results)} results")
        else:
            logger.debug(f"Reranker not enabled or no results: reranker_enabled={config.reranker_enabled}, results_count={len(processed_results)}")
        
        logger.info(f"Search completed successfully with {len(processed_results)} results")
        return processed_results, error
        
    except Exception as e:
        logger.exception(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ: {e}")
        return [], str(e)


def search_collections(query: str, k: int = None, metadata_filter: Optional[Dict[str, Any]] = None) -> List[Tuple[Any, float]]:
    """
    –í—ã–ø–æ–ª–Ω—è–µ—Ç –ø–æ–∏—Å–∫ –ø–æ –∫–æ–ª–ª–µ–∫—Ü–∏—è–º, –∏—Å–ø–æ–ª—å–∑—É—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –∏–∑ config.json.
    
    Args:
        query (str): –ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å.
        k (int): –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤. –ï—Å–ª–∏ None, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –∑–Ω–∞—á–µ–Ω–∏–µ –∏–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏.
        metadata_filter (Optional[Dict[str, Any]]): –§–∏–ª—å—Ç—Ä –ø–æ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º.
        
    Returns:
        List[Tuple[Any, float]]: –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞.
    """
    import asyncio
    try:
        config_manager = ConfigManager.get_instance()
        config: Config = config_manager.get()
        if k is None:
            k = config.search_default_k
            
        # –ü–æ–ª—É—á–∞–µ–º —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –¥–ª—è –ø–æ–∏—Å–∫–∞
        device = get_search_device(config.device)
        
        # –í—ã–ø–æ–ª–Ω—è–µ–º –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –ø–æ–∏—Å–∫
        results, error = asyncio.run(search_in_collection(
            query=query,
            collection_name=config.collection_name,
            device=device,
            k=k,
            hybrid=config.use_hybrid,
            metadata_filter=metadata_filter
        ))
        
        if error:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ: {error}")
            return []
            
        return results
    except Exception as e:
        logger.exception(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ: {e}")
        return []
====================================================================================================
==================== –ö–û–ù–ï–¶ –§–ê–ô–õ–ê: core\search\searcher.py ====================



==================== –ù–ê–ß–ê–õ–û –§–ê–ô–õ–ê: core\search\search_executor.py ====================
# –§–∞–π–ª: core\search\search_executor.py
====================================================================================================
"""–ú–æ–¥—É–ª—å –¥–ª—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –ø–æ–∏—Å–∫–∞."""

import logging
import asyncio
from typing import Any, Dict, List, Optional, Tuple

from qdrant_client import models
from qdrant_client.http.models import FieldCondition, Filter, MatchValue, Range
from qdrant_client.models import SparseVector, Fusion, FusionQuery, Prefetch

from config.config_manager import ConfigManager
from core.embedding.embeddings import get_dense_embedder

logger = logging.getLogger(__name__)


class SearchExecutor:
    """–ö–ª–∞—Å—Å –¥–ª—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –ø–æ–∏—Å–∫–æ–≤—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤."""
    
    @staticmethod
    async def execute_search_with_vector(
        client, 
        query: str,
        query_vector: List[float],
        k: int, 
        metadata_filter: Optional[Dict[str, Any]] = None
    ) -> Tuple[List[Tuple[Any, float]], Optional[str]]:
        """
        –í—ã–ø–æ–ª–Ω—è–µ—Ç –ø–æ–∏—Å–∫ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ –≤—ã—á–∏—Å–ª–µ–Ω–Ω–æ–≥–æ –≤–µ–∫—Ç–æ—Ä–∞ –∑–∞–ø—Ä–æ—Å–∞.
        
        Args:
            client: QdrantClient.
            query (str): –û—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π —Ç–µ–∫—Å—Ç–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å (–¥–ª—è –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è –∏ reranking).
            query_vector (List[float]): –ü—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ –≤—ã—á–∏—Å–ª–µ–Ω–Ω—ã–π –≤–µ–∫—Ç–æ—Ä –∑–∞–ø—Ä–æ—Å–∞.
            k (int): –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤.
            metadata_filter (Optional[Dict[str, Any]]): –§–∏–ª—å—Ç—Ä –ø–æ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º.
            
        Returns:
            Tuple[List[Tuple[Any, float]], Optional[str]]: (—Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞, –æ—à–∏–±–∫–∞)
        """
        try:
            collection_name = ConfigManager.get_instance().get().collection_name  # –ü–æ–ª—É—á–∞–µ–º –∏–∑ –∫–æ–Ω—Ñ–∏–≥–∞
            # –í—ã–ø–æ–ª–Ω—è–µ–º –ø–æ–∏—Å–∫ —Å –æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ–π —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–µ–π –ø–æ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º, –∏—Å–ø–æ–ª—å–∑—É—è –≥–æ—Ç–æ–≤—ã–π –≤–µ–∫—Ç–æ—Ä
            if metadata_filter:
                # –°–æ–∑–¥–∞–µ–º —Ñ–∏–ª—å—Ç—Ä –¥–ª—è Qdrant
                must_conditions = []
                
                for key, value in metadata_filter.items():
                    # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ä–∞–∑–ª–∏—á–Ω—ã–µ —Ç–∏–ø—ã —É—Å–ª–æ–≤–∏–π
                    if isinstance(value, dict):
                        # –°–ª–æ–∂–Ω—ã–µ —É—Å–ª–æ–≤–∏—è (–Ω–∞–ø—Ä–∏–º–µ—Ä, {"$gt": 2020})
                        for op, op_value in value.items():
                            if op == "$gt":
                                must_conditions.append(FieldCondition(
                                    key=f"metadata.{key}",
                                    range=Range(gt=op_value)
                                ))
                            elif op == "$gte":
                                must_conditions.append(FieldCondition(
                                    key=f"metadata.{key}",
                                    range=Range(gte=op_value)
                                ))
                            elif op == "$lt":
                                must_conditions.append(FieldCondition(
                                    key=f"metadata.{key}",
                                    range=Range(lt=op_value)
                                ))
                            elif op == "$lte":
                                must_conditions.append(FieldCondition(
                                    key=f"metadata.{key}",
                                    range=Range(lte=op_value)
                                ))
                            elif op == "$contains":
                                # –î–ª—è –º–∞—Å—Å–∏–≤–æ–≤ –∏–ª–∏ —Å—Ç—Ä–æ–∫
                                must_conditions.append(FieldCondition(
                                    key=f"metadata.{key}",
                                    match=MatchValue(value=op_value)
                                ))
                    else:
                        # –ü—Ä–æ—Å—Ç–æ–µ —Ä–∞–≤–µ–Ω—Å—Ç–≤–æ
                        must_conditions.append(FieldCondition(
                            key=f"metadata.{key}",
                            match=MatchValue(value=value)
                        ))
                
                search_filter = Filter(must=must_conditions)
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º –Ω–∞—Ç–∏–≤–Ω—ã–π –ø–æ–∏—Å–∫ Qdrant —Å –∏–º–µ–Ω–æ–≤–∞–Ω–Ω—ã–º –≤–µ–∫—Ç–æ—Ä–æ–º
                results = client.search(
                    collection_name=collection_name,
                    query_vector=("dense_vector", query_vector),
                    limit=k,
                    query_filter=search_filter,
                    with_payload=True,
                    with_vectors=False
                )
            else:
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º –Ω–∞—Ç–∏–≤–Ω—ã–π –ø–æ–∏—Å–∫ Qdrant –±–µ–∑ —Ñ–∏–ª—å—Ç—Ä–æ–≤ —Å –∏–º–µ–Ω–æ–≤–∞–Ω–Ω—ã–º –≤–µ–∫—Ç–æ—Ä–æ–º
                results = client.search(
                    collection_name=collection_name,
                    query_vector=("dense_vector", query_vector),
                    limit=k,
                    with_payload=True,
                    with_vectors=False
                )

            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ —á–∞–Ω–∫–æ–≤
            processed_results = []
            for point in results:  # –¢–µ–ø–µ—Ä—å results - —ç—Ç–æ —Å–ø–∏—Å–æ–∫ PointStruct
                # –ò–∑–≤–ª–µ–∫–∞–µ–º –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ payload
                payload = point.payload if hasattr(point, 'payload') else {}
                content = payload.get('content', '') or payload.get('page_content', '')
                metadata = payload.get('metadata', {})
                
                # –°–æ–∑–¥–∞–µ–º —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π –æ–±—ä–µ–∫—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
                extended_result = {
                    'content': content if content is not None else '',
                    'metadata': metadata,
                    'original_score': point.score if hasattr(point, 'score') else 0  # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—É—é –æ—Ü–µ–Ω–∫—É
                }
                
                # –ï—Å–ª–∏ —ç—Ç–æ –º–Ω–æ–≥–æ—É—Ä–æ–≤–Ω–µ–≤—ã–π —á–∞–Ω–∫, –¥–æ–±–∞–≤–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –º–∏–∫—Ä–æ-—á–∞–Ω–∫–∞—Ö
                if 'micro_contents' in metadata:
                    extended_result['micro_contents'] = metadata['micro_contents']
                elif 'micro_contents' in payload:
                    extended_result['micro_contents'] = payload['micro_contents']
                    
                # –î–æ–±–∞–≤–ª—è–µ–º source –µ—Å–ª–∏ –µ—Å—Ç—å
                if 'source' in metadata:
                    extended_result['source'] = metadata['source']
                elif 'source' in payload:
                    extended_result['source'] = payload['source']
                elif 'source' not in extended_result and 'source' in metadata:
                    extended_result['source'] = metadata.get('source', '')
                    
                processed_results.append((extended_result, point.score if hasattr(point, 'score') else 0))
                
            logger.debug(f"Search with vector returned {len(processed_results)} results")
            
            # Log first result before returning to see what we have
            if processed_results:
                first_result, first_score = processed_results[0]
                logger.debug(f"Before returning - First result score: {first_score}, keys: {list(first_result.keys()) if isinstance(first_result, dict) else 'not dict'}")
                if isinstance(first_result, dict):
                    logger.debug(f"Before returning - original_score: {first_result.get('original_score')}")
            
            return processed_results, None
            
        except Exception as e:
            logger.exception(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ —Å –≤–µ–∫—Ç–æ—Ä–æ–º: {e}")
            return [], str(e)

    @staticmethod
    async def execute_hybrid_search(
        client, query: str, embedder, sparse_params: Dict, k: int, metadata_filter: Optional[Dict[str, Any]] = None
    ) -> Tuple[List[Any], Optional[str]]:  # Changed to return raw ScoredPoint objects
        """
        –í—ã–ø–æ–ª–Ω—è–µ—Ç hybrid –ø–æ–∏—Å–∫ —Å dense + sparse —á–µ—Ä–µ–∑ Query API.
        """
        try:
            config = ConfigManager.get_instance().get()
            collection_name = config.collection_name
            
            # Dense vector
            dense_vector = embedder.embed_query(query)
            
            # Sparse vector (—Å –≤–∞–ª–∏–¥–∞—Ü–∏–µ–π)
            sparse_embedding = sparse_params.get("sparse_embedding") if sparse_params else None
            sparse_dict = sparse_embedding.embed_query(query) if sparse_embedding else None
            sparse_name = sparse_params.get("sparse_vector_name", "bm25_text") if sparse_params else "bm25_text"
            sparse_vec = None
            if sparse_dict:
                indices = sorted(set(sparse_dict["indices"]))
                values = [sparse_dict["values"][i] for i in range(len(sparse_dict["indices"])) if sparse_dict["indices"][i] in indices]
                non_zero = sum(1 for v in values if v > 0)
                logger.info(f"Sparse query: {len(indices)} indices, {non_zero} non-zero values")
                if non_zero > 0:
                    sparse_vec = models.SparseVector(indices=indices, values=values)
            
            # Filter –Ω–∞ top-level
            search_filter = SearchExecutor._create_filter(metadata_filter) if metadata_filter else None
            
            # –ï—Å–ª–∏ sparse –≤–µ–∫—Ç–æ—Ä–∞ –Ω–µ—Ç, –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–ª—å–∫–æ dense –ø–æ–∏—Å–∫
            if sparse_vec:
                logger.info(f"Sending hybrid query with dense vector (len={len(dense_vector)}) and sparse vector (len={len(sparse_vec.indices)}) to {sparse_name}")
                
                # Prefetch –¥–ª—è RRF (—Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –µ—Å—Ç—å –æ–±–∞ –≤–µ–∫—Ç–æ—Ä–∞)
                prefetch = []
                
                # Dense prefetch
                prefetch.append(
                    models.Prefetch(
                        query=dense_vector,
                        using="dense_vector",
                        limit=k * 2
                    )
                )
                
                # Sparse prefetch (—Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –µ—Å—Ç—å sparse –≤–µ–∫—Ç–æ—Ä)
                prefetch.append(
                    models.Prefetch(
                        query=sparse_vec,
                        using=sparse_name,
                        limit=k * 2
                    )
                )
                
                # Fusion RRF –±–µ–∑ rrf_k (default k=60)
                fusion = models.FusionQuery(fusion=models.Fusion.RRF)
                
                # Query points
                logger.info(f"Executing query_points with prefetch and RRF fusion for collection {collection_name}")
                response = client.query_points(
                    collection_name=collection_name,
                    prefetch=prefetch,
                    query=fusion,
                    limit=k,
                    query_filter=search_filter,  # Top-level!
                    with_payload=True,
                    with_vectors=False
                )
                
                results = response.points  # List[ScoredPoint]
                
                logger.info(f"RRF query returned {len(results)} results")
                
                # Log details about the scores to debug the issue
                if results:
                    logger.info(f"RRF scores - first few: {[f'{point.score:.6f}' for point in results[:5]]}")
                    logger.info(f"RRF scores - all: {[f'{point.score:.6f}' for point in results]}")
                
                # –ü—Ä–æ–≤–µ—Ä–∏–º –∫–∞—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ RRF - –µ—Å–ª–∏ –æ—Ü–µ–Ω–∫–∏ –æ–±–Ω—É–ª–µ–Ω—ã, –∏—Å–ø–æ–ª—å–∑—É–µ–º dense fallback
                if results:
                    # –ü—Ä–æ–≤–µ—Ä–∏–º, –µ—Å—Ç—å –ª–∏ –∑–Ω–∞—á–∏–º—ã–µ –æ—Ü–µ–Ω–∫–∏ (–Ω–µ –≤—Å–µ –±–ª–∏–∑–∫–∏ –∫ 0)
                    significant_scores = [point for point in results if point.score > 0.0001]
                    if not significant_scores:
                        logger.warning(f"RRF returned low-quality scores ({len(significant_scores)} significant out of {len(results)}) ‚Äî fallback to dense-only")
                        results = client.search(
                            collection_name=collection_name,
                            query_vector=("dense_vector", dense_vector),
                            limit=k,
                            query_filter=search_filter,
                            with_payload=True,
                            with_vectors=False
                        )
                    else:
                        logger.info(f"RRF returned {len(significant_scores)} significant scores out of {len(results)} total")
            else:
                # –ï—Å–ª–∏ sparse –≤–µ–∫—Ç–æ—Ä–∞ –Ω–µ—Ç, –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–ª—å–∫–æ dense –ø–æ–∏—Å–∫
                logger.info("No sparse vector available, using dense-only search")
                results = client.search(
                    collection_name=collection_name,
                    query_vector=("dense_vector", dense_vector),
                    limit=k,
                    query_filter=search_filter,
                    with_payload=True,
                    with_vectors=False
                )
            
            # Check if results exist and log scores appropriately
            if not results:
                logger.warning("Hybrid returned empty ‚Äî fallback to dense-only")
                results = client.search(
                    collection_name=collection_name,
                    query_vector=("dense_vector", dense_vector),
                    limit=k,
                    query_filter=search_filter,
                    with_payload=True,
                    with_vectors=False
                )
            
            logger.info(f"Hybrid results: {len(results)}, first score: {results[0].score if results else 'empty'}")
            
            # CRITICAL FIX: The fallback check was incorrectly triggering even with good RRF scores
            # Only perform dense fallback if RRF results are truly poor (all scores extremely low)
            # This was the source of the issue where good RRF scores were being replaced with dense fallback
            all_scores_are_poor = False
            if results:
                scores_list = [point.score for point in results]
                logger.info(f"RRF scores for fallback check: {scores_list}")
                
                # Check if ALL scores are extremely low
                all_scores_are_poor = all(score < 0.001 for score in scores_list)  # Increased threshold to be more conservative
                logger.info(f"All scores are poor: {all_scores_are_poor}, score details: {scores_list}")
            
            # Only do fallback if ALL scores are genuinely poor (very low)
            if results and all_scores_are_poor:
                logger.warning("All RRF scores are extremely low - fallback to dense-only")
                results = client.search(
                    collection_name=collection_name,
                    query_vector=("dense_vector", dense_vector),
                    limit=k,
                    query_filter=search_filter,
                    with_payload=True,
                    with_vectors=False
                )
            else:
                logger.info("RRF scores are acceptable - no fallback needed")
            
            return results, None  # Return raw ScoredPoint objects for processing in execute_search
            
        except Exception as e:
            logger.exception(f"–û—à–∏–±–∫–∞ –≤ hybrid search: {e}")
            # Explicit fallback –≤ except
            try:
                logger.info("Fallback to dense in except block")
                config = ConfigManager.get_instance().get()
                from core.embedding.embeddings import get_dense_embedder
                embedder = get_dense_embedder(config, "auto")  # Re-init if needed
                dense_vector = embedder.embed_query(query)
                search_filter = SearchExecutor._create_filter(metadata_filter) if metadata_filter else None
                fallback_results = client.search(
                    collection_name=collection_name,
                    query_vector=("dense_vector", dense_vector),
                    limit=k,
                    query_filter=search_filter,
                    with_payload=True,
                    with_vectors=False
                )
                return fallback_results, None  # Return raw ScoredPoint objects for processing in execute_search
            except Exception as fallback_e:
                logger.exception(f"Fallback failed: {fallback_e}")
                return [], str(e)

    @staticmethod
    async def execute_search(
        client, 
        search_mode: str,
        vector_name: Optional[str],
        sparse_params: Optional[Dict],
        query: str, 
        k: int, 
        metadata_filter: Optional[Dict[str, Any]] = None
    ) -> Tuple[List[Tuple[Any, float]], Optional[str]]:
        """
        –í—ã–ø–æ–ª–Ω—è–µ—Ç –ø–æ–∏—Å–∫ —Å –æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ–π —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–µ–π –ø–æ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º.
        
        Args:
            client: QdrantClient.
            search_mode (str): –†–µ–∂–∏–º –ø–æ–∏—Å–∫–∞ ("dense", "sparse", "hybrid").
            vector_name (str): –ò–º—è dense vector.
            sparse_params (Dict): –ü–∞—Ä–∞–º–µ—Ç—Ä—ã sparse.
            query (str): –ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å.
            k (int): –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤.
            metadata_filter (Optional[Dict[str, Any]]): –§–∏–ª—å—Ç—Ä –ø–æ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º.
            
        Returns:
            Tuple[List[Tuple[Any, float]], Optional[str]]: (—Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞, –æ—à–∏–±–∫–∞)
        """
        try:
            config = ConfigManager.get_instance().get()
            
            # –°–æ–∑–¥–∞–µ–º —Ñ–∏–ª—å—Ç—Ä, –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
            search_filter = SearchExecutor._create_filter(metadata_filter) if metadata_filter else None
            
            if search_mode == "hybrid":
                logger.info(f"Starting hybrid search mode for collection {config.collection_name}")
                # –î–ª—è hybrid –Ω—É–∂–µ–Ω dense embedder
                embedder = get_dense_embedder(config, "auto")
                query_vector = embedder.embed_query(query)
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ –≤–∞–ª–∏–¥–Ω—ã–π sparse embedding –≤ sparse_params
                sparse_embedding = sparse_params.get("sparse_embedding") if sparse_params else None
                logger.info(f"Sparse embedding in params: {sparse_embedding is not None}, use_bm25 config: {config.use_bm25}")
                
                # –ï—Å–ª–∏ sparse embedding –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –∏–ª–∏ BM25 –æ—Ç–∫–ª—é—á–µ–Ω –≤ –∫–æ–Ω—Ñ–∏–≥–µ, –∏—Å–ø–æ–ª—å–∑—É–µ–º dense-only
                if not config.use_bm25 or sparse_embedding is None:
                    logger.info("Sparse embedding not available, using dense-only search for hybrid mode")
                    results = client.search(
                        collection_name=config.collection_name,
                        query_vector=("dense_vector", query_vector),
                        limit=k,
                        query_filter=search_filter,
                        with_payload=True,
                        with_vectors=False
                    )
                    error = None
                else:
                    logger.info(f"Executing hybrid search with sparse vector name: {sparse_params.get('sparse_vector_name', 'NOT SPECIFIED')}")
                    # –û–±—ã—á–Ω—ã–π –≥–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–∏—Å–∫
                    results, error = await SearchExecutor.execute_hybrid_search(client, query, embedder, sparse_params, k, metadata_filter)
                    if error:
                        logger.warning(f"Hybrid search failed: {error}, falling back to dense search")
                        # Fallback to dense search
                        results = client.search(
                            collection_name=config.collection_name,
                            query_vector=("dense_vector", query_vector),
                            limit=k,
                            query_filter=search_filter,
                            with_payload=True,
                            with_vectors=False
                        )
                        error = None
            elif search_mode == "sparse":
                # Sparse –ø–æ–∏—Å–∫ ‚Äî –ù–ï –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º dense!
                sparse_vector_name = sparse_params["sparse_vector_name"] if sparse_params else config.sparse_vector_name
                # –ü–æ–ª—É—á–∞–µ–º sparse –≤–µ–∫—Ç–æ—Ä —á–µ—Ä–µ–∑ sparse embedding
                sparse_embedding = sparse_params.get("sparse_embedding")
                sparse_vector = None
                if sparse_embedding:
                    # sparse_result —É–∂–µ –≤ —Ñ–æ—Ä–º–∞—Ç–µ {"indices": [...], "values": [...]} –æ—Ç –∞–¥–∞–ø—Ç–µ—Ä–∞
                    sparse_result = sparse_embedding.embed_query(query)
                    sparse_vector = sparse_result  # —É–∂–µ –≤ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–º —Ñ–æ—Ä–º–∞—Ç–µ
                
                if sparse_vector:
                    # –í–∞–ª–∏–¥–∞—Ü–∏—è –∏ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è sparse vector
                    indices = sorted(set(sparse_vector["indices"]))  # Unique + sorted (Qdrant requires)
                    values = [sparse_vector["values"][i] for i in range(len(sparse_vector["indices"])) if sparse_vector["indices"][i] in indices]  # Match to unique indices
                    
                    # Log –¥–ª—è debug
                    non_zero = sum(1 for v in values if abs(v) > 1e-10)  # Check for non-zero values (with tolerance)
                    logger.info(f"Sparse query: {len(indices)} indices, {non_zero} non-zero values")
                    
                    if non_zero == 0:
                        logger.warning("Sparse query has 0 non-zero values ‚Äî skipping sparse search")
                        results = []
                    else:
                        sparse_vec = models.SparseVector(indices=indices, values=values)
                        named_sparse = models.NamedSparseVector(name=sparse_vector_name, vector=sparse_vec)
                        
                        results = client.search(
                            collection_name=config.collection_name,
                            query_vector=named_sparse,  # –ü–µ—Ä–µ–¥–∞–µ–º sparse –∫–∞–∫ query_vector
                            limit=k,
                            query_filter=search_filter,
                            with_payload=True,
                            with_vectors=False
                        )
                else:
                    # –ï—Å–ª–∏ sparse –≤–µ–∫—Ç–æ—Ä –Ω–µ —Å–æ–∑–¥–∞–Ω, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –ø—É—Å—Ç–æ–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç
                    results = []
            else:
                # Dense ‚Äî –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º dense embedder
                embedder = get_dense_embedder(config, "auto")
                query_vector = embedder.embed_query(query)
                # –î–ª—è dense –≤–µ–∫—Ç–æ—Ä–æ–≤ –∏—Å–ø–æ–ª—å–∑—É–µ–º –∏–º–µ–Ω–æ–≤–∞–Ω–Ω—ã–π –≤–µ–∫—Ç–æ—Ä
                if vector_name:
                    results = client.search(
                        collection_name=config.collection_name,
                        query_vector=(vector_name, query_vector),
                        limit=k,
                        query_filter=search_filter,
                        with_payload=True,
                        with_vectors=False
                    )
                else:
                    # –ï—Å–ª–∏ –∏–º—è –≤–µ–∫—Ç–æ—Ä–∞ –Ω–µ —É–∫–∞–∑–∞–Ω–æ, –∏—Å–ø–æ–ª—å–∑—É–µ–º –∏–º—è –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é "dense_vector"
                    results = client.search(
                        collection_name=config.collection_name,
                        query_vector=("dense_vector", query_vector),
                        limit=k,
                        query_filter=search_filter,
                        with_payload=True,
                        with_vectors=False
                    )
        
            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ —á–∞–Ω–∫–æ–≤
            processed_results = []
            for point in results:  # –¢–µ–ø–µ—Ä—å results - —ç—Ç–æ —Å–ø–∏—Å–æ–∫ PointStruct
                # –ò–∑–≤–ª–µ–∫–∞–µ–º –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ payload
                payload = point.payload if hasattr(point, 'payload') else {}
                content = payload.get('content', '') or payload.get('page_content', '')
                metadata = payload.get('metadata', {})
                
                # –°–æ–∑–¥–∞–µ–º —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π –æ–±—ä–µ–∫—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
                extended_result = {
                    'content': content if content is not None else '',
                    'metadata': metadata,
                    'original_score': point.score if hasattr(point, 'score') else 0  # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—É—é –æ—Ü–µ–Ω–∫—É
                }
                
                # –ï—Å–ª–∏ —ç—Ç–æ –º–Ω–æ–≥–æ—É—Ä–æ–≤–Ω–µ–≤—ã–π —á–∞–Ω–∫, –¥–æ–±–∞–≤–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –º–∏–∫—Ä–æ-—á–∞–Ω–∫–∞—Ö
                if 'micro_contents' in metadata:
                    extended_result['micro_contents'] = metadata['micro_contents']
                elif 'micro_contents' in payload:
                    extended_result['micro_contents'] = payload['micro_contents']
                    
                # –î–æ–±–∞–≤–ª—è–µ–º source –µ—Å–ª–∏ –µ—Å—Ç—å
                if 'source' in metadata:
                    extended_result['source'] = metadata['source']
                elif 'source' in payload:
                    extended_result['source'] = payload['source']
                elif 'source' not in extended_result and 'source' in metadata:
                    extended_result['source'] = metadata.get('source', '')
                    
                processed_results.append((extended_result, point.score if hasattr(point, 'score') else 0))
                
            logger.debug(f"Search returned {len(processed_results)} results")
            
            # Log first result before returning to see what we have
            if processed_results:
                first_result, first_score = processed_results[0]
                logger.debug(f"Before returning - First result score: {first_score}, keys: {list(first_result.keys()) if isinstance(first_result, dict) else 'not dict'}")
                if isinstance(first_result, dict):
                    logger.debug(f"Before returning - original_score: {first_result.get('original_score')}")
                    
            # Explicit logging to show which vector names are used in the search
            logger.info(f"Search execution completed with mode '{search_mode}', dense vector: 'dense_vector', sparse vector: {sparse_params.get('sparse_vector_name', 'None') if sparse_params else 'None'}")
        
            return processed_results, None
            
        except Exception as e:
            logger.exception(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ: {e}")
            return [], str(e)

    @staticmethod
    def _create_filter(metadata_filter: Optional[Dict[str, Any]]) -> Optional[Filter]:
        """–í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–π –º–µ—Ç–æ–¥ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —Ñ–∏–ª—å—Ç—Ä–∞."""
        if not metadata_filter:
            return None
        must_conditions = []
        for key, value in metadata_filter.items():
            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ä–∞–∑–ª–∏—á–Ω—ã–µ —Ç–∏–ø—ã —É—Å–ª–æ–≤–∏–π
            if isinstance(value, dict):
                # –°–ª–æ–∂–Ω—ã–µ —É—Å–ª–æ–≤–∏—è (–Ω–∞–ø—Ä–∏–º–µ—Ä, {"$gt": 2020})
                for op, op_value in value.items():
                    if op == "$gt":
                        must_conditions.append(FieldCondition(
                            key=f"metadata.{key}",
                            range=Range(gt=op_value)
                        ))
                    elif op == "$gte":
                        must_conditions.append(FieldCondition(
                            key=f"metadata.{key}",
                            range=Range(gte=op_value)
                        ))
                    elif op == "$lt":
                        must_conditions.append(FieldCondition(
                            key=f"metadata.{key}",
                            range=Range(lt=op_value)
                        ))
                    elif op == "$lte":
                        must_conditions.append(FieldCondition(
                            key=f"metadata.{key}",
                            range=Range(lte=op_value)
                        ))
                    elif op == "$contains":
                        # –î–ª—è –º–∞—Å—Å–∏–≤–æ–≤ –∏–ª–∏ —Å—Ç—Ä–æ–∫
                        must_conditions.append(FieldCondition(
                            key=f"metadata.{key}",
                            match=MatchValue(value=op_value)
                        ))
            else:
                # –ü—Ä–æ—Å—Ç–æ–µ —Ä–∞–≤–µ–Ω—Å—Ç–≤–æ
                must_conditions.append(FieldCondition(
                    key=f"metadata.{key}",
                    match=MatchValue(value=value)
                ))
        return Filter(must=must_conditions)
====================================================================================================
==================== –ö–û–ù–ï–¶ –§–ê–ô–õ–ê: core\search\search_executor.py ====================



==================== –ù–ê–ß–ê–õ–û –§–ê–ô–õ–ê: core\search\search_strategy.py ====================
# –§–∞–π–ª: core\search\search_strategy.py
====================================================================================================
"""–ú–æ–¥—É–ª—å –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ –ø–æ–∏—Å–∫–∞."""

import logging
from typing import Tuple, List, Optional, Dict, Any

from qdrant_client.http.models import RecommendRequest, SearchRequest, ScoredPoint
from qdrant_client.models import SparseVectorParams, Modifier, SparseIndexParams

from core.search.collection_analyzer import CollectionAnalyzer
from config.config_manager import ConfigManager

logger = logging.getLogger(__name__)


class SearchStrategy:
    """–ö–ª–∞—Å—Å –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ –ø–æ–∏—Å–∫–∞."""
    
    def __init__(self, client, collection_name: str, embedder, sparse_emb=None):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Å—Ç—Ä–∞—Ç–µ–≥–∏—é –ø–æ–∏—Å–∫–∞.
        
        Args:
            client: –ö–ª–∏–µ–Ω—Ç Qdrant.
            collection_name (str): –ù–∞–∑–≤–∞–Ω–∏–µ –∫–æ–ª–ª–µ–∫—Ü–∏–∏.
            embedder: –≠–º–±–µ–¥–¥–µ—Ä –¥–ª—è dense –≤–µ–∫—Ç–æ—Ä–æ–≤.
            sparse_emb: –≠–º–±–µ–¥–¥–µ—Ä –¥–ª—è sparse –≤–µ–∫—Ç–æ—Ä–æ–≤ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ).
        """
        self.client = client
        self.collection_name = collection_name
        self.embedder = embedder
        self.sparse_emb = sparse_emb
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∫–æ–ª–ª–µ–∫—Ü–∏—é
        analyzer = CollectionAnalyzer()
        config = ConfigManager.get_instance().get()
        self.has_dense, self.has_sparse, self.sparse_vector_name = analyzer.analyze_collection(
            client, collection_name,
            sparse_name=config.sparse_vector_name if config.use_bm25 else None
        )
    
    def create_or_update_collection_for_bm25(self):
        """Creates/updates collection with native BM25 sparse config."""
        config = ConfigManager.get_instance().get()
        if not config.use_bm25:
            return
        
        sparse_config = {
            config.sparse_vector_name: SparseVectorParams(
                modifier=Modifier.IDF,  # Enable BM25-like IDF
                index=SparseIndexParams(on_disk=True)  # For large collections
            )
        }
        # Integrate with existing create_collection (call in startup or indexer)
        # Note: This is a placeholder - actual implementation would depend on how collections are created
        logger.info(f"Configured collection '{self.collection_name}' with BM25 sparse vector config")

    def determine_search_mode(self, hybrid: bool, search_type: str = "auto") -> str:
        """
        –û–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Ä–µ–∂–∏–º –ø–æ–∏—Å–∫–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤.
        
        Args:
            hybrid (bool): –¢—Ä–µ–±—É–µ—Ç—Å—è –ª–∏ –≥–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–∏—Å–∫.
            search_type (str): –¢–∏–ø –ø–æ–∏—Å–∫–∞ ("auto", "dense", "sparse", "hybrid").
            
        Returns:
            str: –†–µ–∂–∏–º –ø–æ–∏—Å–∫–∞ ("hybrid", "dense", "sparse").
        """
        logger.info(f"Determining search mode: hybrid={hybrid}, search_type={search_type}, has_dense={self.has_dense}, has_sparse={self.has_sparse}")
        
        # –ï—Å–ª–∏ —è–≤–Ω–æ —É–∫–∞–∑–∞–Ω —Ç–∏–ø –ø–æ–∏—Å–∫–∞, –∏—Å–ø–æ–ª—å–∑—É–µ–º –µ–≥–æ
        if search_type == "sparse":
            if self.has_sparse:
                logger.info("Explicit sparse-only search mode selected")
                return "sparse"
            else:
                logger.warning(f"Collection '{self.collection_name}' does not contain sparse vectors. Falling back to dense search.")
                return "dense"
        elif search_type == "dense":
            logger.info("Explicit dense-only search mode selected")
            return "dense"
        elif search_type == "hybrid":
            if self.has_sparse and self.has_dense:
                logger.info("Explicit hybrid search mode selected")
                return "hybrid"
            elif self.has_dense:
                logger.info("Hybrid mode requested but no sparse vectors - using dense search")
                return "dense"
            else:
                logger.info("Hybrid mode requested but no dense vectors - using sparse search")
                return "sparse"
        
        # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ (—Å—Ç–∞—Ä–∞—è –ª–æ–≥–∏–∫–∞)
        if hybrid:
            if not self.has_sparse:
                logger.warning(f"–ö–æ–ª–ª–µ–∫—Ü–∏—è '{self.collection_name}' –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç sparse-–≤–µ–∫—Ç–æ—Ä–æ–≤. –í—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è —Ç–æ–ª—å–∫–æ dense search.")
                return "dense"
            elif not self.has_dense:
                logger.warning(f"–ö–æ–ª–ª–µ–∫—Ü–∏—è '{self.collection_name}' –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç dense-–≤–µ–∫—Ç–æ—Ä–æ–≤. –í—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è —Ç–æ–ª—å–∫–æ sparse search.")
                return "sparse"
            else:
                logger.info("Hybrid search mode selected")
                return "hybrid"
        elif self.has_sparse and not self.has_dense:
            # –¢–æ–ª—å–∫–æ sparse –≤–µ–∫—Ç–æ—Ä—ã
            logger.info("Sparse-only search mode selected")
            return "sparse"
        else:
            # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é dense (–∏–ª–∏ –≥–∏–±—Ä–∏–¥ –Ω–µ–≤–æ–∑–º–æ–∂–µ–Ω)
            logger.info("Dense search mode selected (default)")
            return "dense"
    
    def create_qdrant_searcher(self, search_mode: str):
        """
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –Ω–∞—Å—Ç—Ä–æ–µ–Ω–Ω—ã–π –∫–ª–∏–µ–Ω—Ç –∏ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è –ø–æ–∏—Å–∫–∞.
        
        Args:
            search_mode (str): –†–µ–∂–∏–º –ø–æ–∏—Å–∫–∞.
            
        Returns:
            Tuple[QdrantClient, str, Optional[str], Optional[Any]]: (client, mode, vector_name, sparse_params)
        """
        try:
            if search_mode == "hybrid":
                logger.info(f"Configuring hybrid search for collection '{self.collection_name}'")
                sparse_params = {
                    "sparse_vector_name": self.sparse_vector_name,
                    "sparse_embedding": self.sparse_emb
                }
                return self.client, "hybrid", "dense_vector", sparse_params
            elif search_mode == "sparse":
                logger.info(f"Configuring sparse-only search for collection '{self.collection_name}' with sparse_vector_name='{self.sparse_vector_name}'")
                return self.client, "sparse", None, {
                    "sparse_vector_name": self.sparse_vector_name,
                    "sparse_embedding": self.sparse_emb,
                    "use_idf": True  # BM25 IDF
                }
            else:
                # Dense-only –ø–æ–∏—Å–∫ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é)
                logger.info(f"Configuring dense-only search for collection '{self.collection_name}'")
                return self.client, "dense", "dense_vector", None
        except Exception as e:
            # –ï—Å–ª–∏ –≤–æ–∑–Ω–∏–∫–ª–∞ –æ—à–∏–±–∫–∞, –ø—Ä–æ–≤–µ—Ä–∏–º, –Ω–µ —Å–≤—è–∑–∞–Ω–∞ –ª–∏ –æ–Ω–∞ —Å –∫–æ–Ω—Ñ–ª–∏–∫—Ç–æ–º —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–µ–π
            err_msg = str(e)
            if "dimensions" in err_msg and "force_recreate" in err_msg:
                logger.warning(f"–ö–æ–Ω—Ñ–ª–∏–∫—Ç —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–µ–π –≤–µ–∫—Ç–æ—Ä–æ–≤ –≤ –∫–æ–ª–ª–µ–∫—Ü–∏–∏ '{self.collection_name}'. –ü–æ–ø—ã—Ç–∫–∞ –ø–µ—Ä–µ—Å–æ–∑–¥–∞—Ç—å –∫–æ–ª–ª–µ–∫—Ü–∏—é.")
                # –í –Ω–æ–≤–æ–π –≤–µ—Ä—Å–∏–∏ –Ω–∞—Ç–∏–≤–Ω–æ–≥–æ –∫–ª–∏–µ–Ω—Ç–∞ —ç—Ç–æ—Ç –±–ª–æ–∫ –æ—Å—Ç–∞–µ—Ç—Å—è –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏, –Ω–æ —Ç–µ–ø–µ—Ä—å –Ω–µ –Ω—É–∂–µ–Ω –¥–ª—è –Ω–∞—Ç–∏–≤–Ω–æ–≥–æ
                try:  # –≠—Ç–æ—Ç –±–ª–æ–∫ –æ—Å—Ç–∞–µ—Ç—Å—è –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏, –Ω–æ —Ç–µ–ø–µ—Ä—å –Ω–µ –Ω—É–∂–µ–Ω –¥–ª—è –Ω–∞—Ç–∏–≤–Ω–æ–≥–æ
                    # –£–¥–∞–ª—è–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â—É—é –∫–æ–ª–ª–µ–∫—Ü–∏—é
                    self.client.delete_collection(self.collection_name)
                    logger.info(f"–ö–æ–ª–ª–µ–∫—Ü–∏—è '{self.collection_name}' —É–¥–∞–ª–µ–Ω–∞. –ü–æ–≤—Ç–æ—Ä–Ω–∞—è –ø–æ–ø—ã—Ç–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è QdrantVectorStore.")
                    
                    # –ü–æ–≤—Ç–æ—Ä–Ω–∞—è –ø–æ–ø—ã—Ç–∫–∞
                    if search_mode == "hybrid":
                        sparse_params = {
                            "sparse_vector_name": self.sparse_vector_name,
                            "sparse_embedding": self.sparse_emb
                        }
                        return self.client, "hybrid", "dense_vector", sparse_params
                    elif search_mode == "sparse":
                        return self.client, "sparse", None, {
                            "sparse_vector_name": self.sparse_vector_name,
                            "sparse_embedding": self.sparse_emb
                        }
                    else:
                        return self.client, "dense", "dense_vector", None
                except Exception as recreate_error:
                    logger.exception(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø–µ—Ä–µ—Å–æ–∑–¥–∞—Ç—å –∫–æ–ª–ª–µ–∫—Ü–∏—é '{self.collection_name}': {recreate_error}")
                    raise recreate_error
            else:
                # –ï—Å–ª–∏ –æ—à–∏–±–∫–∞ –Ω–µ —Å–≤—è–∑–∞–Ω–∞ —Å –∫–æ–Ω—Ñ–ª–∏–∫—Ç–æ–º —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–µ–π, –ø—Ä–æ–±—Ä–∞—Å—ã–≤–∞–µ–º –µ—ë –¥–∞–ª—å—à–µ
                logger.exception(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ QdrantVectorStore: {e}")
                raise e
====================================================================================================
==================== –ö–û–ù–ï–¶ –§–ê–ô–õ–ê: core\search\search_strategy.py ====================



==================== –ù–ê–ß–ê–õ–û –§–ê–ô–õ–ê: core\search\__init__.py ====================
# –§–∞–π–ª: core\search\__init__.py
====================================================================================================
from .searcher import search_collections, search_in_collection  # –≠–∫—Å–ø–æ—Ä—Ç

====================================================================================================
==================== –ö–û–ù–ï–¶ –§–ê–ô–õ–ê: core\search\__init__.py ====================



==================== –ù–ê–ß–ê–õ–û –§–ê–ô–õ–ê: core\utils\collection_manager.py ====================
# –§–∞–π–ª: core\utils\collection_manager.py
====================================================================================================
"""Module for centralized Qdrant collection management."""

import logging
import time
from typing import Any, Dict, Optional

from qdrant_client import QdrantClient
from qdrant_client.models import CollectionInfo

from config.config_manager import ConfigManager
from core.qdrant.qdrant_client import get_qdrant_client

logger = logging.getLogger(__name__)


class CollectionError(Exception):
    """Custom exception for collection management errors."""
    pass


class CollectionManager:
    """Centralized manager for Qdrant collections with caching."""
    
    _instance: Optional['CollectionManager'] = None
    
    def __init__(self, cache_ttl: int = 60):
        """Initialize CollectionManager.
        
        Args:
            cache_ttl: Time to live for cached collections in seconds
        """
        self.cache_ttl = cache_ttl
        self._cached_collections: Optional[Dict[str, CollectionInfo]] = None
        self._cached_collections_time: float = 0
        self.config_manager = ConfigManager.get_instance()
        
    @classmethod
    def get_instance(cls, cache_ttl: int = 60) -> 'CollectionManager':
        """Get singleton instance of CollectionManager.
        
        Args:
            cache_ttl: Time to live for cached collections in seconds
            
        Returns:
            CollectionManager: Singleton instance
        """
        if cls._instance is None:
            cls._instance = cls(cache_ttl)
        return cls._instance
    
    def get_collections(self, client: Optional[QdrantClient] = None) -> Dict[str, CollectionInfo]:
        """Get collections from Qdrant with caching.
        
        Args:
            client: Qdrant client (optional, will create new if not provided)
            
        Returns:
            Dict[str, CollectionInfo]: Dictionary of collection names and their info
            
        Raises:
            CollectionError: If there's an error retrieving collections
        """
        current_time = time.time()
        
        # Check if we have valid cached collections
        if (self._cached_collections is not None and 
            current_time - self._cached_collections_time <= self.cache_ttl):
            logger.debug("Returning cached collections")
            return self._cached_collections
            
        # Get client if not provided
        if client is None:
            config = self.config_manager.get()
            client = get_qdrant_client(config)
            
        try:
            # Get collections from Qdrant
            collections_response = client.get_collections()
            collections_dict = {coll.name: coll for coll in collections_response.collections}
            
            # Update cache
            self._cached_collections = collections_dict
            self._cached_collections_time = current_time
            
            logger.debug(f"Loaded {len(collections_dict)} collections from Qdrant")
            return collections_dict
            
        except Exception as e:
            logger.exception(f"Error retrieving collections from Qdrant: {e}")
            raise CollectionError(f"Failed to retrieve collections: {e}")
    
    def refresh_collections(self, client: Optional[QdrantClient] = None) -> Dict[str, CollectionInfo]:
        """Force refresh collections from Qdrant, bypassing cache.
        
        Args:
            client: Qdrant client (optional, will create new if not provided)
            
        Returns:
            Dict[str, CollectionInfo]: Dictionary of collection names and their info
            
        Raises:
            CollectionError: If there's an error retrieving collections
        """
        # Clear cache
        self._cached_collections = None
        self._cached_collections_time = 0
        
        # Get fresh collections
        return self.get_collections(client)
    
    def recreate_collection(
        self, 
        collection_name: str, 
        vectors_config: Any,
        client: Optional[QdrantClient] = None,
        recreate: bool = True
    ) -> bool:
        """Create or recreate a collection in Qdrant.
        
        Args:
            collection_name: Name of the collection
            vectors_config: Vector configuration for the collection
            client: Qdrant client (optional, will create new if not provided)
            recreate: Whether to delete existing collection before creating
            
        Returns:
            bool: True if collection was created/recreated successfully, False if collection
                  already exists and recreate=False
            
        Raises:
            CollectionError: If there's an error creating the collection
        """
        # Get client if not provided
        if client is None:
            config = self.config_manager.get()
            client = get_qdrant_client(config)
            
        try:
            # Check if collection exists
            collections = self.get_collections(client)
            collection_exists = collection_name in collections
            
            # If collection exists and we don't want to recreate, return False
            if collection_exists and not recreate:
                logger.info(f"Collection '{collection_name}' already exists and recreate=False")
                return False
            
            # Delete existing collection if requested
            if collection_exists and recreate:
                logger.info(f"Deleting existing collection '{collection_name}'")
                client.delete_collection(collection_name)
                # Clear cache after deletion
                self._cached_collections = None
                self._cached_collections_time = 0
            
            # Create new collection
            logger.info(f"Creating collection '{collection_name}'")
            client.create_collection(
                collection_name=collection_name,
                vectors_config=vectors_config
            )
            
            # Clear cache after creation
            self._cached_collections = None
            self._cached_collections_time = 0
            
            logger.info(f"Collection '{collection_name}' created successfully")
            return True
            
        except Exception as e:
            logger.exception(f"Error creating collection '{collection_name}': {e}")
            raise CollectionError(f"Failed to create collection '{collection_name}': {e}")
    
    def collection_exists(self, collection_name: str, client: Optional[QdrantClient] = None) -> bool:
        """Check if a collection exists in Qdrant.
        
        Args:
            collection_name: Name of the collection to check
            client: Qdrant client (optional, will create new if not provided)
            
        Returns:
            bool: True if collection exists, False otherwise
        """
        try:
            collections = self.get_collections(client)
            return collection_name in collections
        except CollectionError:
            return False
    
    def get_collection_info(
        self, 
        collection_name: str, 
        client: Optional[QdrantClient] = None
    ) -> Optional[CollectionInfo]:
        """Get information about a specific collection.
        
        Args:
            collection_name: Name of the collection
            client: Qdrant client (optional, will create new if not provided)
            
        Returns:
            Optional[CollectionInfo]: Collection information or None if not found
        """
        try:
            collections = self.get_collections(client)
            return collections.get(collection_name)
        except CollectionError:
            return None
====================================================================================================
==================== –ö–û–ù–ï–¶ –§–ê–ô–õ–ê: core\utils\collection_manager.py ====================



==================== –ù–ê–ß–ê–õ–û –§–ê–ô–õ–ê: core\utils\constants.py ====================
# –§–∞–π–ª: core\utils\constants.py
====================================================================================================
"""–ú–æ–¥—É–ª—å —Å –∫–æ–Ω—Å—Ç–∞–Ω—Ç–∞–º–∏ –ø—Ä–æ–µ–∫—Ç–∞."""

from pathlib import Path

from jinja2 import Environment, FileSystemLoader

# –ü–æ—Ä–æ–≥ –ø–∞–º—è—Ç–∏ –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ä–∞–∑–º–µ—Ä–∞ –±–∞—Ç—á–∞ (500MB –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é)
MEMORY_THRESHOLD = 500 * 1024 * 1024

# –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ–∏—Å–∫–∞ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
DEFAULT_K = 5

# –†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
DEFAULT_BATCH_SIZE = 32

# URL Qdrant –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
QDRANT_DEFAULT_URL = "http://localhost:6333"

# –ò–º—è –∫–æ–ª–ª–µ–∫—Ü–∏–∏ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
DEFAULT_COLLECTION_NAME = "final-dense-collection"

# –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–ø—ã—Ç–æ–∫ –ø–æ–≤—Ç–æ—Ä–∞ –¥–ª—è –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ Qdrant
RETRY_ATTEMPTS = 3

# –í—Ä–µ–º—è –æ–∂–∏–¥–∞–Ω–∏—è –º–µ–∂–¥—É –ø–æ–ø—ã—Ç–∫–∞–º–∏ –ø–æ–≤—Ç–æ—Ä–∞ (–≤ —Å–µ–∫—É–Ω–¥–∞—Ö)
RETRY_WAIT_TIME = 2

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ Jinja2 —à–∞–±–ª–æ–Ω–∏–∑–∞—Ç–æ—Ä–∞
TEMPLATES_DIR = Path(__file__).parent.parent / "web" / "templates"
TEMPLATES = Environment(loader=FileSystemLoader(TEMPLATES_DIR))
====================================================================================================
==================== –ö–û–ù–ï–¶ –§–ê–ô–õ–ê: core\utils\constants.py ====================



==================== –ù–ê–ß–ê–õ–û –§–ê–ô–õ–ê: core\utils\dependencies.py ====================
# –§–∞–π–ª: core\utils\dependencies.py
====================================================================================================
"""–ú–æ–¥—É–ª—å –¥–ª—è –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π FastAPI."""

import logging
import time

from fastapi import Depends
from tenacity import retry, stop_after_attempt, wait_fixed

from config.config_manager import ConfigManager
from config.settings import Config
from core.qdrant.qdrant_client import get_qdrant_client
from core.utils.constants import RETRY_ATTEMPTS, RETRY_WAIT_TIME

logger = logging.getLogger(__name__)

# --- –ö—ç—à –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è ---
_last_connection_check = 0

# Get singleton instance of ConfigManager
config_manager = ConfigManager.get_instance()


def get_config() -> Config:
    """–ó–∞–≤–∏—Å–∏–º–æ—Å—Ç—å –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏."""
    return config_manager.get()


@retry(stop=stop_after_attempt(RETRY_ATTEMPTS), wait=wait_fixed(RETRY_WAIT_TIME))
def get_client(config: Config = Depends(get_config)):
    """–ó–∞–≤–∏—Å–∏–º–æ—Å—Ç—å –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –∫–ª–∏–µ–Ω—Ç–∞ Qdrant."""
    global _last_connection_check
    
    client = get_qdrant_client(config)
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –ø—Ä–æ—à–ª–æ –±–æ–ª—å—à–µ CONNECTION_CHECK_TTL —Å–µ–∫—É–Ω–¥
    current_time = time.time()
    if current_time - _last_connection_check > config.qdrant_client_cache_ttl:
        try:
            client.get_collections()  # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è
            _last_connection_check = current_time
            logger.debug("–ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è —Å Qdrant –≤—ã–ø–æ–ª–Ω–µ–Ω–∞")
        except Exception as e:
            logger.exception(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–æ–≤–µ—Ä–∫–µ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è —Å Qdrant: {e}")
            raise
    else:
        logger.debug("–ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è —Å Qdrant –ø—Ä–æ–ø—É—â–µ–Ω–∞ (–∫—ç—à)")
    
    return client



====================================================================================================
==================== –ö–û–ù–ï–¶ –§–ê–ô–õ–ê: core\utils\dependencies.py ====================



==================== –ù–ê–ß–ê–õ–û –§–ê–ô–õ–ê: core\utils\exception_handlers.py ====================
# –§–∞–π–ª: core\utils\exception_handlers.py
====================================================================================================
"""Module for centralized exception handling in the FastAPI application."""

import logging
import os
import uuid

from fastapi import FastAPI, Request
from fastapi.exceptions import RequestValidationError
from fastapi.responses import JSONResponse
from starlette.exceptions import HTTPException as StarletteHTTPException

logger = logging.getLogger(__name__)


class AppException(Exception):
    """Base application exception."""
    def __init__(self, message: str, status_code: int = 500, error_code: str = None):
        self.message = message
        self.status_code = status_code
        self.error_code = error_code
        super().__init__(self.message)


class ValidationError(AppException):
    """Validation error exception."""
    def __init__(self, message: str):
        super().__init__(message, status_code=400, error_code="VALIDATION_ERROR")


class AuthenticationError(AppException):
    """Authentication error exception."""
    def __init__(self, message: str):
        super().__init__(message, status_code=401, error_code="AUTHENTICATION_ERROR")


class AuthorizationError(AppException):
    """Authorization error exception."""
    def __init__(self, message: str):
        super().__init__(message, status_code=403, error_code="AUTHORIZATION_ERROR")


class NotFoundError(AppException):
    """Not found error exception."""
    def __init__(self, message: str):
        super().__init__(message, status_code=404, error_code="NOT_FOUND_ERROR")


def add_exception_handlers(app: FastAPI) -> None:
    """Add centralized exception handlers to the FastAPI application.
    
    Args:
        app (FastAPI): The FastAPI application instance
    """
    @app.exception_handler(AppException)
    async def app_exception_handler(request: Request, exc: AppException):
        """Handle custom application exceptions."""
        request_id = getattr(request.state, "request_id", str(uuid.uuid4()))
        
        # Log the error with request context
        logger.error(
            f"AppException [{request_id}]: {exc.message} "
            f"(status_code={exc.status_code}, error_code={exc.error_code}) "
            f"Path: {request.url.path}, Method: {request.method}"
        )
        
        # For AJAX requests, return JSON response
        if request.headers.get("accept", "").startswith("application/json") or \
           request.headers.get("content-type", "").startswith("application/json"):
            return JSONResponse(
                status_code=exc.status_code,
                content={
                    "error": {
                        "type": exc.error_code or "APPLICATION_ERROR",
                        "message": exc.message,
                        "request_id": request_id
                    }
                }
            )
        
        # For HTML requests, return error page or redirect with error message
        return JSONResponse(
            status_code=exc.status_code,
            content={
                "error": {
                    "type": exc.error_code or "APPLICATION_ERROR",
                    "message": exc.message,
                    "request_id": request_id
                }
            }
        )

    @app.exception_handler(StarletteHTTPException)
    async def http_exception_handler(request: Request, exc: StarletteHTTPException):
        """Handle HTTP exceptions."""
        request_id = getattr(request.state, "request_id", str(uuid.uuid4()))
        
        # Log the error with request context
        logger.warning(
            f"HTTPException [{request_id}]: {exc.detail} "
            f"(status_code={exc.status_code}) "
            f"Path: {request.url.path}, Method: {request.method}"
        )
        
        # Special handling for 401 Unauthorized in admin routes
        if exc.status_code == 401 and request.url.path.startswith("/api/admin/"):
            # For AJAX requests, return JSON response
            if request.headers.get("accept", "").startswith("application/json") or \
               request.headers.get("content-type", "").startswith("application/json"):
                return JSONResponse(
                    status_code=exc.status_code,
                    content={
                        "error": {
                            "type": "AUTHENTICATION_ERROR",
                            "message": "–¢—Ä–µ–±—É–µ—Ç—Å—è –∞—É—Ç–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏—è. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ HTTP Basic Authentication —Å API –∫–ª—é—á–æ–º –∏–∑ .env —Ñ–∞–π–ª–∞ –≤ –∫–∞—á–µ—Å—Ç–≤–µ –ø–∞—Ä–æ–ª—è.",
                            "request_id": request_id
                        }
                    }
                )
            
            # For HTML requests to admin routes, return a more informative error page
            from fastapi.responses import HTMLResponse
            return HTMLResponse(
                content=f"""
                <html>
                    <head><title>–¢—Ä–µ–±—É–µ—Ç—Å—è –∞—É—Ç–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏—è</title></head>
                    <body>
                        <h1>–¢—Ä–µ–±—É–µ—Ç—Å—è –∞—É—Ç–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏—è</h1>
                        <p>–î–ª—è –¥–æ—Å—Ç—É–ø–∞ –∫ —ç—Ç–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ –≤–æ–π—Ç–∏ –≤ —Å–∏—Å—Ç–µ–º—É.</p>
                        <p>–ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ HTTP Basic Authentication:</p>
                        <ul>
                            <li>–ò–º—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è: –ª—é–±–æ–µ</li>
                            <li>–ü–∞—Ä–æ–ª—å: –∑–Ω–∞—á–µ–Ω–∏–µ –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π ADMIN_API_KEY –∏–∑ —Ñ–∞–π–ª–∞ .env</li>
                        </ul>
                        <p>–¢–µ–∫—É—â–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ ADMIN_API_KEY: {'—É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ' if os.getenv('ADMIN_API_KEY') else '–Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ'}</p>
                        <p><a href="/">–í–µ—Ä–Ω—É—Ç—å—Å—è –Ω–∞ –≥–ª–∞–≤–Ω—É—é</a></p>
                    </body>
                </html>
                """,
                status_code=401,
                headers={"WWW-Authenticate": "Basic"}
            )
        
        # For AJAX requests, return JSON response
        if request.headers.get("accept", "").startswith("application/json") or \
           request.headers.get("content-type", "").startswith("application/json"):
            return JSONResponse(
                status_code=exc.status_code,
                content={
                    "error": {
                        "type": "HTTP_ERROR",
                        "message": exc.detail,
                        "request_id": request_id
                    }
                }
            )
        
        # For HTML requests, we let the default handler take over for now
        # In a real implementation, we might render an error template
        # Instead of raising the exception, we return a proper 404 response
        from fastapi.responses import HTMLResponse
        return HTMLResponse(
            content=f"""
            <html>
                <head><title>404 Not Found</title></head>
                <body>
                    <h1>404 Not Found</h1>
                    <p>The requested URL was not found on this server.</p>
                    <p>Path: {request.url.path}</p>
                </body>
            </html>
            """,
            status_code=404
        )

    @app.exception_handler(RequestValidationError)
    async def validation_exception_handler(request: Request, exc: RequestValidationError):
        """Handle request validation errors."""
        request_id = getattr(request.state, "request_id", str(uuid.uuid4()))
        
        # Log the error with request context
        logger.warning(
            f"ValidationError [{request_id}]: {exc.errors()} "
            f"Path: {request.url.path}, Method: {request.method}"
        )
        
        # Format validation errors for better readability
        errors = []
        for error in exc.errors():
            errors.append({
                "field": ".".join(str(loc) for loc in error["loc"]),
                "message": error["msg"],
                "type": error["type"]
            })
        
        # For AJAX requests, return JSON response
        if request.headers.get("accept", "").startswith("application/json") or \
           request.headers.get("content-type", "").startswith("application/json"):
            return JSONResponse(
                status_code=422,
                content={
                    "error": {
                        "type": "VALIDATION_ERROR",
                        "message": "Validation failed",
                        "details": errors,
                        "request_id": request_id
                    }
                }
            )
        
        # For HTML requests, return JSON for now (would render template in full implementation)
        return JSONResponse(
            status_code=422,
            content={
                "error": {
                    "type": "VALIDATION_ERROR",
                    "message": "Validation failed",
                    "details": errors,
                    "request_id": request_id
                }
            }
        )

    @app.exception_handler(Exception)
    async def generic_exception_handler(request: Request, exc: Exception):
        """Handle all other unhandled exceptions."""
        request_id = getattr(request.state, "request_id", str(uuid.uuid4()))
        
        # Log the error with full traceback
        logger.exception(
            f"UnhandledException [{request_id}]: {str(exc)} "
            f"Path: {request.url.path}, Method: {request.method}"
        )
        
        # For AJAX requests, return JSON response
        if request.headers.get("accept", "").startswith("application/json") or \
           request.headers.get("content-type", "").startswith("application/json"):
            return JSONResponse(
                status_code=500,
                content={
                    "error": {
                        "type": "INTERNAL_SERVER_ERROR",
                        "message": "An unexpected error occurred",
                        "request_id": request_id
                    }
                }
            )
        
        # For HTML requests, return JSON for now (would render template in full implementation)
        return JSONResponse(
            status_code=500,
            content={
                "error": {
                    "type": "INTERNAL_SERVER_ERROR",
                    "message": "An unexpected error occurred",
                    "request_id": request_id
                }
            }
        )


def get_request_id(request: Request) -> str:
    """Get request ID from asgi-request-id middleware or generate new one.
    
    Args:
        request (Request): The FastAPI request object
        
    Returns:
        str: Request ID
    """
    # Try to get request ID from asgi-request-id middleware first
    try:
        from asgi_request_id.context import request_id_ctx_var
        request_id = request_id_ctx_var.get()
        if request_id:
            return request_id
    except ImportError:
        pass
    except Exception:
        pass
    
    # Fallback to old implementation if asgi-request-id is not available
    if not hasattr(request.state, "request_id"):
        request.state.request_id = str(uuid.uuid4())
    return request.state.request_id
====================================================================================================
==================== –ö–û–ù–ï–¶ –§–ê–ô–õ–ê: core\utils\exception_handlers.py ====================



==================== –ù–ê–ß–ê–õ–û –§–ê–ô–õ–ê: core\utils\__init__.py ====================
# –§–∞–π–ª: core\utils\__init__.py
====================================================================================================
 

====================================================================================================
==================== –ö–û–ù–ï–¶ –§–ê–ô–õ–ê: core\utils\__init__.py ====================

